{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "532d3de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Pretraining Bayesian inference module...\n",
      "Generating pretraining data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mhfar\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining Bayesian inference module for 20 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 0.4847, Accuracy: 0.9037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20, Loss: 0.4919, Accuracy: 0.9172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20, Loss: 0.4591, Accuracy: 0.9182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20, Loss: 0.4251, Accuracy: 0.9286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20, Loss: 0.4676, Accuracy: 0.9205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20, Loss: 0.4711, Accuracy: 0.9158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20, Loss: 0.4974, Accuracy: 0.9061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20, Loss: 0.4719, Accuracy: 0.9093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20, Loss: 0.4847, Accuracy: 0.9069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20, Loss: 0.4880, Accuracy: 0.9068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20, Loss: 0.4698, Accuracy: 0.9165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20, Loss: 0.4868, Accuracy: 0.9049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20, Loss: 0.4856, Accuracy: 0.9054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20, Loss: 0.4931, Accuracy: 0.8969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20, Loss: 0.4784, Accuracy: 0.9003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20, Loss: 0.4906, Accuracy: 0.8971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20, Loss: 0.4801, Accuracy: 0.8992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20, Loss: 0.4614, Accuracy: 0.9030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20, Loss: 0.4889, Accuracy: 0.9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Loss: 0.4617, Accuracy: 0.9039\n",
      "Pretraining complete!\n",
      "Pretraining complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▍         | 99/2000 [02:31<45:41,  1.44s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100, Steps: 22672, Avg reward: 172.44, Success rate: 0.61, Belief accuracy: 0.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▌         | 100/2000 [02:51<3:40:50,  6.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best success rate: 0.5500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|▉         | 199/2000 [04:34<52:06,  1.74s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 200, Steps: 38297, Avg reward: 136.83, Success rate: 0.67, Belief accuracy: 0.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█         | 200/2000 [05:35<9:42:00, 19.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best reward: 451.4554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|█▍        | 298/2000 [07:38<59:17,  2.09s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 300, Steps: 57092, Avg reward: 138.76, Success rate: 0.51, Belief accuracy: 0.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|█▌        | 300/2000 [08:08<3:49:31,  8.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 1 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|█▉        | 399/2000 [10:08<26:08,  1.02it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 400, Steps: 75501, Avg reward: 154.14, Success rate: 0.58, Belief accuracy: 0.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 400/2000 [10:41<4:14:59,  9.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 2 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▍       | 499/2000 [13:00<26:02,  1.04s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 500, Steps: 96705, Avg reward: 195.82, Success rate: 0.62, Belief accuracy: 0.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▌       | 500/2000 [13:14<1:52:16,  4.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 3 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|██▉       | 599/2000 [15:57<53:33,  2.29s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 600, Steps: 122492, Avg reward: 257.81, Success rate: 0.46, Belief accuracy: 0.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|███       | 600/2000 [16:57<6:56:52, 17.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best reward: 612.7537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|███▍      | 699/2000 [22:47<1:01:53,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 700, Steps: 157431, Avg reward: 367.17, Success rate: 0.43, Belief accuracy: 0.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|███▌      | 700/2000 [24:54<14:02:43, 38.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 1 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|███▉      | 799/2000 [33:14<55:55,  2.79s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 800, Steps: 187206, Avg reward: 302.08, Success rate: 0.47, Belief accuracy: 0.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 800/2000 [35:02<11:27:28, 34.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 2 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|████▍     | 899/2000 [41:37<1:49:08,  5.95s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 900, Steps: 210509, Avg reward: 245.16, Success rate: 0.34, Belief accuracy: 0.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|████▌     | 900/2000 [42:50<7:45:56, 25.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 3 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|████▉     | 999/2000 [50:54<1:31:32,  5.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000, Steps: 241750, Avg reward: 319.05, Success rate: 0.37, Belief accuracy: 0.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████     | 1000/2000 [52:16<7:03:22, 25.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 4 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|█████▍    | 1099/2000 [58:27<24:42,  1.65s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1100, Steps: 266135, Avg reward: 243.83, Success rate: 0.44, Belief accuracy: 0.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|█████▌    | 1100/2000 [1:00:30<9:30:08, 38.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 5 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|█████▉    | 1199/2000 [1:06:31<10:14,  1.30it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1200, Steps: 287151, Avg reward: 216.61, Success rate: 0.44, Belief accuracy: 0.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████    | 1200/2000 [1:07:34<4:17:14, 19.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best success rate: 0.6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|██████▍   | 1299/2000 [1:15:34<47:59,  4.11s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1300, Steps: 316606, Avg reward: 306.38, Success rate: 0.42, Belief accuracy: 0.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  65%|██████▌   | 1300/2000 [1:16:06<2:19:54, 11.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 1 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|██████▉   | 1399/2000 [1:24:20<43:42,  4.36s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1400, Steps: 343939, Avg reward: 277.53, Success rate: 0.36, Belief accuracy: 0.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|███████   | 1400/2000 [1:26:01<5:07:16, 30.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 2 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|███████▍  | 1499/2000 [1:33:14<1:12:42,  8.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1500, Steps: 371479, Avg reward: 302.53, Success rate: 0.48, Belief accuracy: 0.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  75%|███████▌  | 1500/2000 [1:35:48<7:04:52, 50.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best reward: 678.8909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|███████▉  | 1599/2000 [1:43:03<10:26,  1.56s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1600, Steps: 398675, Avg reward: 293.63, Success rate: 0.50, Belief accuracy: 0.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|████████  | 1601/2000 [1:44:00<1:24:39, 12.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 1 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  85%|████████▍ | 1699/2000 [1:51:13<33:46,  6.73s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1700, Steps: 427648, Avg reward: 309.22, Success rate: 0.37, Belief accuracy: 0.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  85%|████████▌ | 1700/2000 [1:51:52<1:20:59, 16.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 2 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|████████▉ | 1799/2000 [1:54:36<05:17,  1.58s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1800, Steps: 453187, Avg reward: 270.28, Success rate: 0.40, Belief accuracy: 0.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|█████████ | 1800/2000 [1:55:10<31:24,  9.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 3 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|█████████▍| 1899/2000 [2:00:03<12:39,  7.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1900, Steps: 484178, Avg reward: 362.11, Success rate: 0.53, Belief accuracy: 0.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  95%|█████████▌| 1901/2000 [2:02:10<49:26, 29.97s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 4 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████▉| 1999/2000 [2:05:11<00:01,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2000, Steps: 508945, Avg reward: 270.37, Success rate: 0.41, Belief accuracy: 0.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2000/2000 [2:05:55<00:00,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 5 evaluations\n",
      "Training complete. Running final evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ablation studies...\n",
      "Evaluating: Full Model\n",
      "  Success rate: 0.2000\n",
      "  Episode reward: 370.3006\n",
      "  Completion time: 10.0000\n",
      "  Belief accuracy: 0.2242\n",
      "Evaluating: No Assistance\n",
      "  Success rate: 0.6500\n",
      "  Episode reward: 226.5941\n",
      "  Completion time: 169.5385\n",
      "  Belief accuracy: 0.2924\n",
      "Evaluating: Fixed (0.5)\n",
      "  Success rate: 0.5000\n",
      "  Episode reward: 336.4776\n",
      "  Completion time: 181.9000\n",
      "  Belief accuracy: 0.2739\n",
      "Evaluating: MAP Selection\n",
      "  Success rate: 0.3500\n",
      "  Episode reward: 303.2553\n",
      "  Completion time: 112.1429\n",
      "  Belief accuracy: 0.3222\n",
      "\n",
      "Final Evaluation Results:\n",
      "episode_reward: 611.6992\n",
      "episode_length: 490.6000\n",
      "success_rate: 0.2667\n",
      "collision_rate: 0.3333\n",
      "belief_accuracy: 0.2851\n",
      "completion_time: 22.2500\n",
      "Training and evaluation completed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal, Categorical\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Visualization and training parameters\n",
    "VISUALIZE = False  # Set to False to disable visualization\n",
    "VISUALIZE_INTERVAL = 2000  # Show visualization every n steps\n",
    "SAVE_VIDEOS = False  # Save videos of the trained agent\n",
    "NUM_EPISODES = 2000  # Increased from 1000 to allow more learning\n",
    "EVAL_INTERVAL = 100  # Evaluate every n episodes\n",
    "\n",
    "# Environment parameters\n",
    "GOAL_RADIUS = 0.05\n",
    "NUM_GOALS = 3  # Number of potential goals\n",
    "\n",
    "# PPO parameters - IMPROVED\n",
    "GAMMA = 0.99\n",
    "GAE_LAMBDA = 0.95\n",
    "CLIP_EPSILON = 0.2\n",
    "ENTROPY_COEF = 0.02  # Increased for more exploration\n",
    "VALUE_COEF = 0.5\n",
    "LR = 3e-4\n",
    "BATCH_SIZE = 256  # Increased from 64 for better stability\n",
    "STEPS_PER_UPDATE = 512  # Fixed update schedule instead of variable\n",
    "PPO_EPOCHS = 5  # Reduced from 10 to prevent overfitting\n",
    "TARGET_KL = 0.02  # Increased slightly\n",
    "\n",
    "# Bayesian inference parameters - IMPROVED\n",
    "BETA = 5.0  # Increased for sharper belief updates\n",
    "ANGULAR_WEIGHT = 0.6  # Adjusted for better balance\n",
    "DISTANCE_WEIGHT = 0.4  # Adjusted for better balance\n",
    "BELIEF_SMOOTH_ALPHA = 0.5  # Reduced further for faster belief updates\n",
    "\n",
    "# Reward function weights - CONSISTENT USE\n",
    "COLLISION_WEIGHT = 10.0\n",
    "PROXIMITY_WEIGHT = 2.5\n",
    "FAR_WEIGHT = 1.5\n",
    "PROGRESS_WEIGHT = 3.0\n",
    "AUTONOMY_WEIGHT = 1.5\n",
    "GOAL_WEIGHT = 15.0  # Increased to prioritize goal achievement\n",
    "\n",
    "# Pretraining parameters - IMPROVED\n",
    "PRETRAIN = True\n",
    "PRETRAIN_EPOCHS = 20  # Increased for better pretraining\n",
    "PRETRAIN_BATCH_SIZE = 256\n",
    "PRETRAIN_TRAJECTORIES = 200  # Increased for better generalization\n",
    "PRETRAIN_TRAJECTORY_LENGTH = 50\n",
    "PRETRAIN_LR = 1e-3\n",
    "\n",
    "# Early stopping patience - INCREASED\n",
    "EARLY_STOP_PATIENCE = 10  # Increased from 2 for better chance at converging\n",
    "\n",
    "# Create results directory\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "# Check if CUDA is available\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Custom Reacher 2D Environment (No MuJoCo dependency)\n",
    "class CustomReacher2D(gym.Env):\n",
    "    \"\"\"Custom 2D Reacher Environment that doesn't use MuJoCo\"\"\"\n",
    "    \n",
    "    def __init__(self, render_mode=None):\n",
    "        super(CustomReacher2D, self).__init__()\n",
    "        \n",
    "        # Action space: 2D continuous actions for controlling the arm\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(2,), dtype=np.float32)\n",
    "        \n",
    "        # Observation space: joint angles, joint velocities, end-effector position\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(6,), dtype=np.float32)\n",
    "        \n",
    "        # Arm parameters\n",
    "        self.link_lengths = [0.1, 0.11]  # Length of arm segments\n",
    "        self.max_velocity = 1.0  # Maximum joint velocity\n",
    "        self.dt = 0.05  # Time step\n",
    "        \n",
    "        # State\n",
    "        self.joint_angles = np.array([0.0, 0.0])  # Two joints, in radians\n",
    "        self.joint_velocities = np.array([0.0, 0.0])\n",
    "        \n",
    "        # Rendering setup\n",
    "        self.render_mode = render_mode\n",
    "        self.screen = None\n",
    "        self.clock = None\n",
    "        self.window_size = 500  # pixels\n",
    "        self.pygame = None\n",
    "        \n",
    "        # For visualization\n",
    "        if render_mode == \"rgb_array\":\n",
    "            # Import pygame only if rendering is needed\n",
    "            try:\n",
    "                import pygame\n",
    "                self.pygame = pygame\n",
    "                self.screen = pygame.Surface((self.window_size, self.window_size))\n",
    "                self.clock = pygame.time.Clock()\n",
    "            except ImportError:\n",
    "                self.render_mode = None\n",
    "                print(\"Warning: Pygame not available. Rendering disabled.\")\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        # Reset joint state\n",
    "        self.joint_angles = np.array([np.random.uniform(-np.pi/2, np.pi/2), \n",
    "                                       np.random.uniform(-np.pi/2, np.pi/2)])\n",
    "        self.joint_velocities = np.array([0.0, 0.0])\n",
    "        \n",
    "        # Get current observation\n",
    "        observation = self._get_obs()\n",
    "        info = {}\n",
    "        \n",
    "        return observation, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Clip action to action space\n",
    "        action = np.clip(action, -1, 1)\n",
    "        \n",
    "        # Convert actions to joint accelerations\n",
    "        accelerations = action * 8.0  # Scale factor for accelerations\n",
    "        \n",
    "        # Update velocities using accelerations\n",
    "        self.joint_velocities += accelerations * self.dt\n",
    "        \n",
    "        # Clip velocities\n",
    "        self.joint_velocities = np.clip(self.joint_velocities, -self.max_velocity, self.max_velocity)\n",
    "        \n",
    "        # Update joint angles using velocities\n",
    "        self.joint_angles += self.joint_velocities * self.dt\n",
    "        \n",
    "        # Wrap angles to [-pi, pi]\n",
    "        self.joint_angles = np.mod(self.joint_angles + np.pi, 2 * np.pi) - np.pi\n",
    "        \n",
    "        # Get new observation\n",
    "        observation = self._get_obs()\n",
    "        \n",
    "        # Default reward and done (to be overridden by wrapper)\n",
    "        reward = 0.0\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        info = {}\n",
    "        \n",
    "        return observation, reward, terminated, truncated, info\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        \"\"\"Get current observation (joint angles, velocities, and end-effector position)\"\"\"\n",
    "        ee_pos = self._get_end_effector_position()\n",
    "        obs = np.concatenate([\n",
    "            np.cos(self.joint_angles),\n",
    "            np.sin(self.joint_angles),\n",
    "            ee_pos\n",
    "        ])\n",
    "        return obs\n",
    "    \n",
    "    def _get_end_effector_position(self):\n",
    "        \"\"\"Compute the position of the end effector using forward kinematics\"\"\"\n",
    "        theta1, theta2 = self.joint_angles\n",
    "        l1, l2 = self.link_lengths\n",
    "        \n",
    "        # Position of first joint is at origin (0,0)\n",
    "        # Position of second joint\n",
    "        x1 = l1 * np.cos(theta1)\n",
    "        y1 = l1 * np.sin(theta1)\n",
    "        \n",
    "        # Position of end effector\n",
    "        x2 = x1 + l2 * np.cos(theta1 + theta2)\n",
    "        y2 = y1 + l2 * np.sin(theta1 + theta2)\n",
    "        \n",
    "        return np.array([x2, y2])\n",
    "    \n",
    "    def render(self):\n",
    "        if self.render_mode != \"rgb_array\" or self.pygame is None:\n",
    "            return None\n",
    "        \n",
    "        # Clear the screen\n",
    "        self.screen.fill((255, 255, 255))\n",
    "        \n",
    "        # Convert from world coordinates to screen coordinates\n",
    "        def world_to_screen(point):\n",
    "            scale = self.window_size / 2.5  # Scale to make the arm visible\n",
    "            screen_x = int(point[0] * scale + self.window_size / 2)\n",
    "            screen_y = int(-point[1] * scale + self.window_size / 2)  # Negative because screen y is inverted\n",
    "            return (screen_x, screen_y)\n",
    "        \n",
    "        # Draw the arm\n",
    "        # Base position (origin)\n",
    "        base_pos = world_to_screen((0, 0))\n",
    "        \n",
    "        # First joint position\n",
    "        theta1 = self.joint_angles[0]\n",
    "        l1 = self.link_lengths[0]\n",
    "        joint1_x = l1 * np.cos(theta1)\n",
    "        joint1_y = l1 * np.sin(theta1)\n",
    "        joint1_pos = world_to_screen((joint1_x, joint1_y))\n",
    "        \n",
    "        # End effector position\n",
    "        ee_pos = self._get_end_effector_position()\n",
    "        ee_screen_pos = world_to_screen(ee_pos)\n",
    "        \n",
    "        # Draw the links\n",
    "        self.pygame.draw.line(self.screen, (0, 0, 0), base_pos, joint1_pos, 6)\n",
    "        self.pygame.draw.line(self.screen, (0, 0, 0), joint1_pos, ee_screen_pos, 6)\n",
    "        \n",
    "        # Draw the joints\n",
    "        self.pygame.draw.circle(self.screen, (255, 0, 0), base_pos, 10)\n",
    "        self.pygame.draw.circle(self.screen, (0, 255, 0), joint1_pos, 8)\n",
    "        self.pygame.draw.circle(self.screen, (0, 0, 255), ee_screen_pos, 8)\n",
    "        \n",
    "        return np.transpose(np.array(self.pygame.surfarray.pixels3d(self.screen)), axes=(1, 0, 2))\n",
    "    \n",
    "    def close(self):\n",
    "        if self.screen is not None:\n",
    "            self.screen = None\n",
    "\n",
    "\n",
    "# Environment wrapper for multiple goals\n",
    "class MultiGoalReacherEnv(gym.Wrapper):\n",
    "    def __init__(self, num_goals=3, goal_radius=0.05, render_mode=None):\n",
    "        # Create the base environment\n",
    "        self.env = CustomReacher2D(render_mode=render_mode)\n",
    "        super().__init__(self.env)\n",
    "        \n",
    "        # Potential goal positions (normalized to [-1, 1])\n",
    "        self.num_goals = num_goals\n",
    "        self.goal_radius = goal_radius\n",
    "        self.goals = []\n",
    "        self.true_goal = None\n",
    "        self.true_goal_idx = None\n",
    "        self.obstacles = []\n",
    "        \n",
    "        # Override observation space to include goal information\n",
    "        obs_dim = self.env.observation_space.shape[0]\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=(obs_dim + num_goals * 2,)\n",
    "        )\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        \n",
    "        # Generate random goal positions - IMPROVED goal placement\n",
    "        self.goals = []\n",
    "        # Start with a wider distribution of goals\n",
    "        angles = np.linspace(0, 2*np.pi, self.num_goals, endpoint=False)\n",
    "        angles += np.random.uniform(0, 2*np.pi/self.num_goals)  # Add randomness\n",
    "        \n",
    "        for i in range(self.num_goals):\n",
    "            # Place goals in a circular pattern with some randomness\n",
    "            radius = np.random.uniform(0.12, 0.18)\n",
    "            goal_x = radius * np.cos(angles[i])\n",
    "            goal_y = radius * np.sin(angles[i])\n",
    "            self.goals.append(np.array([goal_x, goal_y]))\n",
    "        \n",
    "        # Choose one goal as the true goal\n",
    "        self.true_goal_idx = np.random.randint(0, self.num_goals)\n",
    "        self.true_goal = self.goals[self.true_goal_idx]\n",
    "        \n",
    "        # Generate random obstacles (simplified as positions to avoid)\n",
    "        self.obstacles = []\n",
    "        for _ in range(2):  # 2 obstacles\n",
    "            obs_x = np.random.uniform(-0.15, 0.15)\n",
    "            obs_y = np.random.uniform(-0.15, 0.15)\n",
    "            radius = np.random.uniform(0.02, 0.04)\n",
    "            \n",
    "            # Ensure obstacles don't overlap with goals\n",
    "            valid = True\n",
    "            for goal in self.goals:\n",
    "                if np.linalg.norm(goal - np.array([obs_x, obs_y])) < radius + self.goal_radius:\n",
    "                    valid = False\n",
    "                    break\n",
    "            \n",
    "            if valid:\n",
    "                self.obstacles.append((np.array([obs_x, obs_y]), radius))\n",
    "        \n",
    "        # Track previous distance for progress reward\n",
    "        self.prev_distance = np.linalg.norm(self._get_end_effector_position() - self.true_goal)\n",
    "        \n",
    "        # Augment observation with goal positions\n",
    "        augmented_obs = self._augment_observation(obs)\n",
    "        return augmented_obs, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, _, terminated, truncated, info = self.env.step(action)\n",
    "        \n",
    "        # Get current end effector position\n",
    "        ee_pos = self._get_end_effector_position()\n",
    "        \n",
    "        # Check if the end effector reached the true goal\n",
    "        distance_to_goal = np.linalg.norm(ee_pos - self.true_goal)\n",
    "        goal_reached = distance_to_goal < self.goal_radius\n",
    "        \n",
    "        # Check for collision with obstacles\n",
    "        collision = False\n",
    "        for obs_pos, obs_radius in self.obstacles:\n",
    "            if np.linalg.norm(ee_pos - obs_pos) < obs_radius:\n",
    "                collision = True\n",
    "                break\n",
    "        \n",
    "        # Custom reward function\n",
    "        reward = self._compute_reward(ee_pos, action, goal_reached, collision)\n",
    "        \n",
    "        # Override termination conditions\n",
    "        done = goal_reached or collision\n",
    "        \n",
    "        # Augment observation with goal positions\n",
    "        augmented_obs = self._augment_observation(obs)\n",
    "        \n",
    "        # Update info dictionary\n",
    "        info['true_goal'] = self.true_goal\n",
    "        info['goal_reached'] = goal_reached\n",
    "        info['collision'] = collision\n",
    "        info['distance_to_goal'] = distance_to_goal\n",
    "        \n",
    "        return augmented_obs, reward, done, truncated, info\n",
    "    \n",
    "    def _get_end_effector_position(self):\n",
    "        \"\"\"Get the end effector position from the environment\"\"\"\n",
    "        return self.env._get_end_effector_position()\n",
    "    \n",
    "    def _compute_reward(self, ee_pos, action, goal_reached, collision):\n",
    "        \"\"\"Compute reward based on components - IMPROVED\"\"\"\n",
    "        # Initialize reward\n",
    "        reward = 0\n",
    "        \n",
    "        # 1. Collision penalty\n",
    "        if collision:\n",
    "            reward -= COLLISION_WEIGHT\n",
    "            return reward  # Early return on collision\n",
    "        \n",
    "        # 2. Distance to true goal\n",
    "        distance_to_goal = np.linalg.norm(ee_pos - self.true_goal)\n",
    "        \n",
    "        # 3. Goal reached bonus - now using GOAL_WEIGHT\n",
    "        if goal_reached:\n",
    "            reward += GOAL_WEIGHT\n",
    "        \n",
    "        # 4. Progress reward (encourage moving toward the goal)\n",
    "        progress = self.prev_distance - distance_to_goal\n",
    "        reward += PROGRESS_WEIGHT * progress\n",
    "        \n",
    "        # Store current distance for next step progress calculation\n",
    "        self.prev_distance = distance_to_goal\n",
    "        \n",
    "        # 5. Action penalty (to encourage smooth actions)\n",
    "        action_penalty = 0.1 * np.square(action).sum()\n",
    "        reward -= action_penalty\n",
    "        \n",
    "        # 6. Proximity reward (higher reward when closer to goal)\n",
    "        proximity_reward = PROXIMITY_WEIGHT * np.exp(-5.0 * distance_to_goal)\n",
    "        reward += proximity_reward\n",
    "        \n",
    "        # 7. Far penalty (discourage being too far from goal)\n",
    "        if distance_to_goal > 0.2:\n",
    "            far_penalty = FAR_WEIGHT * (distance_to_goal - 0.2)\n",
    "            reward -= far_penalty\n",
    "        \n",
    "        # 8. Autonomy reward (consistent with defined constants)\n",
    "        if distance_to_goal < 0.15:  # More autonomy as we get closer to goal\n",
    "            reward += AUTONOMY_WEIGHT * (0.15 - distance_to_goal) / 0.15\n",
    "        \n",
    "        # Normalize reward to avoid extreme values\n",
    "        reward = np.clip(reward, -20.0, 20.0)\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def _augment_observation(self, obs):\n",
    "        \"\"\"Concatenate goal positions to the observation\"\"\"\n",
    "        goal_info = np.concatenate([goal for goal in self.goals])\n",
    "        return np.concatenate([obs, goal_info])\n",
    "    \n",
    "    def render(self):\n",
    "        frame = self.env.render()\n",
    "        if frame is None:\n",
    "            return None\n",
    "        \n",
    "        if self.env.pygame is None:\n",
    "            return frame\n",
    "        \n",
    "        # Draw goals and obstacles on the frame\n",
    "        def world_to_screen(point):\n",
    "            scale = self.env.window_size / 2.5\n",
    "            screen_x = int(point[0] * scale + self.env.window_size / 2)\n",
    "            screen_y = int(-point[1] * scale + self.env.window_size / 2)\n",
    "            return (screen_x, screen_y)\n",
    "        \n",
    "        # Draw goals\n",
    "        for i, goal in enumerate(self.goals):\n",
    "            goal_pos = world_to_screen(goal)\n",
    "            color = (255, 215, 0) if i == self.true_goal_idx else (200, 200, 200)\n",
    "            self.env.pygame.draw.circle(self.env.screen, color, goal_pos, int(self.goal_radius * self.env.window_size / 2.5))\n",
    "        \n",
    "        # Draw obstacles\n",
    "        for obs_pos, obs_radius in self.obstacles:\n",
    "            obs_screen_pos = world_to_screen(obs_pos)\n",
    "            self.env.pygame.draw.circle(\n",
    "                self.env.screen,\n",
    "                (100, 100, 100),\n",
    "                obs_screen_pos,\n",
    "                int(obs_radius * self.env.window_size / 2.5)\n",
    "            )\n",
    "        \n",
    "        return np.transpose(np.array(self.env.pygame.surfarray.pixels3d(self.env.screen)), axes=(1, 0, 2))\n",
    "\n",
    "\n",
    "# IMPROVED: Completely redesigned Bayesian Inference Module with better numerical stability\n",
    "class BayesianInferenceModule(nn.Module):\n",
    "    def __init__(self, num_goals=3, beta=5.0, angular_weight=0.6, distance_weight=0.4, smooth_alpha=0.5):\n",
    "        super(BayesianInferenceModule, self).__init__()\n",
    "        self.num_goals = num_goals\n",
    "        \n",
    "        # Define trainable parameters with more stable initialization\n",
    "        self.beta = nn.Parameter(torch.tensor(float(beta), dtype=torch.float32))\n",
    "        self.angular_weight = nn.Parameter(torch.tensor(float(angular_weight), dtype=torch.float32))\n",
    "        self.distance_weight = nn.Parameter(torch.tensor(float(distance_weight), dtype=torch.float32))\n",
    "        self.smooth_alpha = nn.Parameter(torch.tensor(float(smooth_alpha), dtype=torch.float32))\n",
    "        \n",
    "        # Prior probabilities (initialized as uniform)\n",
    "        self.register_buffer('prior', torch.ones(num_goals, dtype=torch.float32) / num_goals)\n",
    "        \n",
    "        # Store belief history\n",
    "        self.beliefs = None\n",
    "        \n",
    "        # Add confidence network for better belief updates\n",
    "        self.confidence_net = nn.Sequential(\n",
    "            nn.Linear(4, 16),  # Increased network capacity\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(16),  # Added layer normalization for stability\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(8),\n",
    "            nn.Linear(8, 1),\n",
    "            nn.Sigmoid()  # Output between 0 and 1\n",
    "        )\n",
    "        \n",
    "        # Initialize weights properly\n",
    "        for m in self.confidence_net.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.orthogonal_(m.weight, gain=1.0)  # Better initialization\n",
    "                nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, state, action, goals, prev_belief=None):\n",
    "        \"\"\"\n",
    "        Update belief over goals based on observed state and action.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state (end effector position)\n",
    "            action: Human action\n",
    "            goals: List of potential goal positions\n",
    "            prev_belief: Previous belief distribution\n",
    "            \n",
    "        Returns:\n",
    "            Updated belief over goals\n",
    "        \"\"\"\n",
    "        # Ensure all input tensors are on the same device\n",
    "        device = state.device\n",
    "        \n",
    "        if prev_belief is None:\n",
    "            belief = self.prior.clone().to(device)\n",
    "        else:\n",
    "            belief = prev_belief.clone().to(device)\n",
    "        \n",
    "        # Constrain parameters to valid ranges to avoid numerical issues\n",
    "        constrained_beta = torch.sigmoid(self.beta) * 10.0 + 1.0  # Range [1.0, 11.0]\n",
    "        constrained_angular_weight = torch.sigmoid(self.angular_weight)\n",
    "        constrained_distance_weight = 1.0 - constrained_angular_weight  # Ensure weights sum to 1\n",
    "        constrained_smooth_alpha = torch.sigmoid(self.smooth_alpha) * 0.8  # Range [0, 0.8]\n",
    "        \n",
    "        # Calculate action magnitude for confidence computation\n",
    "        action_magnitude = torch.norm(action)\n",
    "        \n",
    "        # Calculate likelihoods for each goal\n",
    "        likelihoods = torch.zeros(self.num_goals, device=device)\n",
    "        \n",
    "        for i in range(self.num_goals):\n",
    "            goal = goals[i].to(device)\n",
    "            \n",
    "            # Calculate optimal action toward this goal\n",
    "            goal_direction = goal - state\n",
    "            goal_direction_norm = torch.norm(goal_direction) + 1e-8  # Avoid division by zero\n",
    "            \n",
    "            # Calculate confidence based on distance and action magnitude\n",
    "            distance_to_goal = goal_direction_norm\n",
    "            confidence_input = torch.tensor([\n",
    "                distance_to_goal.item(), \n",
    "                action_magnitude.item(),\n",
    "                distance_to_goal.item() * action_magnitude.item(),\n",
    "                1.0 / (distance_to_goal.item() + 0.1)  # Inverse distance feature\n",
    "            ], device=device).float()\n",
    "            \n",
    "            # Use confidence network\n",
    "            confidence = self.confidence_net(confidence_input).squeeze()\n",
    "            \n",
    "            # Normalize goal direction\n",
    "            optimal_action = goal_direction / goal_direction_norm\n",
    "            \n",
    "            # Calculate cost components (deviation from optimal action)\n",
    "            angular_cost = self._angular_deviation(action, optimal_action)\n",
    "            distance_cost = self._distance_deviation(action, optimal_action, goal_direction_norm)\n",
    "            \n",
    "            # Use constrained weights\n",
    "            total_cost = (constrained_angular_weight * angular_cost + \n",
    "                         constrained_distance_weight * distance_cost)\n",
    "            \n",
    "            # Scale by confidence - more confidence means cost matters more\n",
    "            scaled_cost = total_cost * confidence\n",
    "            \n",
    "            # Calculate likelihood using noisy-rational model with constrained beta\n",
    "            # Clamp the cost to avoid numerical issues\n",
    "            clamped_cost = torch.clamp(-constrained_beta * scaled_cost, min=-20.0, max=20.0)\n",
    "            likelihoods[i] = torch.exp(clamped_cost)\n",
    "        \n",
    "        # Apply softmax for better numerical stability\n",
    "        likelihoods = F.softmax(likelihoods, dim=0)\n",
    "        \n",
    "        # Bayesian update\n",
    "        posterior = belief * likelihoods\n",
    "        posterior_sum = torch.sum(posterior)\n",
    "        \n",
    "        if posterior_sum > 1e-10:\n",
    "            posterior = posterior / posterior_sum\n",
    "        else:\n",
    "            # If update fails, use a weighted combination instead\n",
    "            posterior = 0.5 * belief + 0.5 * likelihoods\n",
    "        \n",
    "        # Apply temporal smoothing with constrained alpha\n",
    "        if prev_belief is not None:\n",
    "            # Calculate adaptive smoothing factor based on action magnitude\n",
    "            # Less smoothing (lower alpha) when action is strong\n",
    "            adaptive_alpha = constrained_smooth_alpha * (1.0 - torch.tanh(action_magnitude * 0.5))\n",
    "            posterior = (1.0 - adaptive_alpha) * posterior + adaptive_alpha * prev_belief\n",
    "        \n",
    "        # Check for NaN values and replace with prior if needed\n",
    "        if torch.isnan(posterior).any() or torch.isinf(posterior).any():\n",
    "            posterior = self.prior.clone().to(device)\n",
    "        \n",
    "        return posterior\n",
    "    \n",
    "    def _angular_deviation(self, actual, optimal):\n",
    "        \"\"\"Calculate angular deviation between actual and optimal action.\"\"\"\n",
    "        actual_norm = torch.norm(actual) + 1e-8  # Avoid division by zero\n",
    "        optimal_norm = torch.norm(optimal) + 1e-8\n",
    "        \n",
    "        # Normalize vectors\n",
    "        actual_norm_vec = actual / actual_norm\n",
    "        optimal_norm_vec = optimal / optimal_norm\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        cos_sim = torch.dot(actual_norm_vec, optimal_norm_vec)\n",
    "        # Clamp to avoid numerical errors\n",
    "        cos_sim = torch.clamp(cos_sim, -1.0 + 1e-6, 1.0 - 1e-6)\n",
    "        \n",
    "        # Convert to angle\n",
    "        angle = torch.acos(cos_sim)\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        return angle / np.pi\n",
    "    \n",
    "    def _distance_deviation(self, actual, optimal, distance_to_goal):\n",
    "        \"\"\"Calculate deviation in action magnitude.\"\"\"\n",
    "        # Optimal magnitude decreases as we get closer to the goal\n",
    "        # IMPROVED: Better scaling of optimal magnitude\n",
    "        optimal_magnitude = torch.clamp(distance_to_goal * 2.0, 0.1, 1.0)\n",
    "        \n",
    "        # Calculate actual magnitude\n",
    "        actual_magnitude = torch.norm(actual)\n",
    "        \n",
    "        # Normalized deviation - IMPROVED\n",
    "        # Scale by factor relative to distance\n",
    "        scale_factor = 1.0 / (optimal_magnitude + 0.1)\n",
    "        deviation = torch.abs(actual_magnitude - optimal_magnitude) * scale_factor\n",
    "        \n",
    "        return torch.clamp(deviation, 0.0, 1.0)\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset beliefs to prior.\"\"\"\n",
    "        self.beliefs = [self.prior.clone()]\n",
    "    \n",
    "    def update_belief(self, state, action, goals):\n",
    "        \"\"\"Update belief and store history.\"\"\"\n",
    "        if self.beliefs is None:\n",
    "            self.reset()\n",
    "        \n",
    "        # Convert inputs to torch tensors if they're not already\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "        if not isinstance(action, torch.Tensor):\n",
    "            action = torch.tensor(action, dtype=torch.float32)\n",
    "        if not isinstance(goals[0], torch.Tensor):\n",
    "            goals = [torch.tensor(g, dtype=torch.float32) for g in goals]\n",
    "        \n",
    "        # Ensure all tensors are on the same device\n",
    "        device = self.beta.device  # Use a parameter's device\n",
    "        state = state.to(device)\n",
    "        action = action.to(device)\n",
    "        goals = [g.to(device) for g in goals]\n",
    "        \n",
    "        # Update belief\n",
    "        new_belief = self.forward(state, action, goals, self.beliefs[-1].to(device))\n",
    "        self.beliefs.append(new_belief)\n",
    "        \n",
    "        return new_belief\n",
    "\n",
    "\n",
    "# IMPROVED: Data generator for pretraining the Bayesian inference module\n",
    "class TrajectoryGenerator:\n",
    "    def __init__(self, num_goals=3, noise_levels=[0.1, 0.2, 0.3, 0.4, 0.5]):\n",
    "        self.num_goals = num_goals\n",
    "        self.noise_levels = noise_levels\n",
    "    \n",
    "    def generate_trajectories(self, num_trajectories, trajectory_length):\n",
    "        \"\"\"Generate trajectories for pretraining the Bayesian inference module.\"\"\"\n",
    "        trajectories = []\n",
    "        \n",
    "        # Generate trajectories with varying noise levels for better generalization\n",
    "        noise_batch_size = num_trajectories // len(self.noise_levels)\n",
    "        \n",
    "        for noise_idx, noise_level in enumerate(self.noise_levels):\n",
    "            for _ in range(noise_batch_size):\n",
    "                # Generate random goals with increased spread\n",
    "                goals = []\n",
    "                angles = np.linspace(0, 2*np.pi, self.num_goals, endpoint=False)\n",
    "                angles += np.random.uniform(0, 2*np.pi/self.num_goals)\n",
    "                \n",
    "                for i in range(self.num_goals):\n",
    "                    # Place goals in a circular pattern with randomness\n",
    "                    radius = np.random.uniform(0.12, 0.18)\n",
    "                    goal_x = radius * np.cos(angles[i])\n",
    "                    goal_y = radius * np.sin(angles[i])\n",
    "                    goals.append(np.array([goal_x, goal_y]))\n",
    "                \n",
    "                # Choose true goal\n",
    "                true_goal_idx = np.random.randint(0, self.num_goals)\n",
    "                true_goal = goals[true_goal_idx]\n",
    "                \n",
    "                # Generate random starting position\n",
    "                pos = np.random.uniform(-0.15, 0.15, size=2)\n",
    "                \n",
    "                # Generate trajectory\n",
    "                states = []\n",
    "                actions = []\n",
    "                \n",
    "                # Add waypoints for more realistic paths\n",
    "                num_waypoints = np.random.randint(1, 3)\n",
    "                waypoints = [true_goal]\n",
    "                if num_waypoints > 1:\n",
    "                    for _ in range(num_waypoints - 1):\n",
    "                        # Random waypoint near the path to the goal\n",
    "                        wp = pos + np.random.uniform(0.3, 0.7) * (true_goal - pos)\n",
    "                        wp += np.random.normal(0, 0.05, size=2)  # Add small randomness\n",
    "                        waypoints.insert(0, wp)  # Insert at beginning\n",
    "                \n",
    "                current_waypoint_idx = 0\n",
    "                \n",
    "                for _ in range(trajectory_length):\n",
    "                    # Get current target waypoint\n",
    "                    if current_waypoint_idx < len(waypoints):\n",
    "                        target = waypoints[current_waypoint_idx]\n",
    "                    else:\n",
    "                        target = true_goal\n",
    "                    \n",
    "                    # Generate optimal action toward current waypoint\n",
    "                    goal_dir = target - pos\n",
    "                    goal_dist = np.linalg.norm(goal_dir)\n",
    "                    \n",
    "                    if goal_dist > 0:\n",
    "                        optimal_action = goal_dir / goal_dist\n",
    "                        \n",
    "                        # Scale action magnitude based on distance\n",
    "                        magnitude = min(1.0, goal_dist * 2)\n",
    "                        optimal_action *= magnitude\n",
    "                    else:\n",
    "                        optimal_action = np.zeros_like(goal_dir)\n",
    "                        current_waypoint_idx += 1\n",
    "                    \n",
    "                    # Add noise to simulate human control\n",
    "                    # Add directional bias to noise\n",
    "                    bias_dir = np.random.normal(0, 1, size=2)\n",
    "                    bias_dir = bias_dir / np.linalg.norm(bias_dir) if np.linalg.norm(bias_dir) > 0 else bias_dir\n",
    "                    \n",
    "                    # Blend random noise with directional bias\n",
    "                    noise = np.random.normal(0, noise_level, size=2) + bias_dir * noise_level * 0.5\n",
    "                    \n",
    "                    # Action has a small probability of being completely random\n",
    "                    if np.random.random() < 0.05:  # 5% chance of random action\n",
    "                        action = np.random.uniform(-1, 1, size=2)\n",
    "                    else:\n",
    "                        action = optimal_action + noise\n",
    "                    \n",
    "                    # Clip to action space\n",
    "                    action = np.clip(action, -1, 1)\n",
    "                    \n",
    "                    # Store state and action\n",
    "                    states.append(pos)\n",
    "                    actions.append(action)\n",
    "                    \n",
    "                    # Update position with some damping\n",
    "                    new_pos = pos + 0.05 * action\n",
    "                    pos = 0.9 * new_pos + 0.1 * pos  # Add inertia for more realistic movement\n",
    "                    \n",
    "                    # Clip position to domain bounds\n",
    "                    pos = np.clip(pos, -0.3, 0.3)\n",
    "                    \n",
    "                    # Check if we've reached the current waypoint\n",
    "                    if current_waypoint_idx < len(waypoints):\n",
    "                        if np.linalg.norm(pos - waypoints[current_waypoint_idx]) < 0.05:\n",
    "                            current_waypoint_idx += 1\n",
    "                \n",
    "                trajectories.append({\n",
    "                    'states': states,\n",
    "                    'actions': actions,\n",
    "                    'goals': goals,\n",
    "                    'true_goal_idx': true_goal_idx\n",
    "                })\n",
    "        \n",
    "        # Fill in any remaining trajectories due to integer division\n",
    "        remaining = num_trajectories - (noise_batch_size * len(self.noise_levels))\n",
    "        if remaining > 0:\n",
    "            # Use medium noise level for remaining trajectories\n",
    "            noise_level = self.noise_levels[len(self.noise_levels) // 2]\n",
    "            \n",
    "            # Generate additional trajectories with medium noise\n",
    "            for _ in range(remaining):\n",
    "                # Same trajectory generation logic as above...\n",
    "                # Generate random goals with increased spread\n",
    "                goals = []\n",
    "                angles = np.linspace(0, 2*np.pi, self.num_goals, endpoint=False)\n",
    "                angles += np.random.uniform(0, 2*np.pi/self.num_goals)\n",
    "                \n",
    "                for i in range(self.num_goals):\n",
    "                    radius = np.random.uniform(0.12, 0.18)\n",
    "                    goal_x = radius * np.cos(angles[i])\n",
    "                    goal_y = radius * np.sin(angles[i])\n",
    "                    goals.append(np.array([goal_x, goal_y]))\n",
    "                \n",
    "                true_goal_idx = np.random.randint(0, self.num_goals)\n",
    "                true_goal = goals[true_goal_idx]\n",
    "                pos = np.random.uniform(-0.15, 0.15, size=2)\n",
    "                \n",
    "                states = []\n",
    "                actions = []\n",
    "                \n",
    "                # Add waypoints\n",
    "                num_waypoints = np.random.randint(1, 3)\n",
    "                waypoints = [true_goal]\n",
    "                if num_waypoints > 1:\n",
    "                    for _ in range(num_waypoints - 1):\n",
    "                        wp = pos + np.random.uniform(0.3, 0.7) * (true_goal - pos)\n",
    "                        wp += np.random.normal(0, 0.05, size=2)\n",
    "                        waypoints.insert(0, wp)\n",
    "                \n",
    "                current_waypoint_idx = 0\n",
    "                \n",
    "                for _ in range(trajectory_length):\n",
    "                    if current_waypoint_idx < len(waypoints):\n",
    "                        target = waypoints[current_waypoint_idx]\n",
    "                    else:\n",
    "                        target = true_goal\n",
    "                    \n",
    "                    goal_dir = target - pos\n",
    "                    goal_dist = np.linalg.norm(goal_dir)\n",
    "                    \n",
    "                    if goal_dist > 0:\n",
    "                        optimal_action = goal_dir / goal_dist\n",
    "                        magnitude = min(1.0, goal_dist * 2)\n",
    "                        optimal_action *= magnitude\n",
    "                    else:\n",
    "                        optimal_action = np.zeros_like(goal_dir)\n",
    "                        current_waypoint_idx += 1\n",
    "                    \n",
    "                    bias_dir = np.random.normal(0, 1, size=2)\n",
    "                    bias_dir = bias_dir / np.linalg.norm(bias_dir) if np.linalg.norm(bias_dir) > 0 else bias_dir\n",
    "                    \n",
    "                    noise = np.random.normal(0, noise_level, size=2) + bias_dir * noise_level * 0.5\n",
    "                    \n",
    "                    if np.random.random() < 0.05:\n",
    "                        action = np.random.uniform(-1, 1, size=2)\n",
    "                    else:\n",
    "                        action = optimal_action + noise\n",
    "                    \n",
    "                    action = np.clip(action, -1, 1)\n",
    "                    \n",
    "                    states.append(pos)\n",
    "                    actions.append(action)\n",
    "                    \n",
    "                    new_pos = pos + 0.05 * action\n",
    "                    pos = 0.9 * new_pos + 0.1 * pos\n",
    "                    pos = np.clip(pos, -0.3, 0.3)\n",
    "                    \n",
    "                    if current_waypoint_idx < len(waypoints):\n",
    "                        if np.linalg.norm(pos - waypoints[current_waypoint_idx]) < 0.05:\n",
    "                            current_waypoint_idx += 1\n",
    "                \n",
    "                trajectories.append({\n",
    "                    'states': states,\n",
    "                    'actions': actions,\n",
    "                    'goals': goals,\n",
    "                    'true_goal_idx': true_goal_idx\n",
    "                })\n",
    "            \n",
    "        return trajectories\n",
    "\n",
    "\n",
    "# Improved pretraining function with curriculum learning\n",
    "def pretrain_belief_module(module, num_trajectories=1000, trajectory_length=50, \n",
    "                           batch_size=128, num_epochs=100, lr=1e-3):\n",
    "    \"\"\"Pretrain the Bayesian inference module.\"\"\"\n",
    "    # Generate pretraining data\n",
    "    print(\"Generating pretraining data...\")\n",
    "    generator = TrajectoryGenerator(num_goals=module.num_goals, \n",
    "                                   noise_levels=[0.1, 0.2, 0.3, 0.4])\n",
    "    trajectories = generator.generate_trajectories(num_trajectories, trajectory_length)\n",
    "    \n",
    "    # Move module to device\n",
    "    device = DEVICE  # Use the global device\n",
    "    module = module.to(device)\n",
    "    \n",
    "    # Prepare optimizer with weight decay to prevent parameter explosion\n",
    "    optimizer = optim.Adam(module.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    \n",
    "    # Learning rate scheduler to reduce learning rate over time\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5, verbose=True)\n",
    "    \n",
    "    # Training loop with curriculum learning\n",
    "    print(f\"Pretraining Bayesian inference module for {num_epochs} epochs...\")\n",
    "    \n",
    "    # Split trajectories into difficulty levels based on noise\n",
    "    easy_trajectories = trajectories[:num_trajectories//3]\n",
    "    medium_trajectories = trajectories[num_trajectories//3:2*num_trajectories//3]\n",
    "    hard_trajectories = trajectories[2*num_trajectories//3:]\n",
    "    \n",
    "    # Curriculum schedule\n",
    "    curriculum_trajectories = [\n",
    "        easy_trajectories,  # First third of epochs: train on easy trajectories\n",
    "        easy_trajectories + medium_trajectories,  # Second third: add medium \n",
    "        trajectories  # Final third: all trajectories\n",
    "    ]\n",
    "    \n",
    "    # Define epoch ranges for curriculum stages\n",
    "    epoch_stage1 = num_epochs // 3\n",
    "    epoch_stage2 = 2 * num_epochs // 3\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    best_state_dict = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Select trajectories based on curriculum stage\n",
    "        if epoch < epoch_stage1:\n",
    "            current_trajectories = curriculum_trajectories[0]\n",
    "            stage_name = \"easy\"\n",
    "        elif epoch < epoch_stage2:\n",
    "            current_trajectories = curriculum_trajectories[1]\n",
    "            stage_name = \"medium\"\n",
    "        else:\n",
    "            current_trajectories = curriculum_trajectories[2]\n",
    "            stage_name = \"hard\"\n",
    "        \n",
    "        # Shuffle trajectories\n",
    "        random.shuffle(current_trajectories)\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        valid_losses = 0  # To avoid division by zero if all losses are invalid\n",
    "        \n",
    "        # Process each trajectory\n",
    "        for traj in tqdm(current_trajectories, desc=f\"Epoch {epoch+1}/{num_epochs} ({stage_name})\", leave=False):\n",
    "            # Reset belief to prior\n",
    "            belief = module.prior.clone().to(device)\n",
    "            \n",
    "            # Convert goals to tensors\n",
    "            goals = [torch.tensor(g, dtype=torch.float32, device=device) for g in traj['goals']]\n",
    "            \n",
    "            # Process each step in the trajectory\n",
    "            for t in range(len(traj['states'])):\n",
    "                state = torch.tensor(traj['states'][t], dtype=torch.float32, device=device)\n",
    "                action = torch.tensor(traj['actions'][t], dtype=torch.float32, device=device)\n",
    "                \n",
    "                # Update belief using the module\n",
    "                belief = module.forward(state, action, goals, belief)\n",
    "                \n",
    "                # Skip initial steps for loss calculation (let beliefs stabilize)\n",
    "                if t < 5:\n",
    "                    continue\n",
    "                \n",
    "                # Occasionally introduce random goals to improve robustness\n",
    "                true_goal_idx = traj['true_goal_idx']\n",
    "                if random.random() < 0.1:  # 10% of the time\n",
    "                    # Randomly swap the true goal\n",
    "                    true_goal_idx = random.randint(0, module.num_goals-1)\n",
    "                \n",
    "                # Ensure belief is valid\n",
    "                if torch.isnan(belief).any() or torch.isinf(belief).any():\n",
    "                    belief = module.prior.clone().to(device)\n",
    "                    continue\n",
    "                \n",
    "                # Smooth target distribution (label smoothing)\n",
    "                target = torch.ones_like(belief) * 0.01 / (module.num_goals - 1)\n",
    "                target[true_goal_idx] = 0.99\n",
    "                \n",
    "                # Cross entropy loss\n",
    "                epsilon = 1e-10  # Small epsilon to avoid log(0)\n",
    "                loss = -torch.sum(target * torch.log(belief + epsilon))\n",
    "                \n",
    "                # Skip backpropagation if loss is NaN or Inf\n",
    "                if torch.isnan(loss) or torch.isinf(loss):\n",
    "                    continue\n",
    "                \n",
    "                # Backpropagate\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                \n",
    "                # Gradient clipping for stability\n",
    "                torch.nn.utils.clip_grad_norm_(module.parameters(), max_norm=1.0)\n",
    "                \n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                valid_losses += 1\n",
    "                \n",
    "                # Track accuracy\n",
    "                pred_goal_idx = torch.argmax(belief).item()\n",
    "                correct_predictions += (pred_goal_idx == true_goal_idx)\n",
    "                total_predictions += 1\n",
    "                \n",
    "                # Update belief for next step (detach from computation graph)\n",
    "                belief = belief.detach()\n",
    "        \n",
    "        # Update learning rate based on validation loss\n",
    "        avg_loss = total_loss / valid_losses if valid_losses > 0 else float('inf')\n",
    "        scheduler.step(avg_loss)\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            best_state_dict = module.state_dict().copy()\n",
    "        \n",
    "        # Print progress\n",
    "        accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Load best model\n",
    "    if best_state_dict:\n",
    "        module.load_state_dict(best_state_dict)\n",
    "    \n",
    "    print(\"Pretraining complete!\")\n",
    "    return module\n",
    "\n",
    "\n",
    "# Actor-Critic Networks for PPO - IMPROVED\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256, belief_dim=None):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        # Feature extraction layers\n",
    "        self.feature_network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),  # Added normalization for stability\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),  # Added normalization for stability\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Separate belief processing if provided\n",
    "        self.belief_network = None\n",
    "        if belief_dim is not None:\n",
    "            self.belief_network = nn.Sequential(\n",
    "                nn.Linear(belief_dim, hidden_dim // 2),\n",
    "                nn.LayerNorm(hidden_dim // 2),  # Added normalization\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            \n",
    "            # Fusion layer for state and belief features\n",
    "            self.fusion_network = nn.Sequential(\n",
    "                nn.Linear(hidden_dim + hidden_dim // 2, hidden_dim),\n",
    "                nn.LayerNorm(hidden_dim),  # Added normalization\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        \n",
    "        # Actor network (policy)\n",
    "        self.actor_mean = nn.Linear(hidden_dim, action_dim)\n",
    "        self.actor_log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "        \n",
    "        # Critic network (value function)\n",
    "        self.critic = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.orthogonal_(module.weight, gain=np.sqrt(2))\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, state, belief=None):\n",
    "        # Extract features from state\n",
    "        features = self.feature_network(state)\n",
    "        \n",
    "        # Process belief and fuse with state features if available\n",
    "        if belief is not None and self.belief_network is not None:\n",
    "            belief_features = self.belief_network(belief)\n",
    "            features = torch.cat([features, belief_features], dim=-1)\n",
    "            features = self.fusion_network(features)\n",
    "        \n",
    "        # Actor: Get action distribution\n",
    "        action_mean = self.actor_mean(features)\n",
    "        \n",
    "        # Variable action std based on training progress\n",
    "        action_std = torch.exp(torch.clamp(self.actor_log_std, min=-20, max=2))\n",
    "        \n",
    "        # Critic: Get state value\n",
    "        value = self.critic(features)\n",
    "        \n",
    "        return action_mean, action_std, value\n",
    "    \n",
    "    def get_action(self, state, belief=None, deterministic=False):\n",
    "        # Move data to the model's device\n",
    "        device = next(self.parameters()).device\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        if belief is not None:\n",
    "            belief = torch.FloatTensor(belief).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action_mean, action_std, _ = self.forward(state, belief)\n",
    "            \n",
    "            if deterministic:\n",
    "                action = action_mean\n",
    "            else:\n",
    "                normal = Normal(action_mean, action_std)\n",
    "                action = normal.sample()\n",
    "                \n",
    "            return action.squeeze(0).cpu().numpy()\n",
    "    \n",
    "    def evaluate_actions(self, states, actions, beliefs=None):\n",
    "        action_mean, action_std, values = self.forward(states, beliefs)\n",
    "        \n",
    "        # Create normal distribution\n",
    "        dist = Normal(action_mean, action_std)\n",
    "        \n",
    "        # Get log probabilities\n",
    "        log_probs = dist.log_prob(actions).sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Get entropy\n",
    "        entropy = dist.entropy().sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        return log_probs, entropy, values\n",
    "\n",
    "\n",
    "# IMPROVED PPO Agent\n",
    "class PPOAgent:\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256, num_goals=3,\n",
    "                 lr=3e-4, gamma=0.99, gae_lambda=0.95, clip_epsilon=0.2,\n",
    "                 value_coef=0.5, entropy_coef=0.01, target_kl=0.01):\n",
    "        \n",
    "        # Initialize actor-critic\n",
    "        self.actor_critic = ActorCritic(\n",
    "            state_dim=state_dim,\n",
    "            action_dim=action_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            belief_dim=num_goals\n",
    "        )\n",
    "        \n",
    "        # Initialize Bayesian inference module\n",
    "        self.bayesian_module = BayesianInferenceModule(\n",
    "            num_goals=num_goals,\n",
    "            beta=BETA,\n",
    "            angular_weight=ANGULAR_WEIGHT,\n",
    "            distance_weight=DISTANCE_WEIGHT,\n",
    "            smooth_alpha=BELIEF_SMOOTH_ALPHA\n",
    "        )\n",
    "        \n",
    "        # Move models to the appropriate device\n",
    "        self.device = DEVICE\n",
    "        self.actor_critic.to(self.device)\n",
    "        self.bayesian_module.to(self.device)\n",
    "        \n",
    "        # Initialize optimizer with L2 regularization\n",
    "        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=lr, weight_decay=1e-5)\n",
    "        \n",
    "        # Add learning rate scheduler\n",
    "        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=1000, eta_min=lr/10)\n",
    "        \n",
    "        # Store hyperparameters\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.value_coef = value_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.target_kl = target_kl\n",
    "        \n",
    "        # Memory buffer for experience\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.log_probs = []\n",
    "        self.beliefs = []\n",
    "        self.dones = []\n",
    "        \n",
    "        # Tracking variables\n",
    "        self.steps_since_update = 0\n",
    "        self.epochs_trained = 0\n",
    "        self.update_count = 0\n",
    "    \n",
    "    def select_action(self, state, goals, deterministic=False):\n",
    "        \"\"\"Select action based on current state and belief.\"\"\"\n",
    "        # Get end effector position\n",
    "        ee_pos = state[-4:-2]  # Position is at the end of the observation before goal info\n",
    "        \n",
    "        # Extract goals from state if necessary\n",
    "        if goals is None:\n",
    "            # Assume the state includes goal information as per the environment wrapper\n",
    "            goals_data = state[state.shape[0] - 2*NUM_GOALS:]\n",
    "            goals = [goals_data[i:i+2] for i in range(0, len(goals_data), 2)]\n",
    "        \n",
    "        # Initialize belief if needed\n",
    "        if not hasattr(self, 'current_belief') or self.current_belief is None:\n",
    "            self.current_belief = self.bayesian_module.prior.clone().to(self.device)\n",
    "        \n",
    "        # Handle case where we don't have previous action yet (e.g., at start of episode)\n",
    "        if not hasattr(self, 'prev_action') or self.prev_action is None:\n",
    "            # Use a default no-movement action for the first update\n",
    "            self.prev_action = torch.zeros(2, device=self.device)\n",
    "        \n",
    "        # Update belief based on observed state and previous action\n",
    "        with torch.no_grad():  # No need to track gradients for inference\n",
    "            updated_belief = self.bayesian_module.forward(\n",
    "                state=torch.tensor(ee_pos, dtype=torch.float32, device=self.device),\n",
    "                action=self.prev_action,\n",
    "                goals=[torch.tensor(g, dtype=torch.float32, device=self.device) for g in goals],\n",
    "                prev_belief=self.current_belief\n",
    "            )\n",
    "            \n",
    "            # Check for NaN values in updated belief\n",
    "            if torch.isnan(updated_belief).any() or torch.isinf(updated_belief).any():\n",
    "                updated_belief = self.bayesian_module.prior.clone().to(self.device)\n",
    "            \n",
    "            self.current_belief = updated_belief\n",
    "        \n",
    "        # Select action using actor network\n",
    "        action = self.actor_critic.get_action(\n",
    "            state=state,\n",
    "            belief=self.current_belief.cpu().numpy(),\n",
    "            deterministic=deterministic\n",
    "        )\n",
    "        \n",
    "        # Store action for next belief update\n",
    "        self.prev_action = torch.tensor(action, device=self.device)\n",
    "        \n",
    "        # Check for NaN actions\n",
    "        if np.isnan(action).any() or np.isinf(action).any():\n",
    "            action = np.zeros_like(action)\n",
    "            self.prev_action = torch.zeros(2, device=self.device)\n",
    "        \n",
    "        return action, self.current_belief.cpu().numpy()\n",
    "    \n",
    "    def remember(self, state, action, reward, value, log_prob, belief, done):\n",
    "        \"\"\"Store experience in memory.\"\"\"\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.values.append(value)\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.beliefs.append(belief)\n",
    "        self.dones.append(done)\n",
    "        \n",
    "        # Track steps since last update\n",
    "        self.steps_since_update += 1\n",
    "    \n",
    "    def compute_gae(self, next_value):\n",
    "        \"\"\"Compute Generalized Advantage Estimation.\"\"\"\n",
    "        values = self.values + [next_value]\n",
    "        gae = 0\n",
    "        returns = []\n",
    "        \n",
    "        # GAE calculation\n",
    "        for step in reversed(range(len(self.rewards))):\n",
    "            delta = (self.rewards[step] + \n",
    "                     self.gamma * values[step + 1] * (1 - self.dones[step]) - \n",
    "                     values[step])\n",
    "            gae = delta + self.gamma * self.gae_lambda * (1 - self.dones[step]) * gae\n",
    "            returns.insert(0, gae + values[step])\n",
    "        \n",
    "        return returns\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"Update policy using PPO algorithm.\"\"\"\n",
    "        # Skip update if not enough data\n",
    "        if len(self.states) < BATCH_SIZE:\n",
    "            return 0.0\n",
    "            \n",
    "        # Get next value for GAE\n",
    "        with torch.no_grad():\n",
    "            if len(self.states) > 0:\n",
    "                state = torch.FloatTensor(self.states[-1]).unsqueeze(0).to(self.device)\n",
    "                belief = torch.FloatTensor(self.beliefs[-1]).unsqueeze(0).to(self.device)\n",
    "                _, _, next_value = self.actor_critic(state, belief)\n",
    "                next_value = next_value.squeeze().cpu().item()\n",
    "            else:\n",
    "                next_value = 0\n",
    "        \n",
    "        # Compute returns using GAE\n",
    "        returns = self.compute_gae(next_value)\n",
    "        \n",
    "        # Convert lists to tensors\n",
    "        states = torch.FloatTensor(np.array(self.states)).to(self.device)\n",
    "        actions = torch.FloatTensor(np.array(self.actions)).to(self.device)\n",
    "        returns = torch.FloatTensor(returns).unsqueeze(1).to(self.device)\n",
    "        old_log_probs = torch.FloatTensor(np.array(self.log_probs)).unsqueeze(1).to(self.device)\n",
    "        beliefs = torch.FloatTensor(np.array(self.beliefs)).to(self.device)\n",
    "        values = torch.FloatTensor(np.array(self.values)).unsqueeze(1).to(self.device)\n",
    "        \n",
    "        # Calculate advantages\n",
    "        advantages = returns - values\n",
    "        \n",
    "        # Normalize advantages\n",
    "        if len(advantages) > 1:\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        total_loss = 0\n",
    "        kl_divs = []\n",
    "        \n",
    "        # Mini-batch training\n",
    "        batch_size = min(BATCH_SIZE, len(self.states))\n",
    "        indices = np.arange(len(self.states))\n",
    "        \n",
    "        # Calculate adaptive clip epsilon based on training progress\n",
    "        # Start with higher clip (more exploration) and reduce over time\n",
    "        adaptive_clip = max(0.1, self.clip_epsilon * (1.0 - self.epochs_trained / 2000.0))\n",
    "        \n",
    "        # IMPROVED: Better entropy scheduling\n",
    "        # Start with higher entropy coefficient and decay over time\n",
    "        adaptive_entropy_coef = max(0.001, self.entropy_coef * (1.0 - self.epochs_trained / 1500.0))\n",
    "        \n",
    "        # Randomize and shuffle batches\n",
    "        np.random.shuffle(indices)\n",
    "        batches = [indices[i:i + batch_size] for i in range(0, len(indices), batch_size)]\n",
    "        \n",
    "        for _ in range(PPO_EPOCHS):\n",
    "            # Shuffle batches\n",
    "            np.random.shuffle(batches)\n",
    "            \n",
    "            for batch_indices in batches:\n",
    "                # Evaluate actions again\n",
    "                new_log_probs, entropy, values = self.actor_critic.evaluate_actions(\n",
    "                    states[batch_indices],\n",
    "                    actions[batch_indices],\n",
    "                    beliefs[batch_indices]\n",
    "                )\n",
    "                \n",
    "                # Compute ratio\n",
    "                ratio = torch.exp(new_log_probs - old_log_probs[batch_indices])\n",
    "                \n",
    "                # Clamp ratio to avoid numerical instability\n",
    "                ratio = torch.clamp(ratio, 0.0, 10.0)\n",
    "                \n",
    "                # Compute surrogate losses\n",
    "                surrogate1 = ratio * advantages[batch_indices]\n",
    "                surrogate2 = torch.clamp(ratio, 1.0 - adaptive_clip, 1.0 + adaptive_clip) * advantages[batch_indices]\n",
    "                \n",
    "                # Value loss\n",
    "                value_loss = F.mse_loss(values, returns[batch_indices])\n",
    "                \n",
    "                # Policy loss\n",
    "                policy_loss = -torch.min(surrogate1, surrogate2).mean()\n",
    "                \n",
    "                # Entropy bonus\n",
    "                entropy_loss = -entropy.mean()\n",
    "                \n",
    "                # Total loss with value weighting that starts lower and increases over time\n",
    "                adaptive_value_coef = min(1.0, self.value_coef * (1.0 + self.epochs_trained / 1000.0))\n",
    "                loss = policy_loss + adaptive_value_coef * value_loss + adaptive_entropy_coef * entropy_loss\n",
    "                \n",
    "                # Update parameters\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # Gradient clipping for stability\n",
    "                nn.utils.clip_grad_norm_(self.actor_critic.parameters(), max_norm=0.5)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # Check KL divergence for early stopping\n",
    "                with torch.no_grad():\n",
    "                    approx_kl = ((old_log_probs[batch_indices] - new_log_probs).exp() - 1 - \n",
    "                                (old_log_probs[batch_indices] - new_log_probs)).mean().item()\n",
    "                    kl_divs.append(approx_kl)\n",
    "                \n",
    "                if approx_kl > self.target_kl:\n",
    "                    break\n",
    "            \n",
    "            # Early stopping based on average KL divergence\n",
    "            if np.mean(kl_divs) > self.target_kl:\n",
    "                break\n",
    "        \n",
    "        # Update learning rate periodically\n",
    "        self.scheduler.step()\n",
    "        \n",
    "        # Increment epoch count\n",
    "        self.epochs_trained += 1\n",
    "        self.update_count += 1\n",
    "        \n",
    "        # Reset memory buffer\n",
    "        self.clear_memory()\n",
    "        \n",
    "        # Return average loss\n",
    "        return total_loss / (len(batches) * PPO_EPOCHS)\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        \"\"\"Clear memory buffer after updates.\"\"\"\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.log_probs = []\n",
    "        self.beliefs = []\n",
    "        self.dones = []\n",
    "        self.steps_since_update = 0\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset agent state between episodes.\"\"\"\n",
    "        self.current_belief = None\n",
    "        self.prev_action = None\n",
    "        self.bayesian_module.reset()\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"Save model parameters.\"\"\"\n",
    "        torch.save({\n",
    "            'actor_critic_state_dict': self.actor_critic.state_dict(),\n",
    "            'bayesian_module_state_dict': self.bayesian_module.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'epochs_trained': self.epochs_trained\n",
    "        }, path)\n",
    "    \n",
    "    def load(self, path):\n",
    "        \"\"\"Load model parameters.\"\"\"\n",
    "        checkpoint = torch.load(path, map_location=self.device)\n",
    "        self.actor_critic.load_state_dict(checkpoint['actor_critic_state_dict'])\n",
    "        self.bayesian_module.load_state_dict(checkpoint['bayesian_module_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        self.epochs_trained = checkpoint.get('epochs_trained', 0)\n",
    "\n",
    "\n",
    "# IMPROVED: Performance tracking with moving averages\n",
    "class PerformanceTracker:\n",
    "    def __init__(self):\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.success_rates = []\n",
    "        self.collision_rates = []\n",
    "        self.belief_accuracy = []\n",
    "        self.assistance_levels = []\n",
    "        self.losses = []\n",
    "        self.avg_returns = []\n",
    "        \n",
    "        # For visualization\n",
    "        self.frames = []\n",
    "    \n",
    "    def add_episode_metrics(self, episode_reward, episode_length, success, collision, belief_accuracy, avg_assistance):\n",
    "        self.episode_rewards.append(episode_reward)\n",
    "        self.episode_lengths.append(episode_length)\n",
    "        self.success_rates.append(1 if success else 0)\n",
    "        self.collision_rates.append(1 if collision else 0)\n",
    "        self.belief_accuracy.append(belief_accuracy)\n",
    "        self.assistance_levels.append(avg_assistance)\n",
    "    \n",
    "    def add_training_metrics(self, loss, avg_return):\n",
    "        self.losses.append(loss)\n",
    "        self.avg_returns.append(avg_return)\n",
    "    \n",
    "    def add_frame(self, frame):\n",
    "        self.frames.append(frame)\n",
    "    \n",
    "    def plot_learning_curves(self, window=15):\n",
    "        \"\"\"Plot learning curves with moving averages.\"\"\"\n",
    "        sns.set(style=\"darkgrid\")\n",
    "        \n",
    "        # Create a figure with subplots\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        \n",
    "        # Episode rewards\n",
    "        self._plot_smoothed_curve(axes[0, 0], self.episode_rewards, window, \n",
    "                                 \"Episode Rewards\", \"Episode\", \"Reward\")\n",
    "        \n",
    "        # Success rate\n",
    "        self._plot_smoothed_curve(axes[0, 1], self.success_rates, window,\n",
    "                                 \"Success Rate\", \"Episode\", \"Success Rate\", \n",
    "                                 is_rate=True)\n",
    "        \n",
    "        # Collision rate\n",
    "        self._plot_smoothed_curve(axes[0, 2], self.collision_rates, window,\n",
    "                                 \"Collision Rate\", \"Episode\", \"Collision Rate\", \n",
    "                                 is_rate=True)\n",
    "        \n",
    "        # Belief accuracy\n",
    "        self._plot_smoothed_curve(axes[1, 0], self.belief_accuracy, window,\n",
    "                                 \"Belief Accuracy\", \"Episode\", \"Accuracy\")\n",
    "        \n",
    "        # Assistance level\n",
    "        self._plot_smoothed_curve(axes[1, 1], self.assistance_levels, window,\n",
    "                                 \"Average Assistance Level\", \"Episode\", \"Assistance γ\")\n",
    "        \n",
    "        # Episode length\n",
    "        self._plot_smoothed_curve(axes[1, 2], self.episode_lengths, window,\n",
    "                                 \"Episode Length\", \"Episode\", \"Steps\")\n",
    "        \n",
    "        # Adjust layout\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"results/learning_curves.png\", dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        # Plot additional training metrics\n",
    "        self._plot_training_metrics(window)\n",
    "    \n",
    "    def _smooth_data(self, data, window):\n",
    "        \"\"\"Apply moving average smoothing to data.\"\"\"\n",
    "        if len(data) < window:\n",
    "            return np.array(data)\n",
    "        \n",
    "        smoothed = np.convolve(data, np.ones(window)/window, mode='valid')\n",
    "        return smoothed\n",
    "    \n",
    "    def _plot_smoothed_curve(self, ax, data, window, title, xlabel, ylabel, is_rate=False):\n",
    "        \"\"\"Helper to plot smoothed curve with confidence interval.\"\"\"\n",
    "        if not data:\n",
    "            return\n",
    "            \n",
    "        x = np.arange(len(data))\n",
    "        y = np.array(data)\n",
    "        \n",
    "        # Plot raw data with low alpha\n",
    "        ax.plot(x, y, alpha=0.2, color='blue', label='Raw')\n",
    "        \n",
    "        # Apply smoothing for moving average if sufficient data\n",
    "        if len(x) > window:\n",
    "            smoothed_y = self._smooth_data(y, window)\n",
    "            smoothed_x = np.arange(window-1, len(x))\n",
    "            \n",
    "            # Plot moving average\n",
    "            ax.plot(smoothed_x, smoothed_y, alpha=1.0, color='blue', linewidth=2, label=f'Moving Avg (window={window})')\n",
    "            \n",
    "            # Compute rolling std for confidence interval\n",
    "            rolling_std = [np.std(y[max(0, i-window):i+1]) for i in range(window-1, len(y))]\n",
    "            rolling_std = np.array(rolling_std)\n",
    "            \n",
    "            # Plot confidence intervals\n",
    "            ax.fill_between(smoothed_x, \n",
    "                           np.maximum(0, smoothed_y - rolling_std), \n",
    "                           np.minimum(1 if is_rate else float('inf'), smoothed_y + rolling_std), \n",
    "                           alpha=0.2, color='blue')\n",
    "        \n",
    "        # Add title and labels\n",
    "        ax.set_title(title, fontsize=14)\n",
    "        ax.set_xlabel(xlabel, fontsize=12)\n",
    "        ax.set_ylabel(ylabel, fontsize=12)\n",
    "        \n",
    "        # Set y-axis limits for rate plots\n",
    "        if is_rate:\n",
    "            ax.set_ylim([-0.05, 1.05])\n",
    "        \n",
    "        # Add legend if we have both raw and smoothed data\n",
    "        if len(x) > window:\n",
    "            ax.legend(loc='best')\n",
    "    \n",
    "    def _plot_training_metrics(self, window=15):\n",
    "        \"\"\"Plot additional training metrics.\"\"\"\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        # Losses\n",
    "        self._plot_smoothed_curve(axes[0], self.losses, window,\n",
    "                                 \"Training Loss\", \"Update\", \"Loss\")\n",
    "        \n",
    "        # Average returns\n",
    "        self._plot_smoothed_curve(axes[1], self.avg_returns, window,\n",
    "                                 \"Average Returns\", \"Update\", \"Return\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"results/training_metrics.png\", dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_ablation_study(self, results_dict):\n",
    "        \"\"\"Plot ablation study results.\"\"\"\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "        \n",
    "        # Process data for plotting\n",
    "        methods = list(results_dict.keys())\n",
    "        success_rates = [results_dict[m]['success_rate'] for m in methods]\n",
    "        episode_rewards = [results_dict[m]['episode_reward'] for m in methods]\n",
    "        completion_times = [results_dict[m]['completion_time'] for m in methods]\n",
    "        \n",
    "        # Plot success rates\n",
    "        axes[0].bar(methods, success_rates, color='skyblue')\n",
    "        axes[0].set_title('Success Rate by Method', fontsize=14)\n",
    "        axes[0].set_ylabel('Success Rate', fontsize=12)\n",
    "        axes[0].set_ylim([0, 1.0])\n",
    "        \n",
    "        # Plot episode rewards\n",
    "        axes[1].bar(methods, episode_rewards, color='lightgreen')\n",
    "        axes[1].set_title('Average Episode Reward by Method', fontsize=14)\n",
    "        axes[1].set_ylabel('Reward', fontsize=12)\n",
    "        \n",
    "        # Plot completion times\n",
    "        axes[2].bar(methods, completion_times, color='salmon')\n",
    "        axes[2].set_title('Average Completion Time by Method', fontsize=14)\n",
    "        axes[2].set_ylabel('Steps', fontsize=12)\n",
    "        \n",
    "        # Add values on top of bars\n",
    "        for ax, data in zip(axes, [success_rates, episode_rewards, completion_times]):\n",
    "            for i, v in enumerate(data):\n",
    "                ax.text(i, v, f'{v:.2f}', ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"results/ablation_study.png\", dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    def save_video(self, filename=\"episode\", fps=30):\n",
    "        \"\"\"Save frames as a video.\"\"\"\n",
    "        if not self.frames:\n",
    "            return\n",
    "            \n",
    "        # Create writer\n",
    "        frames = [frame for frame in self.frames]\n",
    "        \n",
    "        # Check if frames are valid\n",
    "        if not frames or frames[0] is None:\n",
    "            print(\"No valid frames to save.\")\n",
    "            return\n",
    "            \n",
    "        height, width, _ = frames[0].shape\n",
    "        \n",
    "        # Create video writer\n",
    "        video_path = f\"results/{filename}.mp4\"\n",
    "        \n",
    "        # Save using matplotlib animation\n",
    "        fig = plt.figure(figsize=(width/100, height/100), dpi=100)\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.set_axis_off()\n",
    "        \n",
    "        images = [[ax.imshow(frame)] for frame in frames]\n",
    "        anim = animation.ArtistAnimation(fig, images, interval=1000/fps, blit=True)\n",
    "        \n",
    "        try:\n",
    "            # Use Pillow writer instead of ffmpeg\n",
    "            anim.save(video_path, writer='pillow', fps=fps)\n",
    "            print(f\"Video saved to {video_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving video: {e}\")\n",
    "            # Try to save individual frames instead\n",
    "            try:\n",
    "                os.makedirs(f\"results/{filename}_frames\", exist_ok=True)\n",
    "                for i, frame in enumerate(frames):\n",
    "                    plt.imsave(f\"results/{filename}_frames/frame_{i:04d}.png\", frame)\n",
    "                print(f\"Saved {len(frames)} individual frames to results/{filename}_frames folder\")\n",
    "            except Exception as e2:\n",
    "                print(f\"Error saving frames: {e2}\")\n",
    "        \n",
    "        plt.close()\n",
    "    \n",
    "    def clear_frames(self):\n",
    "        \"\"\"Clear saved frames to free memory.\"\"\"\n",
    "        self.frames = []\n",
    "\n",
    "\n",
    "# IMPROVED: Training loop with better monitoring and stability\n",
    "def train(env, agent, num_episodes=5000, max_steps=1000, eval_interval=200, visualize=False, visualize_interval=2000):\n",
    "    \"\"\"Train the agent using PPO.\"\"\"\n",
    "    tracker = PerformanceTracker()\n",
    "    total_steps = 0\n",
    "    update_counter = 0\n",
    "    best_success_rate = 0\n",
    "    best_reward = -float('inf')\n",
    "    episodes_without_improvement = 0\n",
    "    early_stop_patience = EARLY_STOP_PATIENCE  # Increased patience\n",
    "    \n",
    "    # Initialize evaluation metrics\n",
    "    eval_metrics = {\n",
    "        'success_rate': [],\n",
    "        'episode_length': [],\n",
    "        'episode_reward': [],\n",
    "        'belief_accuracy': [],\n",
    "        'collision_rate': []\n",
    "    }\n",
    "    \n",
    "    # For visualization\n",
    "    if visualize:\n",
    "        print(f\"Training with visualization every {visualize_interval} steps\")\n",
    "    \n",
    "    # Training loop\n",
    "    for episode in tqdm(range(1, num_episodes + 1), desc=\"Training\"):\n",
    "        state, _ = env.reset()\n",
    "        agent.reset()\n",
    "        \n",
    "        episode_reward = 0\n",
    "        episode_length = 0\n",
    "        episode_beliefs = []\n",
    "        episode_true_goals = []\n",
    "        episode_assistance = []\n",
    "        \n",
    "        # Record frames for visualization\n",
    "        record_video = visualize and (episode % eval_interval == 0 or \n",
    "                                     (total_steps % visualize_interval < max_steps))\n",
    "        \n",
    "        # Extract goals from the environment\n",
    "        goals = []\n",
    "        for i in range(NUM_GOALS):\n",
    "            goal_idx = state.shape[0] - 2 * NUM_GOALS + 2 * i\n",
    "            goals.append(state[goal_idx:goal_idx + 2])\n",
    "        \n",
    "        # Episode loop\n",
    "        for step in range(max_steps):\n",
    "            # Render for visualization\n",
    "            if record_video:\n",
    "                frame = env.render()\n",
    "                if frame is not None:\n",
    "                    tracker.add_frame(frame)\n",
    "            \n",
    "            # Select action\n",
    "            action, belief = agent.select_action(state, goals)\n",
    "            \n",
    "            # Check for NaN values in action\n",
    "            if np.isnan(action).any() or np.isinf(action).any():\n",
    "                print(f\"Warning: NaN or Inf detected in action! Using zero action instead.\")\n",
    "                action = np.zeros_like(action)\n",
    "            \n",
    "            # Get action log probability and value\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(agent.device)\n",
    "                belief_tensor = torch.FloatTensor(belief).unsqueeze(0).to(agent.device)\n",
    "                action_tensor = torch.FloatTensor(action).unsqueeze(0).to(agent.device)\n",
    "                \n",
    "                log_prob, _, value = agent.actor_critic.evaluate_actions(\n",
    "                    state_tensor, action_tensor, belief_tensor\n",
    "                )\n",
    "                log_prob = log_prob.squeeze().cpu().item()\n",
    "                value = value.squeeze().cpu().item()\n",
    "            \n",
    "            # Take action in environment\n",
    "            next_state, reward, done, truncated, info = env.step(action)\n",
    "            \n",
    "            # Store experience\n",
    "            agent.remember(state, action, reward, value, log_prob, belief, done)\n",
    "            \n",
    "            # Update state and counters\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            episode_length += 1\n",
    "            total_steps += 1\n",
    "            \n",
    "            # Store belief accuracy and assistance level\n",
    "            true_goal_idx = env.true_goal_idx\n",
    "            \n",
    "            # Compute belief accuracy\n",
    "            predicted_goal_idx = np.argmax(belief)\n",
    "            episode_beliefs.append(predicted_goal_idx == true_goal_idx)\n",
    "            episode_true_goals.append(true_goal_idx)\n",
    "            \n",
    "            # Estimate assistance level (gamma) as max belief probability\n",
    "            max_belief = np.max(belief)\n",
    "            episode_assistance.append(max_belief)\n",
    "            \n",
    "            # Update policy if enough steps have been taken\n",
    "            if agent.steps_since_update >= STEPS_PER_UPDATE:\n",
    "                loss = agent.update()\n",
    "                tracker.add_training_metrics(loss, np.mean(agent.rewards) if agent.rewards else 0)\n",
    "                update_counter += 1\n",
    "            \n",
    "            # Check if episode is done\n",
    "            if done or truncated:\n",
    "                break\n",
    "        \n",
    "        # Calculate episode metrics\n",
    "        success = info.get('goal_reached', False)\n",
    "        collision = info.get('collision', False)\n",
    "        belief_accuracy = np.mean(episode_beliefs) if episode_beliefs else 0\n",
    "        avg_assistance = np.mean(episode_assistance) if episode_assistance else 0\n",
    "        \n",
    "        # Add episode metrics to tracker\n",
    "        tracker.add_episode_metrics(\n",
    "            episode_reward, episode_length, success, collision, \n",
    "            belief_accuracy, avg_assistance\n",
    "        )\n",
    "        \n",
    "        # Log progress\n",
    "        if episode % 100 == 0:\n",
    "            mean_reward = np.mean(tracker.episode_rewards[-100:]) if tracker.episode_rewards else 0\n",
    "            mean_success = np.mean(tracker.success_rates[-100:]) if tracker.success_rates else 0\n",
    "            mean_belief = np.mean(tracker.belief_accuracy[-100:]) if tracker.belief_accuracy else 0\n",
    "            \n",
    "            print(f\"Episode {episode}, Steps: {total_steps}, Avg reward: {mean_reward:.2f}, \" \n",
    "                  f\"Success rate: {mean_success:.2f}, Belief accuracy: {mean_belief:.2f}\")\n",
    "            \n",
    "            # Plot current learning curves periodically\n",
    "            tracker.plot_learning_curves()\n",
    "        \n",
    "        # Evaluate agent periodically\n",
    "        if episode % eval_interval == 0:\n",
    "            eval_results = evaluate(env, agent, num_episodes=20, max_steps=max_steps)\n",
    "            \n",
    "            for key in eval_metrics:\n",
    "                eval_metrics[key].append(eval_results[key])\n",
    "            \n",
    "            # Check for improvement\n",
    "            if eval_results['success_rate'] > best_success_rate:\n",
    "                best_success_rate = eval_results['success_rate']\n",
    "                agent.save(\"results/best_model_success.pt\")\n",
    "                episodes_without_improvement = 0\n",
    "                print(f\"New best success rate: {best_success_rate:.4f}\")\n",
    "            elif eval_results['episode_reward'] > best_reward:\n",
    "                best_reward = eval_results['episode_reward']\n",
    "                agent.save(\"results/best_model_reward.pt\")\n",
    "                episodes_without_improvement = 0\n",
    "                print(f\"New best reward: {best_reward:.4f}\")\n",
    "            else:\n",
    "                episodes_without_improvement += 1\n",
    "                print(f\"No improvement for {episodes_without_improvement} evaluations\")\n",
    "                \n",
    "            # Early stopping check\n",
    "            if episodes_without_improvement >= early_stop_patience:\n",
    "                print(f\"No improvement for {early_stop_patience} evaluations. Stopping training.\")\n",
    "                break\n",
    "                \n",
    "            # Save video of the best evaluation episode\n",
    "            if visualize and tracker.frames:\n",
    "                tracker.save_video(f\"episode_{episode}\")\n",
    "                tracker.clear_frames()\n",
    "    \n",
    "    # Final evaluation and visualization\n",
    "    print(\"Training complete. Running final evaluation...\")\n",
    "    final_eval = evaluate(env, agent, num_episodes=30, max_steps=max_steps, visualize=visualize)\n",
    "    \n",
    "    # Save final model\n",
    "    agent.save(\"results/final_model.pt\")\n",
    "    \n",
    "    # Plot final learning curves\n",
    "    tracker.plot_learning_curves()\n",
    "    \n",
    "    # Run ablation study\n",
    "    run_ablation_study(env, agent, tracker, visualize=visualize)\n",
    "    \n",
    "    # Print final results\n",
    "    print(\"\\nFinal Evaluation Results:\")\n",
    "    for key, value in final_eval.items():\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "    \n",
    "    return tracker, final_eval\n",
    "\n",
    "\n",
    "# IMPROVED: Evaluation function with deterministic policy\n",
    "def evaluate(env, agent, num_episodes=10, max_steps=1000, visualize=False):\n",
    "    \"\"\"Evaluate the agent's performance.\"\"\"\n",
    "    eval_rewards = []\n",
    "    eval_lengths = []\n",
    "    eval_successes = []\n",
    "    eval_collisions = []\n",
    "    eval_belief_accuracy = []\n",
    "    \n",
    "    tracker = PerformanceTracker()\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        agent.reset()\n",
    "        \n",
    "        episode_reward = 0\n",
    "        episode_length = 0\n",
    "        episode_beliefs = []\n",
    "        \n",
    "        # Extract goals from the environment\n",
    "        goals = []\n",
    "        for i in range(NUM_GOALS):\n",
    "            goal_idx = state.shape[0] - 2 * NUM_GOALS + 2 * i\n",
    "            goals.append(state[goal_idx:goal_idx + 2])\n",
    "        \n",
    "        # Episode loop\n",
    "        for step in range(max_steps):\n",
    "            # Record video if requested\n",
    "            if visualize:\n",
    "                frame = env.render()\n",
    "                if frame is not None:\n",
    "                    tracker.add_frame(frame)\n",
    "            \n",
    "            # Select action (deterministic for evaluation)\n",
    "            action, belief = agent.select_action(state, goals, deterministic=True)\n",
    "            \n",
    "            # Check for NaN values in action\n",
    "            if np.isnan(action).any() or np.isinf(action).any():\n",
    "                action = np.zeros_like(action)\n",
    "            \n",
    "            # Take action\n",
    "            next_state, reward, done, truncated, info = env.step(action)\n",
    "            \n",
    "            # Update state and counters\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            episode_length += 1\n",
    "            \n",
    "            # Track belief accuracy\n",
    "            true_goal_idx = env.true_goal_idx\n",
    "            predicted_goal_idx = np.argmax(belief)\n",
    "            episode_beliefs.append(predicted_goal_idx == true_goal_idx)\n",
    "            \n",
    "            if done or truncated:\n",
    "                break\n",
    "        \n",
    "        # Calculate episode metrics\n",
    "        success = info.get('goal_reached', False)\n",
    "        collision = info.get('collision', False)\n",
    "        belief_accuracy = np.mean(episode_beliefs) if episode_beliefs else 0\n",
    "        \n",
    "        # Store metrics\n",
    "        eval_rewards.append(episode_reward)\n",
    "        eval_lengths.append(episode_length)\n",
    "        eval_successes.append(1 if success else 0)\n",
    "        eval_collisions.append(1 if collision else 0)\n",
    "        eval_belief_accuracy.append(belief_accuracy)\n",
    "    \n",
    "    # Save video of the evaluation\n",
    "    if visualize and tracker.frames:\n",
    "        tracker.save_video(\"evaluation\")\n",
    "        tracker.clear_frames()\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    results = {\n",
    "        'episode_reward': np.mean(eval_rewards) if eval_rewards else 0,\n",
    "        'episode_length': np.mean(eval_lengths) if eval_lengths else 0,\n",
    "        'success_rate': np.mean(eval_successes) if eval_successes else 0,\n",
    "        'collision_rate': np.mean(eval_collisions) if eval_collisions else 0,\n",
    "        'belief_accuracy': np.mean(eval_belief_accuracy) if eval_belief_accuracy else 0,\n",
    "        'completion_time': np.mean([l for l, s in zip(eval_lengths, eval_successes) if s]) if any(eval_successes) else float('inf')\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# IMPROVED: Ablation study with more realistic human simulation\n",
    "def run_ablation_study(env, agent, tracker, visualize=False):\n",
    "    \"\"\"Run ablation studies to compare different approaches.\"\"\"\n",
    "    print(\"Running ablation studies...\")\n",
    "    \n",
    "    # Define methods to compare\n",
    "    methods = {\n",
    "        'Full Model': agent,  # Our integrated approach\n",
    "        'No Assistance': None,  # Manual control only\n",
    "        'Fixed (0.5)': None,  # Fixed blending parameter\n",
    "        'MAP Selection': None,  # Assist toward most likely goal\n",
    "    }\n",
    "    \n",
    "    # Create human simulation agent for more realistic tests\n",
    "    class HumanSimulation:\n",
    "        def __init__(self, noise_level=0.2, optimal_ratio=0.8, random_action_prob=0.05):\n",
    "            self.noise_level = noise_level  # Noise in human control\n",
    "            self.optimal_ratio = optimal_ratio  # How closely human follows optimal path\n",
    "            self.random_action_prob = random_action_prob  # Probability of random action\n",
    "            \n",
    "        def get_human_action(self, state, goal):\n",
    "            \"\"\"Generate realistic simulated human action toward the goal.\"\"\"\n",
    "            # Extract end effector position\n",
    "            ee_pos = state[-4:-2]  # Position is at the end of the observation before goal info\n",
    "            \n",
    "            # Calculate direction to goal\n",
    "            goal_dir = goal - ee_pos\n",
    "            goal_dist = np.linalg.norm(goal_dir)\n",
    "            \n",
    "            # Random action with small probability\n",
    "            if np.random.random() < self.random_action_prob:\n",
    "                return np.random.uniform(-1, 1, size=2)\n",
    "            \n",
    "            # Normalize direction if non-zero\n",
    "            if goal_dist > 0:\n",
    "                optimal_dir = goal_dir / goal_dist\n",
    "            else:\n",
    "                optimal_dir = np.zeros(2)\n",
    "            \n",
    "            # Scale action magnitude based on distance\n",
    "            optimal_magnitude = min(1.0, goal_dist * 2.0)\n",
    "            optimal_action = optimal_dir * optimal_magnitude\n",
    "            \n",
    "            # Add noise to simulate human variability\n",
    "            noise = np.random.normal(0, self.noise_level, size=2)\n",
    "            \n",
    "            # Add directional bias to simulate hand tremor\n",
    "            bias_dir = np.random.normal(0, 1, size=2)\n",
    "            bias_dir = bias_dir / np.linalg.norm(bias_dir) if np.linalg.norm(bias_dir) > 0 else bias_dir\n",
    "            bias = bias_dir * self.noise_level * 0.3\n",
    "            \n",
    "            # Sometimes take suboptimal paths\n",
    "            if np.random.random() > self.optimal_ratio:\n",
    "                # Add deviation to simulate strategic errors\n",
    "                angle = np.random.uniform(-np.pi/4, np.pi/4)\n",
    "                cos_angle, sin_angle = np.cos(angle), np.sin(angle)\n",
    "                rotation = np.array([[cos_angle, -sin_angle], [sin_angle, cos_angle]])\n",
    "                optimal_dir = np.dot(rotation, optimal_dir)\n",
    "                optimal_action = optimal_dir * optimal_magnitude\n",
    "            \n",
    "            # Combine optimal action with noise and bias\n",
    "            action = optimal_action + noise + bias\n",
    "            \n",
    "            # Add some tremor when close to goal (nervousness)\n",
    "            if goal_dist < 0.1:\n",
    "                tremor = np.random.normal(0, 0.1 + 0.3 * (1 - goal_dist/0.1), size=2)\n",
    "                action += tremor\n",
    "            \n",
    "            # Clip to action space\n",
    "            return np.clip(action, -1, 1)\n",
    "    \n",
    "    # Create human simulation\n",
    "    human_sim = HumanSimulation(noise_level=0.2)\n",
    "    \n",
    "    # 1. No Assistance: γ = 0\n",
    "    class NoAssistanceAgent:\n",
    "        def __init__(self, base_agent, human_sim):\n",
    "            self.base_agent = base_agent\n",
    "            self.human_sim = human_sim\n",
    "            self.device = base_agent.device\n",
    "        \n",
    "        def select_action(self, state, goals, deterministic=False):\n",
    "            # Update belief as normal\n",
    "            _, belief = self.base_agent.select_action(state, goals, deterministic)\n",
    "            \n",
    "            # Generate simulated human action (noisy optimal action toward true goal)\n",
    "            human_action = self.human_sim.get_human_action(state, env.true_goal)\n",
    "            \n",
    "            # Return pure human action (no assistance)\n",
    "            return human_action, belief\n",
    "        \n",
    "        def reset(self):\n",
    "            self.base_agent.reset()\n",
    "    \n",
    "    # 2. Fixed blending: γ = 0.5\n",
    "    class FixedBlendingAgent:\n",
    "        def __init__(self, base_agent, human_sim, gamma=0.5):\n",
    "            self.base_agent = base_agent\n",
    "            self.human_sim = human_sim\n",
    "            self.gamma = gamma\n",
    "            self.device = base_agent.device\n",
    "        \n",
    "        def select_action(self, state, goals, deterministic=False):\n",
    "            # Update belief and get AI action\n",
    "            ai_action, belief = self.base_agent.select_action(state, goals, deterministic)\n",
    "            \n",
    "            # Generate human action\n",
    "            human_action = self.human_sim.get_human_action(state, env.true_goal)\n",
    "            \n",
    "            # Blend with fixed parameter\n",
    "            action = (1 - self.gamma) * human_action + self.gamma * ai_action\n",
    "            \n",
    "            # Check for NaN values\n",
    "            if np.isnan(action).any() or np.isinf(action).any():\n",
    "                action = human_action  # Fall back to human action\n",
    "            \n",
    "            return action, belief\n",
    "        \n",
    "        def reset(self):\n",
    "            self.base_agent.reset()\n",
    "    \n",
    "    # 3. MAP Selection: Assist toward most likely goal only\n",
    "    class MAPSelectionAgent:\n",
    "        def __init__(self, base_agent, human_sim):\n",
    "            self.base_agent = base_agent\n",
    "            self.human_sim = human_sim\n",
    "            self.device = base_agent.device\n",
    "        \n",
    "        def select_action(self, state, goals, deterministic=False):\n",
    "            # Get belief from base agent\n",
    "            _, belief = self.base_agent.select_action(state, goals, deterministic)\n",
    "            \n",
    "            # Generate human action\n",
    "            human_action = self.human_sim.get_human_action(state, env.true_goal)\n",
    "            \n",
    "            # Get most likely goal\n",
    "            map_goal_idx = np.argmax(belief)\n",
    "            map_goal = goals[map_goal_idx]\n",
    "            \n",
    "            # Generate expert action toward MAP goal\n",
    "            ee_pos = state[-4:-2]  # Position is at the end of the observation before goal info\n",
    "            goal_dir = map_goal - ee_pos\n",
    "            if np.linalg.norm(goal_dir) > 0:\n",
    "                goal_dist = np.linalg.norm(goal_dir)\n",
    "                expert_dir = goal_dir / goal_dist\n",
    "                # Scale magnitude based on distance\n",
    "                magnitude = min(1.0, goal_dist * 2.0)\n",
    "                expert_action = expert_dir * magnitude\n",
    "            else:\n",
    "                expert_action = np.zeros(2)\n",
    "            \n",
    "            # Adaptive blending based on maximum belief\n",
    "            gamma = np.max(belief)\n",
    "            action = (1 - gamma) * human_action + gamma * expert_action\n",
    "            \n",
    "            # Check for NaN values\n",
    "            if np.isnan(action).any() or np.isinf(action).any():\n",
    "                action = human_action  # Fall back to human action\n",
    "            \n",
    "            return action, belief\n",
    "        \n",
    "        def reset(self):\n",
    "            self.base_agent.reset()\n",
    "    \n",
    "    # Improved Full Model: Better handles the human simulation\n",
    "    class FullModelAgent:\n",
    "        def __init__(self, base_agent, human_sim):\n",
    "            self.base_agent = base_agent\n",
    "            self.human_sim = human_sim\n",
    "            self.device = base_agent.device\n",
    "        \n",
    "        def select_action(self, state, goals, deterministic=False):\n",
    "            # Get AI action and belief from base agent\n",
    "            ai_action, belief = self.base_agent.select_action(state, goals, deterministic)\n",
    "            \n",
    "            # Generate human action\n",
    "            human_action = self.human_sim.get_human_action(state, env.true_goal)\n",
    "            \n",
    "            # Blend actions based on confidence and proximity to goal\n",
    "            ee_pos = state[-4:-2]\n",
    "            max_belief_idx = np.argmax(belief)\n",
    "            confidence = belief[max_belief_idx]\n",
    "            \n",
    "            # Increase AI influence when close to goal or when confidence is high\n",
    "            distance_to_goal = np.linalg.norm(env.true_goal - ee_pos)\n",
    "            \n",
    "            # Adaptive assistance based on belief and distance\n",
    "            gamma = min(0.8, confidence * (1.0 + 0.5 * (1.0 - min(1.0, distance_to_goal / 0.2))))\n",
    "            \n",
    "            # Blend with adaptive gamma\n",
    "            action = (1 - gamma) * human_action + gamma * ai_action\n",
    "            \n",
    "            # Check for NaN values\n",
    "            if np.isnan(action).any() or np.isinf(action).any():\n",
    "                action = human_action  # Fall back to human action\n",
    "            \n",
    "            return action, belief\n",
    "        \n",
    "        def reset(self):\n",
    "            self.base_agent.reset()\n",
    "    \n",
    "    # Create instances of each ablated agent\n",
    "    methods['No Assistance'] = NoAssistanceAgent(agent, human_sim)\n",
    "    methods['Fixed (0.5)'] = FixedBlendingAgent(agent, human_sim, gamma=0.5)\n",
    "    methods['MAP Selection'] = MAPSelectionAgent(agent, human_sim)\n",
    "    methods['Full Model'] = FullModelAgent(agent, human_sim)  # Replace with improved version\n",
    "    \n",
    "    # Evaluate each method\n",
    "    results = {}\n",
    "    for method_name, method_agent in methods.items():\n",
    "        print(f\"Evaluating: {method_name}\")\n",
    "        \n",
    "        # Skip if no agent is provided\n",
    "        if method_agent is None:\n",
    "            continue\n",
    "        \n",
    "        # Evaluate the method\n",
    "        eval_results = evaluate(env, method_agent, num_episodes=20, visualize=visualize and method_name=='Full Model')\n",
    "        \n",
    "        # Store results\n",
    "        results[method_name] = eval_results\n",
    "        \n",
    "        print(f\"  Success rate: {eval_results['success_rate']:.4f}\")\n",
    "        print(f\"  Episode reward: {eval_results['episode_reward']:.4f}\")\n",
    "        print(f\"  Completion time: {eval_results['completion_time']:.4f}\")\n",
    "        print(f\"  Belief accuracy: {eval_results['belief_accuracy']:.4f}\")\n",
    "    \n",
    "    # Plot ablation study results\n",
    "    tracker.plot_ablation_study(results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Check if visualization is possible\n",
    "    render_mode = None\n",
    "    visualization_enabled = VISUALIZE\n",
    "    \n",
    "    if visualization_enabled:\n",
    "        try:\n",
    "            import pygame\n",
    "            pygame.init()\n",
    "            render_mode = \"rgb_array\"\n",
    "        except ImportError:\n",
    "            print(\"Warning: Pygame not found. Visualization will be disabled.\")\n",
    "            visualization_enabled = False\n",
    "    \n",
    "    # Create environment\n",
    "    env = MultiGoalReacherEnv(num_goals=NUM_GOALS, render_mode=render_mode)\n",
    "    \n",
    "    # Get dimensions\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    \n",
    "    # Create agent\n",
    "    agent = PPOAgent(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        hidden_dim=256,\n",
    "        num_goals=NUM_GOALS,\n",
    "        lr=LR,\n",
    "        gamma=GAMMA,\n",
    "        gae_lambda=GAE_LAMBDA,\n",
    "        clip_epsilon=CLIP_EPSILON,\n",
    "        value_coef=VALUE_COEF,\n",
    "        entropy_coef=ENTROPY_COEF,\n",
    "        target_kl=TARGET_KL\n",
    "    )\n",
    "    \n",
    "    # Pretrain the Bayesian inference module if enabled\n",
    "    if PRETRAIN:\n",
    "        print(\"Pretraining Bayesian inference module...\")\n",
    "        agent.bayesian_module = pretrain_belief_module(\n",
    "            agent.bayesian_module,\n",
    "            num_trajectories=PRETRAIN_TRAJECTORIES,\n",
    "            trajectory_length=PRETRAIN_TRAJECTORY_LENGTH,\n",
    "            batch_size=PRETRAIN_BATCH_SIZE,\n",
    "            num_epochs=PRETRAIN_EPOCHS,\n",
    "            lr=PRETRAIN_LR\n",
    "        )\n",
    "        print(\"Pretraining complete!\")\n",
    "    \n",
    "    # Train agent\n",
    "    tracker, final_eval = train(\n",
    "        env=env,\n",
    "        agent=agent,\n",
    "        num_episodes=NUM_EPISODES,\n",
    "        eval_interval=EVAL_INTERVAL,\n",
    "        visualize=visualization_enabled,\n",
    "        visualize_interval=VISUALIZE_INTERVAL\n",
    "    )\n",
    "    \n",
    "    # Close environment\n",
    "    env.close()\n",
    "    \n",
    "    print(\"Training and evaluation completed!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
