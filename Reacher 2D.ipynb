{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "532d3de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "==================================================\n",
      "PHASE 1: Establishing solid PPO baseline\n",
      "==================================================\n",
      "\n",
      "=== Phase 1: Training Pure PPO ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 49/2000 [00:07<04:43,  6.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50, Steps: 3680, Reward: 73.53, Success: 0.68, Length: 73.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▍         | 99/2000 [00:20<15:18,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100, Steps: 8623, Reward: 80.50, Success: 0.68, Length: 98.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   7%|▋         | 148/2000 [00:31<04:26,  6.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 150, Steps: 12849, Reward: 73.29, Success: 0.72, Length: 84.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|▉         | 198/2000 [00:42<04:24,  6.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 200, Steps: 17256, Reward: 76.88, Success: 0.64, Length: 88.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█         | 201/2000 [00:50<26:29,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best success rate: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|█         | 220/2000 [00:56<24:46,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 1 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|█▏        | 240/2000 [01:02<21:01,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 2 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|█▏        | 249/2000 [01:03<07:26,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 250, Steps: 21484, Reward: 73.67, Success: 0.60, Length: 84.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|█▎        | 261/2000 [01:12<18:05,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best success rate: 0.6600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|█▍        | 280/2000 [01:19<19:52,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best reward: 111.0942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|█▍        | 297/2000 [01:22<04:33,  6.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 300, Steps: 26020, Reward: 77.77, Success: 0.66, Length: 90.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|█▌        | 300/2000 [01:28<31:00,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 1 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|█▌        | 320/2000 [01:36<26:27,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best reward: 129.1350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|█▋        | 341/2000 [01:45<21:34,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 1 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|█▋        | 349/2000 [01:47<08:12,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 350, Steps: 31561, Reward: 86.48, Success: 0.72, Length: 110.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|█▊        | 360/2000 [01:56<29:08,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 2 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|█▉        | 381/2000 [02:05<22:45,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best reward: 134.9475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|█▉        | 399/2000 [02:08<05:47,  4.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 400, Steps: 36592, Reward: 83.51, Success: 0.70, Length: 100.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 400/2000 [02:18<57:03,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best reward: 138.3319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|██        | 420/2000 [02:29<31:10,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best reward: 156.6859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|██▏       | 440/2000 [02:39<35:06,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 1 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|██▏       | 448/2000 [02:44<21:32,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 450, Steps: 43817, Reward: 98.04, Success: 0.52, Length: 144.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|██▎       | 460/2000 [02:54<28:47,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 2 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|██▍       | 480/2000 [03:04<37:40,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 3 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▍       | 499/2000 [03:11<12:09,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 500, Steps: 50627, Reward: 103.11, Success: 0.66, Length: 136.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▌       | 500/2000 [03:21<1:24:54,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 4 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|██▌       | 520/2000 [03:37<50:50,  2.06s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best reward: 160.8415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  27%|██▋       | 540/2000 [03:45<20:56,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 1 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  27%|██▋       | 548/2000 [03:46<06:34,  3.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 550, Steps: 58796, Reward: 108.14, Success: 0.48, Length: 163.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|██▊       | 560/2000 [03:59<33:48,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 2 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|██▉       | 580/2000 [04:06<23:26,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 3 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|██▉       | 599/2000 [04:12<08:02,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 600, Steps: 65116, Reward: 93.36, Success: 0.56, Length: 126.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|███       | 601/2000 [04:21<40:25,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 4 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|███       | 620/2000 [04:28<21:39,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 5 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|███▏      | 640/2000 [04:38<19:56,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 6 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|███▏      | 649/2000 [04:42<11:49,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 650, Steps: 71864, Reward: 94.59, Success: 0.48, Length: 134.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███▎      | 660/2000 [04:57<37:29,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 7 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|███▍      | 680/2000 [05:10<47:00,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best reward: 173.8065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|███▍      | 698/2000 [05:17<06:37,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 700, Steps: 79820, Reward: 105.31, Success: 0.52, Length: 159.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|███▌      | 700/2000 [05:30<51:25,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best reward: 177.3420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|███▌      | 720/2000 [05:43<44:31,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 1 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 740/2000 [05:57<22:35,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 2 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 749/2000 [06:00<07:55,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 750, Steps: 89818, Reward: 127.98, Success: 0.46, Length: 199.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|███▊      | 761/2000 [06:16<25:57,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 3 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|███▉      | 780/2000 [06:26<19:12,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 4 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|███▉      | 798/2000 [06:31<05:32,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 800, Steps: 97210, Reward: 113.08, Success: 0.64, Length: 147.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 800/2000 [06:43<46:26,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 5 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|████      | 820/2000 [06:52<17:28,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 6 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  42%|████▏     | 840/2000 [07:07<38:51,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 7 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  42%|████▏     | 849/2000 [07:11<08:35,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 850, Steps: 105648, Reward: 109.60, Success: 0.38, Length: 168.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|████▎     | 860/2000 [07:28<30:14,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 8 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|████▍     | 880/2000 [07:42<37:26,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 9 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|████▍     | 899/2000 [07:51<09:37,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 900, Steps: 115517, Reward: 128.14, Success: 0.44, Length: 197.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|████▌     | 900/2000 [08:04<1:16:40,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 10 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|████▌     | 920/2000 [08:18<35:14,  1.96s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 11 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  47%|████▋     | 940/2000 [08:29<19:56,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 12 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  47%|████▋     | 949/2000 [08:31<04:10,  4.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 950, Steps: 124939, Reward: 123.01, Success: 0.40, Length: 188.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|████▊     | 960/2000 [08:48<22:03,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 13 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  49%|████▉     | 981/2000 [09:00<13:18,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 14 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|████▉     | 997/2000 [09:05<06:42,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000, Steps: 133700, Reward: 121.15, Success: 0.56, Length: 175.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████     | 1000/2000 [09:17<31:09,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 15 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|█████     | 1020/2000 [09:26<25:04,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 16 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|█████▏    | 1040/2000 [09:39<25:02,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 17 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|█████▏    | 1048/2000 [09:43<09:39,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1050, Steps: 141567, Reward: 100.89, Success: 0.44, Length: 157.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  53%|█████▎    | 1060/2000 [10:00<32:30,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 18 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  54%|█████▍    | 1080/2000 [10:12<16:20,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 19 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|█████▍    | 1099/2000 [10:19<04:48,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1100, Steps: 149529, Reward: 100.32, Success: 0.48, Length: 159.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|█████▌    | 1103/2000 [10:33<28:07,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 20 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|█████▌    | 1120/2000 [10:39<09:31,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 21 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  57%|█████▋    | 1141/2000 [10:54<15:33,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 22 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  57%|█████▋    | 1148/2000 [10:56<05:38,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1150, Steps: 158459, Reward: 115.96, Success: 0.48, Length: 178.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|█████▊    | 1160/2000 [11:14<19:28,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 23 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  59%|█████▉    | 1180/2000 [11:26<14:38,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 24 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|█████▉    | 1199/2000 [11:29<02:28,  5.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1200, Steps: 165530, Reward: 103.36, Success: 0.50, Length: 141.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|█████▉    | 1199/2000 [11:44<07:50,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No improvement for 25 evaluations\n",
      "Early stopping after 1200 episodes due to no improvement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Evaluation Results:\n",
      "episode_reward: 155.0599\n",
      "episode_length: 212.7400\n",
      "success_rate: 0.3400\n",
      "collision_rate: 0.3400\n",
      "completion_time: 71.5294\n",
      "Performing detailed evaluation of Phase 1 agent...\n",
      "Pure PPO Success Rate: 0.3300\n",
      "\n",
      "WARNING: Pure PPO agent performance is below target.\n",
      "Consider running Phase 1 again with different hyperparameters.\n",
      "Proceeding anyway, but results may be suboptimal.\n",
      "\n",
      "==================================================\n",
      "PHASE 2: Training with belief module\n",
      "==================================================\n",
      "\n",
      "=== Phase 2: Training with Belief Module ===\n",
      "Phase 2A: Training belief module...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 1/300 [00:00<04:10,  1.19it/s]C:\\Users\\tnlab\\AppData\\Local\\Temp\\ipykernel_23948\\1959878983.py:753: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state=torch.tensor(ee_pos, dtype=torch.float32, device=self.device),\n",
      "C:\\Users\\tnlab\\AppData\\Local\\Temp\\ipykernel_23948\\1959878983.py:755: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  goals=[torch.tensor(g, dtype=torch.float32, device=self.device) for g in goals],\n",
      "Training:   1%|          | 2/300 [00:01<03:52,  1.28it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x2 and 4x32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2223\u001b[0m\n\u001b[0;32m   2219\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining and evaluation completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 2223\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[5], line 2188\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m   2186\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPHASE 2: Training with belief module\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2187\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m-> 2188\u001b[0m belief_agent \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtrain_phase2(ppo_agent, num_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n\u001b[0;32m   2190\u001b[0m \u001b[38;5;66;03m# Evaluate assistance methods with more episodes for reliable results\u001b[39;00m\n\u001b[0;32m   2191\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 1692\u001b[0m, in \u001b[0;36mPhasedTraining.train_phase2\u001b[1;34m(self, base_agent, num_episodes)\u001b[0m\n\u001b[0;32m   1689\u001b[0m belief_pretraining_episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m\n\u001b[0;32m   1691\u001b[0m \u001b[38;5;66;03m# Custom training loop for belief module only\u001b[39;00m\n\u001b[1;32m-> 1692\u001b[0m tracker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_agent(\n\u001b[0;32m   1693\u001b[0m     agent\u001b[38;5;241m=\u001b[39mbelief_agent,\n\u001b[0;32m   1694\u001b[0m     num_episodes\u001b[38;5;241m=\u001b[39mbelief_pretraining_episodes,\n\u001b[0;32m   1695\u001b[0m     early_stop_target\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m,  \u001b[38;5;66;03m# Lower goal for this phase\u001b[39;00m\n\u001b[0;32m   1696\u001b[0m     log_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mphase2_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/belief_only\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1697\u001b[0m     custom_optimizer\u001b[38;5;241m=\u001b[39mbelief_optimizer\n\u001b[0;32m   1698\u001b[0m )\n\u001b[0;32m   1700\u001b[0m \u001b[38;5;66;03m# Phase 2B: Unfreeze actor-critic and train jointly\u001b[39;00m\n\u001b[0;32m   1701\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPhase 2B: Joint training of belief and policy...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 1817\u001b[0m, in \u001b[0;36mPhasedTraining._train_agent\u001b[1;34m(self, agent, num_episodes, early_stop_target, log_dir, custom_optimizer)\u001b[0m\n\u001b[0;32m   1814\u001b[0m \u001b[38;5;66;03m# Episode loop\u001b[39;00m\n\u001b[0;32m   1815\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps):\n\u001b[0;32m   1816\u001b[0m     \u001b[38;5;66;03m# Select action\u001b[39;00m\n\u001b[1;32m-> 1817\u001b[0m     action, belief \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mselect_action(state, goals)\n\u001b[0;32m   1819\u001b[0m     \u001b[38;5;66;03m# Check for NaN values in action\u001b[39;00m\n\u001b[0;32m   1820\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(action)\u001b[38;5;241m.\u001b[39many():\n",
      "Cell \u001b[1;32mIn[5], line 752\u001b[0m, in \u001b[0;36mPPOAgent.select_action\u001b[1;34m(self, state, goals, deterministic)\u001b[0m\n\u001b[0;32m    750\u001b[0m \u001b[38;5;66;03m# Update belief based on observed state and previous action\u001b[39;00m\n\u001b[0;32m    751\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 752\u001b[0m     updated_belief \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbelief_module\u001b[38;5;241m.\u001b[39mforward(\n\u001b[0;32m    753\u001b[0m         state\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor(ee_pos, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice),\n\u001b[0;32m    754\u001b[0m         action\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev_action,\n\u001b[0;32m    755\u001b[0m         goals\u001b[38;5;241m=\u001b[39m[torch\u001b[38;5;241m.\u001b[39mtensor(g, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m goals],\n\u001b[0;32m    756\u001b[0m         prev_belief\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_belief\n\u001b[0;32m    757\u001b[0m     )\n\u001b[0;32m    759\u001b[0m     \u001b[38;5;66;03m# Safety check\u001b[39;00m\n\u001b[0;32m    760\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(updated_belief)\u001b[38;5;241m.\u001b[39many():\n",
      "Cell \u001b[1;32mIn[5], line 507\u001b[0m, in \u001b[0;36mSimpleBeliefModule.forward\u001b[1;34m(self, state, action, goals, prev_belief)\u001b[0m\n\u001b[0;32m    504\u001b[0m state_action \u001b[38;5;241m=\u001b[39m state_action\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m# Extract features from state-action pair\u001b[39;00m\n\u001b[1;32m--> 507\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_net(state_action)\n\u001b[0;32m    509\u001b[0m \u001b[38;5;66;03m# Calculate likelihoods for each goal\u001b[39;00m\n\u001b[0;32m    510\u001b[0m likelihoods \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_goals, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x2 and 4x32)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Visualization and training parameters\n",
    "VISUALIZE = False  # Set to False to disable visualization\n",
    "VISUALIZE_INTERVAL = 2000  # Show visualization every n steps\n",
    "SAVE_VIDEOS = False  # Save videos of the trained agent\n",
    "NUM_EPISODES = 2000  # Number of training episodes\n",
    "\n",
    "# Environment parameters\n",
    "GOAL_RADIUS = 0.05\n",
    "NUM_GOALS = 3  # Number of potential goals\n",
    "\n",
    "# PPO parameters - Tuned for stable learning\n",
    "GAMMA = 0.995  # Increased for better long-term rewards\n",
    "GAE_LAMBDA = 0.95\n",
    "CLIP_EPSILON = 0.2\n",
    "ENTROPY_COEF = 0.015  # Slightly increased for better exploration\n",
    "VALUE_COEF = 0.5\n",
    "LR = 2e-4  # Slightly lower for more stable learning\n",
    "BATCH_SIZE = 512  # Larger batch size for better statistics\n",
    "STEPS_PER_UPDATE = 256  # Frequent updates\n",
    "PPO_EPOCHS = 12  # More epochs per update for better learning\n",
    "TARGET_KL = 0.015  # Slightly increased to allow more policy change\n",
    "\n",
    "# Beliefs will be added in Phase 2\n",
    "USE_BELIEF_MODULE = False  # Will be toggled to True in Phase 2\n",
    "BETA = 2.0  # Higher temperature for sharper belief distributions\n",
    "\n",
    "# Reward function parameters - carefully balanced\n",
    "COLLISION_PENALTY = -10.0\n",
    "GOAL_REWARD = 25.0  # Increased goal reward for stronger signal\n",
    "PROGRESS_REWARD = 1.5  # Increased to encourage moving toward goal\n",
    "ACTION_PENALTY = 0.05  # Reduced to allow more movement\n",
    "PROXIMITY_REWARD = 1.5  # Increased to create better gradient near goal\n",
    "\n",
    "# Training parameters\n",
    "EVAL_INTERVAL = 20  # Evaluate less frequently but more thoroughly\n",
    "EARLY_STOP_PATIENCE = 25  # More patience to find good solutions\n",
    "IMPROVEMENT_THRESHOLD = 0.03  # Lower threshold - 3% improvement is significant\n",
    "MIN_TRAINING_EPISODES = 200  # Ensure minimum training before stopping\n",
    "\n",
    "# Create results directory\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "# Check if CUDA is available\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "# Custom Reacher 2D Environment\n",
    "class CustomReacher2D(gym.Env):\n",
    "    \"\"\"Custom 2D Reacher Environment with realistic physics\"\"\"\n",
    "    \n",
    "    def __init__(self, render_mode=None):\n",
    "        super(CustomReacher2D, self).__init__()\n",
    "        \n",
    "        # Action space: 2D continuous actions for controlling the arm\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(2,), dtype=np.float32)\n",
    "        \n",
    "        # Observation space: joint angles, joint velocities, end-effector position\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(6,), dtype=np.float32)\n",
    "        \n",
    "        # Arm parameters\n",
    "        self.link_lengths = [0.1, 0.11]  # Length of arm segments\n",
    "        self.max_velocity = 1.0  # Maximum joint velocity\n",
    "        self.dt = 0.05  # Time step\n",
    "        \n",
    "        # State\n",
    "        self.joint_angles = np.array([0.0, 0.0])  # Two joints, in radians\n",
    "        self.joint_velocities = np.array([0.0, 0.0])\n",
    "        \n",
    "        # Rendering setup\n",
    "        self.render_mode = render_mode\n",
    "        self.screen = None\n",
    "        self.clock = None\n",
    "        self.window_size = 500  # pixels\n",
    "        self.pygame = None\n",
    "        \n",
    "        # For visualization\n",
    "        if render_mode == \"rgb_array\":\n",
    "            # Import pygame only if rendering is needed\n",
    "            try:\n",
    "                import pygame\n",
    "                self.pygame = pygame\n",
    "                self.screen = pygame.Surface((self.window_size, self.window_size))\n",
    "                self.clock = pygame.time.Clock()\n",
    "            except ImportError:\n",
    "                self.render_mode = None\n",
    "                print(\"Warning: Pygame not available. Rendering disabled.\")\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        # Reset joint state\n",
    "        self.joint_angles = np.array([np.random.uniform(-np.pi/2, np.pi/2), \n",
    "                                       np.random.uniform(-np.pi/2, np.pi/2)])\n",
    "        self.joint_velocities = np.array([0.0, 0.0])\n",
    "        \n",
    "        # Get current observation\n",
    "        observation = self._get_obs()\n",
    "        info = {}\n",
    "        \n",
    "        return observation, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Clip action to action space\n",
    "        action = np.clip(action, -1, 1)\n",
    "        \n",
    "        # Convert actions to joint accelerations - more realistic physics\n",
    "        accelerations = action * 8.0  # Scale factor for accelerations\n",
    "        \n",
    "        # Update velocities using accelerations\n",
    "        self.joint_velocities += accelerations * self.dt\n",
    "        \n",
    "        # Clip velocities\n",
    "        self.joint_velocities = np.clip(self.joint_velocities, -self.max_velocity, self.max_velocity)\n",
    "        \n",
    "        # Update joint angles using velocities\n",
    "        self.joint_angles += self.joint_velocities * self.dt\n",
    "        \n",
    "        # Wrap angles to [-pi, pi]\n",
    "        self.joint_angles = np.mod(self.joint_angles + np.pi, 2 * np.pi) - np.pi\n",
    "        \n",
    "        # Get new observation\n",
    "        observation = self._get_obs()\n",
    "        \n",
    "        # Default reward and done (to be overridden by wrapper)\n",
    "        reward = 0.0\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        info = {}\n",
    "        \n",
    "        return observation, reward, terminated, truncated, info\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        \"\"\"Get current observation (joint angles, velocities, and end-effector position)\"\"\"\n",
    "        ee_pos = self._get_end_effector_position()\n",
    "        obs = np.concatenate([\n",
    "            np.cos(self.joint_angles),\n",
    "            np.sin(self.joint_angles),\n",
    "            ee_pos\n",
    "        ])\n",
    "        return obs\n",
    "    \n",
    "    def _get_end_effector_position(self):\n",
    "        \"\"\"Compute the position of the end effector using forward kinematics\"\"\"\n",
    "        theta1, theta2 = self.joint_angles\n",
    "        l1, l2 = self.link_lengths\n",
    "        \n",
    "        # Position of first joint is at origin (0,0)\n",
    "        # Position of second joint\n",
    "        x1 = l1 * np.cos(theta1)\n",
    "        y1 = l1 * np.sin(theta1)\n",
    "        \n",
    "        # Position of end effector\n",
    "        x2 = x1 + l2 * np.cos(theta1 + theta2)\n",
    "        y2 = y1 + l2 * np.sin(theta1 + theta2)\n",
    "        \n",
    "        return np.array([x2, y2])\n",
    "    \n",
    "    def render(self):\n",
    "        if self.render_mode != \"rgb_array\" or self.pygame is None:\n",
    "            return None\n",
    "        \n",
    "        # Clear the screen\n",
    "        self.screen.fill((255, 255, 255))\n",
    "        \n",
    "        # Convert from world coordinates to screen coordinates\n",
    "        def world_to_screen(point):\n",
    "            scale = self.window_size / 2.5  # Scale to make the arm visible\n",
    "            screen_x = int(point[0] * scale + self.window_size / 2)\n",
    "            screen_y = int(-point[1] * scale + self.window_size / 2)  # Negative because screen y is inverted\n",
    "            return (screen_x, screen_y)\n",
    "        \n",
    "        # Draw the arm\n",
    "        # Base position (origin)\n",
    "        base_pos = world_to_screen((0, 0))\n",
    "        \n",
    "        # First joint position\n",
    "        theta1 = self.joint_angles[0]\n",
    "        l1 = self.link_lengths[0]\n",
    "        joint1_x = l1 * np.cos(theta1)\n",
    "        joint1_y = l1 * np.sin(theta1)\n",
    "        joint1_pos = world_to_screen((joint1_x, joint1_y))\n",
    "        \n",
    "        # End effector position\n",
    "        ee_pos = self._get_end_effector_position()\n",
    "        ee_screen_pos = world_to_screen(ee_pos)\n",
    "        \n",
    "        # Draw the links\n",
    "        self.pygame.draw.line(self.screen, (0, 0, 0), base_pos, joint1_pos, 6)\n",
    "        self.pygame.draw.line(self.screen, (0, 0, 0), joint1_pos, ee_screen_pos, 6)\n",
    "        \n",
    "        # Draw the joints\n",
    "        self.pygame.draw.circle(self.screen, (255, 0, 0), base_pos, 10)\n",
    "        self.pygame.draw.circle(self.screen, (0, 255, 0), joint1_pos, 8)\n",
    "        self.pygame.draw.circle(self.screen, (0, 0, 255), ee_screen_pos, 8)\n",
    "        \n",
    "        return np.transpose(np.array(self.pygame.surfarray.pixels3d(self.screen)), axes=(1, 0, 2))\n",
    "    \n",
    "    def close(self):\n",
    "        if self.screen is not None:\n",
    "            self.screen = None\n",
    "\n",
    "\n",
    "# Environment wrapper for multiple goals\n",
    "class MultiGoalReacherEnv(gym.Wrapper):\n",
    "    def __init__(self, num_goals=3, goal_radius=0.05, render_mode=None):\n",
    "        # Create the base environment\n",
    "        self.env = CustomReacher2D(render_mode=render_mode)\n",
    "        super().__init__(self.env)\n",
    "        \n",
    "        # Potential goal positions (normalized to [-1, 1])\n",
    "        self.num_goals = num_goals\n",
    "        self.goal_radius = goal_radius\n",
    "        self.goals = []\n",
    "        self.true_goal = None\n",
    "        self.true_goal_idx = None\n",
    "        self.obstacles = []\n",
    "        \n",
    "        # Override observation space to include goal information\n",
    "        obs_dim = self.env.observation_space.shape[0]\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=(obs_dim + num_goals * 2,)\n",
    "        )\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        \n",
    "        # Generate random goal positions - better distribution\n",
    "        self.goals = []\n",
    "        angles = np.linspace(0, 2*np.pi, self.num_goals, endpoint=False)\n",
    "        angles += np.random.uniform(0, 2*np.pi/self.num_goals)  # Add randomness\n",
    "        \n",
    "        for i in range(self.num_goals):\n",
    "            # Place goals in a circular pattern with some randomness\n",
    "            radius = np.random.uniform(0.12, 0.18)\n",
    "            goal_x = radius * np.cos(angles[i])\n",
    "            goal_y = radius * np.sin(angles[i])\n",
    "            self.goals.append(np.array([goal_x, goal_y]))\n",
    "        \n",
    "        # Choose one goal as the true goal\n",
    "        self.true_goal_idx = np.random.randint(0, self.num_goals)\n",
    "        self.true_goal = self.goals[self.true_goal_idx]\n",
    "        \n",
    "        # Generate random obstacles (simplified as positions to avoid)\n",
    "        self.obstacles = []\n",
    "        for _ in range(2):  # 2 obstacles\n",
    "            obs_x = np.random.uniform(-0.15, 0.15)\n",
    "            obs_y = np.random.uniform(-0.15, 0.15)\n",
    "            radius = np.random.uniform(0.02, 0.04)\n",
    "            \n",
    "            # Ensure obstacles don't overlap with goals\n",
    "            valid = True\n",
    "            for goal in self.goals:\n",
    "                if np.linalg.norm(goal - np.array([obs_x, obs_y])) < radius + self.goal_radius:\n",
    "                    valid = False\n",
    "                    break\n",
    "            \n",
    "            if valid:\n",
    "                self.obstacles.append((np.array([obs_x, obs_y]), radius))\n",
    "        \n",
    "        # Track previous distance for progress reward\n",
    "        self.prev_distance = np.linalg.norm(self._get_end_effector_position() - self.true_goal)\n",
    "        \n",
    "        # Augment observation with goal positions\n",
    "        augmented_obs = self._augment_observation(obs)\n",
    "        return augmented_obs, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, _, terminated, truncated, info = self.env.step(action)\n",
    "        \n",
    "        # Get current end effector position\n",
    "        ee_pos = self._get_end_effector_position()\n",
    "        \n",
    "        # Check if the end effector reached the true goal\n",
    "        distance_to_goal = np.linalg.norm(ee_pos - self.true_goal)\n",
    "        goal_reached = distance_to_goal < self.goal_radius\n",
    "        \n",
    "        # Check for collision with obstacles\n",
    "        collision = False\n",
    "        for obs_pos, obs_radius in self.obstacles:\n",
    "            if np.linalg.norm(ee_pos - obs_pos) < obs_radius:\n",
    "                collision = True\n",
    "                break\n",
    "        \n",
    "        # Simplified reward function\n",
    "        reward = self._compute_reward(ee_pos, action, goal_reached, collision)\n",
    "        \n",
    "        # Override termination conditions\n",
    "        done = goal_reached or collision\n",
    "        \n",
    "        # Augment observation with goal positions\n",
    "        augmented_obs = self._augment_observation(obs)\n",
    "        \n",
    "        # Update info dictionary\n",
    "        info['true_goal'] = self.true_goal\n",
    "        info['goal_reached'] = goal_reached\n",
    "        info['collision'] = collision\n",
    "        info['distance_to_goal'] = distance_to_goal\n",
    "        \n",
    "        return augmented_obs, reward, done, truncated, info\n",
    "    \n",
    "    def _get_end_effector_position(self):\n",
    "        \"\"\"Get the end effector position from the environment\"\"\"\n",
    "        return self.env._get_end_effector_position()\n",
    "    \n",
    "    def _compute_reward(self, ee_pos, action, goal_reached, collision):\n",
    "        \"\"\"Improved reward function with better shaped rewards\"\"\"\n",
    "        # Distance to true goal\n",
    "        distance_to_goal = np.linalg.norm(ee_pos - self.true_goal)\n",
    "        \n",
    "        # Base reward starts at 0\n",
    "        reward = 0.0\n",
    "        \n",
    "        # Terminal states\n",
    "        if collision:\n",
    "            return -COLLISION_PENALTY\n",
    "        \n",
    "        if goal_reached:\n",
    "            # Bonus for faster completion - encourages efficiency\n",
    "            reward += GOAL_REWARD + GOAL_REWARD * 0.5 * (1.0 - min(1.0, distance_to_goal))\n",
    "            return reward\n",
    "        \n",
    "        # Progress reward (encourage moving toward the goal)\n",
    "        progress = self.prev_distance - distance_to_goal\n",
    "        \n",
    "        # Enhanced progress reward - higher gradient when making good progress\n",
    "        if progress > 0:\n",
    "            # Reward more for making progress when closer to goal\n",
    "            progress_scaling = 1.0 + 2.0 * (1.0 - min(1.0, distance_to_goal / 0.2))\n",
    "            reward += PROGRESS_REWARD * progress * progress_scaling\n",
    "        else:\n",
    "            # Small penalty for moving away from goal\n",
    "            reward += 0.5 * PROGRESS_REWARD * progress\n",
    "        \n",
    "        # Store current distance for next step\n",
    "        self.prev_distance = distance_to_goal\n",
    "        \n",
    "        # Action smoothness reward - quadratic penalty scaled by distance\n",
    "        # Less penalty when far, more when close (encourages precision near goal)\n",
    "        action_smoothness = ACTION_PENALTY * np.square(action).sum()\n",
    "        action_scale = min(1.0, distance_to_goal / 0.1)  # Scale penalty by distance\n",
    "        reward -= action_smoothness * (0.5 + 0.5 * (1.0 - action_scale))\n",
    "        \n",
    "        # Proximity reward (higher when closer to goal)\n",
    "        # Exponential scaling provides better gradient\n",
    "        proximity_reward = PROXIMITY_REWARD * np.exp(-5.0 * distance_to_goal)\n",
    "        reward += proximity_reward\n",
    "        \n",
    "        # Add time penalty to encourage faster completion\n",
    "        reward -= 0.01\n",
    "        \n",
    "        # Check if near obstacles and add avoidance reward\n",
    "        for obs_pos, obs_radius in self.obstacles:\n",
    "            dist_to_obs = np.linalg.norm(ee_pos - obs_pos)\n",
    "            if dist_to_obs < obs_radius * 3:\n",
    "                # Encourage staying away from obstacles\n",
    "                obstacle_margin = dist_to_obs - obs_radius\n",
    "                if obstacle_margin > 0:\n",
    "                    # Reward for maintaining safe distance\n",
    "                    reward += 0.5 * np.exp(-5.0 * obstacle_margin)\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def _augment_observation(self, obs):\n",
    "        \"\"\"Concatenate goal positions to the observation\"\"\"\n",
    "        goal_info = np.concatenate([goal for goal in self.goals])\n",
    "        return np.concatenate([obs, goal_info])\n",
    "    \n",
    "    def render(self):\n",
    "        frame = self.env.render()\n",
    "        if frame is None:\n",
    "            return None\n",
    "        \n",
    "        if self.env.pygame is None:\n",
    "            return frame\n",
    "        \n",
    "        # Draw goals and obstacles on the frame\n",
    "        def world_to_screen(point):\n",
    "            scale = self.env.window_size / 2.5\n",
    "            screen_x = int(point[0] * scale + self.env.window_size / 2)\n",
    "            screen_y = int(-point[1] * scale + self.env.window_size / 2)\n",
    "            return (screen_x, screen_y)\n",
    "        \n",
    "        # Draw goals\n",
    "        for i, goal in enumerate(self.goals):\n",
    "            goal_pos = world_to_screen(goal)\n",
    "            color = (255, 215, 0) if i == self.true_goal_idx else (200, 200, 200)\n",
    "            self.env.pygame.draw.circle(self.env.screen, color, goal_pos, int(self.goal_radius * self.env.window_size / 2.5))\n",
    "        \n",
    "        # Draw obstacles\n",
    "        for obs_pos, obs_radius in self.obstacles:\n",
    "            obs_screen_pos = world_to_screen(obs_pos)\n",
    "            self.env.pygame.draw.circle(\n",
    "                self.env.screen,\n",
    "                (100, 100, 100),\n",
    "                obs_screen_pos,\n",
    "                int(obs_radius * self.env.window_size / 2.5)\n",
    "            )\n",
    "        \n",
    "        return np.transpose(np.array(self.env.pygame.surfarray.pixels3d(self.env.screen)), axes=(1, 0, 2))\n",
    "\n",
    "\n",
    "# Simple Belief Module - To be used in Phase 2\n",
    "class SimpleBeliefModule(nn.Module):\n",
    "    def __init__(self, num_goals=3, state_dim=2, action_dim=2, beta=1.0):\n",
    "        super(SimpleBeliefModule, self).__init__()\n",
    "        self.num_goals = num_goals\n",
    "        \n",
    "        # Just one temperature parameter for simplicity\n",
    "        self.beta = nn.Parameter(torch.tensor(float(beta), dtype=torch.float32))\n",
    "        \n",
    "        # Prior probabilities (initialized as uniform)\n",
    "        self.register_buffer('prior', torch.ones(num_goals, dtype=torch.float32) / num_goals)\n",
    "        \n",
    "        # Feature extractor for better state representation\n",
    "        in_dim = state_dim + action_dim\n",
    "        self.feature_net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        \n",
    "        # Goal comparator network\n",
    "        self.comparison_net = nn.Sequential(\n",
    "            nn.Linear(32 + 2, 32),  # Features + goal\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.orthogonal_(m.weight, gain=1.0)\n",
    "            nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, state, action, goals, prev_belief=None):\n",
    "        \"\"\"\n",
    "        Update belief over goals based on observed state and action.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state (end effector position) [2]\n",
    "            action: Agent action [2]\n",
    "            goals: List of potential goal positions [num_goals x 2]\n",
    "            prev_belief: Previous belief distribution [num_goals]\n",
    "            \n",
    "        Returns:\n",
    "            Updated belief over goals [num_goals]\n",
    "        \"\"\"\n",
    "        # Ensure all input tensors are on the same device\n",
    "        device = self.beta.device\n",
    "        \n",
    "        if prev_belief is None:\n",
    "            prev_belief = self.prior.clone().to(device)\n",
    "        \n",
    "        # Ensure state and action are correctly shaped\n",
    "        if isinstance(state, np.ndarray):\n",
    "            state = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "        if isinstance(action, np.ndarray):\n",
    "            action = torch.tensor(action, dtype=torch.float32, device=device)\n",
    "        \n",
    "        # Ensure we have flat vectors (no batch dimension)\n",
    "        if state.dim() > 1:\n",
    "            state = state.reshape(-1)\n",
    "        if action.dim() > 1:\n",
    "            action = action.reshape(-1)\n",
    "        \n",
    "        # Concatenate state and action to get a tensor of shape (4,)\n",
    "        state_action = torch.cat([state, action])\n",
    "        \n",
    "        # Add batch dimension: (4,) -> (1, 4)\n",
    "        state_action = state_action.unsqueeze(0)\n",
    "        \n",
    "        # Extract features from state-action pair\n",
    "        features = self.feature_net(state_action)\n",
    "        \n",
    "        # Calculate likelihoods for each goal\n",
    "        likelihoods = torch.zeros(self.num_goals, device=device)\n",
    "        \n",
    "        for i in range(self.num_goals):\n",
    "            goal = goals[i]\n",
    "            if isinstance(goal, np.ndarray):\n",
    "                goal = torch.tensor(goal, dtype=torch.float32, device=device)\n",
    "            \n",
    "            # Ensure goal is a flat vector (no batch dimension)\n",
    "            if goal.dim() > 1:\n",
    "                goal = goal.reshape(-1)\n",
    "            \n",
    "            # Add batch dimension to goal: (2,) -> (1, 2)\n",
    "            goal = goal.unsqueeze(0)\n",
    "            \n",
    "            # Concatenate features with goal\n",
    "            # features has shape (1, 32) and goal has shape (1, 2)\n",
    "            feature_goal = torch.cat([features, goal], dim=1)\n",
    "            \n",
    "            # Score this goal based on state-action\n",
    "            score = self.comparison_net(feature_goal).squeeze()\n",
    "            \n",
    "            # Store the score\n",
    "            likelihoods[i] = score\n",
    "        \n",
    "        # Apply softmax with temperature\n",
    "        beta = F.softplus(self.beta) + 0.1  # Ensure positive\n",
    "        likelihoods = F.softmax(beta * likelihoods, dim=0)\n",
    "        \n",
    "        # Bayesian update\n",
    "        posterior = prev_belief * likelihoods\n",
    "        posterior_sum = torch.sum(posterior)\n",
    "        \n",
    "        # Check for numerical stability\n",
    "        if posterior_sum > 1e-8:\n",
    "            posterior = posterior / posterior_sum\n",
    "        else:\n",
    "            # If update fails, use a mixture\n",
    "            posterior = 0.5 * prev_belief + 0.5 * likelihoods\n",
    "        \n",
    "        # Check for NaN values\n",
    "        if torch.isnan(posterior).any():\n",
    "            posterior = prev_belief.clone()\n",
    "        \n",
    "        return posterior\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset belief to prior.\"\"\"\n",
    "        return self.prior.clone()\n",
    "\n",
    "\n",
    "# Improved Actor-Critic Networks for PPO\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256, belief_dim=None, use_belief=False):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.use_belief = use_belief\n",
    "        \n",
    "        # Feature extraction layers\n",
    "        self.feature_network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Separate belief processing if provided and enabled\n",
    "        self.belief_network = None\n",
    "        if use_belief and belief_dim is not None:\n",
    "            self.belief_network = nn.Sequential(\n",
    "                nn.Linear(belief_dim, hidden_dim // 2),\n",
    "                nn.LayerNorm(hidden_dim // 2),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            \n",
    "            # Fusion layer for state and belief features\n",
    "            self.fusion_network = nn.Sequential(\n",
    "                nn.Linear(hidden_dim + hidden_dim // 2, hidden_dim),\n",
    "                nn.LayerNorm(hidden_dim),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        \n",
    "        # Actor network (policy)\n",
    "        self.actor_mean = nn.Linear(hidden_dim, action_dim)\n",
    "        self.actor_log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "        \n",
    "        # Critic network (value function)\n",
    "        self.critic = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.orthogonal_(module.weight, gain=np.sqrt(2))\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, state, belief=None):\n",
    "        # Extract features from state\n",
    "        features = self.feature_network(state)\n",
    "        \n",
    "        # Process belief and fuse with state features if available\n",
    "        if self.use_belief and belief is not None and self.belief_network is not None:\n",
    "            belief_features = self.belief_network(belief)\n",
    "            features = torch.cat([features, belief_features], dim=-1)\n",
    "            features = self.fusion_network(features)\n",
    "        \n",
    "        # Actor: Get action distribution\n",
    "        action_mean = self.actor_mean(features)\n",
    "        action_std = torch.exp(torch.clamp(self.actor_log_std, min=-20, max=2))\n",
    "        \n",
    "        # Critic: Get state value\n",
    "        value = self.critic(features)\n",
    "        \n",
    "        return action_mean, action_std, value\n",
    "    \n",
    "    def get_action(self, state, belief=None, deterministic=False):\n",
    "        # Move data to the model's device\n",
    "        device = next(self.parameters()).device\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        if belief is not None and self.use_belief:\n",
    "            belief = torch.FloatTensor(belief).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action_mean, action_std, _ = self.forward(state, belief)\n",
    "            \n",
    "            if deterministic:\n",
    "                action = action_mean\n",
    "            else:\n",
    "                normal = Normal(action_mean, action_std)\n",
    "                action = normal.sample()\n",
    "                \n",
    "            return action.squeeze(0).cpu().numpy()\n",
    "    \n",
    "    def evaluate_actions(self, states, actions, beliefs=None):\n",
    "        action_mean, action_std, values = self.forward(states, beliefs)\n",
    "        \n",
    "        # Create normal distribution\n",
    "        dist = Normal(action_mean, action_std)\n",
    "        \n",
    "        # Get log probabilities\n",
    "        log_probs = dist.log_prob(actions).sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Get entropy\n",
    "        entropy = dist.entropy().sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        return log_probs, entropy, values\n",
    "\n",
    "\n",
    "# Improved PPO Agent\n",
    "class PPOAgent:\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256, num_goals=3, lr=3e-4, \n",
    "                 gamma=0.99, gae_lambda=0.95, clip_epsilon=0.2, value_coef=0.5, \n",
    "                 entropy_coef=0.01, target_kl=0.01, use_belief=False):\n",
    "        \n",
    "        # Initialize actor-critic\n",
    "        self.actor_critic = ActorCritic(\n",
    "            state_dim=state_dim,\n",
    "            action_dim=action_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            belief_dim=num_goals if use_belief else None,\n",
    "            use_belief=use_belief\n",
    "        )\n",
    "        \n",
    "        # Initialize belief module if needed\n",
    "        self.belief_module = None\n",
    "        if use_belief:\n",
    "            self.belief_module = SimpleBeliefModule(num_goals=num_goals, beta=BETA)\n",
    "        \n",
    "        self.use_belief = use_belief\n",
    "        self.num_goals = num_goals\n",
    "        \n",
    "        # Move models to the appropriate device\n",
    "        self.device = DEVICE\n",
    "        self.actor_critic.to(self.device)\n",
    "        if self.belief_module:\n",
    "            self.belief_module.to(self.device)\n",
    "        \n",
    "        # Initialize optimizer - joint optimization of both networks\n",
    "        if self.belief_module:\n",
    "            self.optimizer = optim.Adam(\n",
    "                list(self.actor_critic.parameters()) + list(self.belief_module.parameters()), \n",
    "                lr=lr\n",
    "            )\n",
    "        else:\n",
    "            self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=lr)\n",
    "            \n",
    "        # Add learning rate scheduler\n",
    "        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=1000, eta_min=lr/10)\n",
    "        \n",
    "        # Store hyperparameters\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.value_coef = value_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.target_kl = target_kl\n",
    "        \n",
    "        # Memory buffer for experience\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.log_probs = []\n",
    "        self.beliefs = []\n",
    "        self.dones = []\n",
    "        \n",
    "        # Current belief and previous action\n",
    "        self.current_belief = None\n",
    "        self.prev_action = None\n",
    "        \n",
    "        # Tracking variables\n",
    "        self.steps_since_update = 0\n",
    "        self.epochs_trained = 0\n",
    "        self.update_count = 0\n",
    "    \n",
    "    def select_action(self, state, goals=None, deterministic=False):\n",
    "        \"\"\"Select action based on current state and belief.\"\"\"\n",
    "        # Extract goals from state if not provided\n",
    "        if goals is None:\n",
    "            # Assume the state includes goal information as per the environment wrapper\n",
    "            goals_data = state[state.shape[0] - 2*self.num_goals:]\n",
    "            goals = [goals_data[i:i+2] for i in range(0, len(goals_data), 2)]\n",
    "        \n",
    "                # Get end effector position# Get end effector position - corrected to account for goal information\n",
    "        ee_pos = state[-2-2*self.num_goals:-2*self.num_goals]  # Position right before goal info\n",
    "        \n",
    "        # Update belief if using belief module\n",
    "        belief = None\n",
    "        if self.use_belief and self.belief_module:\n",
    "            # Initialize belief if needed\n",
    "            if self.current_belief is None:\n",
    "                self.current_belief = self.belief_module.reset().to(self.device)\n",
    "            \n",
    "            # Handle case where we don't have previous action yet\n",
    "            if self.prev_action is None:\n",
    "                self.prev_action = torch.zeros(2, device=self.device)\n",
    "            \n",
    "            # Update belief based on observed state and previous action\n",
    "            with torch.no_grad():\n",
    "                updated_belief = self.belief_module.forward(\n",
    "                    state=torch.tensor(ee_pos, dtype=torch.float32, device=self.device),\n",
    "                    action=self.prev_action,\n",
    "                    goals=[torch.tensor(g, dtype=torch.float32, device=self.device) for g in goals],\n",
    "                    prev_belief=self.current_belief\n",
    "                )\n",
    "                \n",
    "                # Safety check\n",
    "                if torch.isnan(updated_belief).any():\n",
    "                    updated_belief = self.belief_module.reset().to(self.device)\n",
    "                \n",
    "                self.current_belief = updated_belief\n",
    "                belief = self.current_belief.cpu().numpy()\n",
    "        \n",
    "        # Select action using actor network\n",
    "        action = self.actor_critic.get_action(\n",
    "            state=state,\n",
    "            belief=belief if self.use_belief else None,\n",
    "            deterministic=deterministic\n",
    "        )\n",
    "        \n",
    "        # Store action for next belief update\n",
    "        if self.use_belief:\n",
    "            self.prev_action = torch.tensor(action, device=self.device)\n",
    "        \n",
    "        # Safety check for NaN actions\n",
    "        if np.isnan(action).any():\n",
    "            action = np.zeros_like(action)\n",
    "            if self.use_belief:\n",
    "                self.prev_action = torch.zeros(2, device=self.device)\n",
    "        \n",
    "        return action, belief if self.use_belief else None\n",
    "    \n",
    "    def remember(self, state, action, reward, value, log_prob, belief, done):\n",
    "        \"\"\"Store experience in memory.\"\"\"\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.values.append(value)\n",
    "        self.log_probs.append(log_prob)\n",
    "        if belief is not None:\n",
    "            self.beliefs.append(belief)\n",
    "        self.dones.append(done)\n",
    "        \n",
    "        # Track steps since last update\n",
    "        self.steps_since_update += 1\n",
    "    \n",
    "    def compute_gae(self, next_value):\n",
    "        \"\"\"Compute Generalized Advantage Estimation.\"\"\"\n",
    "        values = self.values + [next_value]\n",
    "        gae = 0\n",
    "        returns = []\n",
    "        \n",
    "        # GAE calculation\n",
    "        for step in reversed(range(len(self.rewards))):\n",
    "            delta = (self.rewards[step] + \n",
    "                     self.gamma * values[step + 1] * (1 - self.dones[step]) - \n",
    "                     values[step])\n",
    "            gae = delta + self.gamma * self.gae_lambda * (1 - self.dones[step]) * gae\n",
    "            returns.insert(0, gae + values[step])\n",
    "        \n",
    "        return returns\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"Update policy using PPO algorithm.\"\"\"\n",
    "        # Skip update if not enough data\n",
    "        if len(self.states) < BATCH_SIZE:\n",
    "            return 0.0\n",
    "            \n",
    "        # Get next value for GAE\n",
    "        with torch.no_grad():\n",
    "            if len(self.states) > 0:\n",
    "                state = torch.FloatTensor(self.states[-1]).unsqueeze(0).to(self.device)\n",
    "                belief = None\n",
    "                if self.use_belief and len(self.beliefs) > 0:\n",
    "                    belief = torch.FloatTensor(self.beliefs[-1]).unsqueeze(0).to(self.device)\n",
    "                _, _, next_value = self.actor_critic(state, belief)\n",
    "                next_value = next_value.squeeze().cpu().item()\n",
    "            else:\n",
    "                next_value = 0\n",
    "        \n",
    "        # Compute returns using GAE\n",
    "        returns = self.compute_gae(next_value)\n",
    "        \n",
    "        # Convert lists to tensors\n",
    "        states = torch.FloatTensor(np.array(self.states)).to(self.device)\n",
    "        actions = torch.FloatTensor(np.array(self.actions)).to(self.device)\n",
    "        returns = torch.FloatTensor(returns).unsqueeze(1).to(self.device)\n",
    "        old_log_probs = torch.FloatTensor(np.array(self.log_probs)).unsqueeze(1).to(self.device)\n",
    "        values = torch.FloatTensor(np.array(self.values)).unsqueeze(1).to(self.device)\n",
    "        \n",
    "        # Prepare beliefs if used\n",
    "        beliefs = None\n",
    "        if self.use_belief and len(self.beliefs) > 0:\n",
    "            beliefs = torch.FloatTensor(np.array(self.beliefs)).to(self.device)\n",
    "        \n",
    "        # Calculate advantages\n",
    "        advantages = returns - values\n",
    "        \n",
    "        # Normalize advantages\n",
    "        if len(advantages) > 1:\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        total_loss = 0\n",
    "        policy_losses = []\n",
    "        value_losses = []\n",
    "        entropy_losses = []\n",
    "        kl_divs = []\n",
    "        \n",
    "        # Mini-batch training\n",
    "        batch_size = min(BATCH_SIZE, len(self.states))\n",
    "        indices = np.arange(len(self.states))\n",
    "        \n",
    "        # Randomize and shuffle batches\n",
    "        np.random.shuffle(indices)\n",
    "        batches = [indices[i:i + batch_size] for i in range(0, len(indices), batch_size)]\n",
    "        \n",
    "        for _ in range(PPO_EPOCHS):\n",
    "            # Shuffle batches\n",
    "            np.random.shuffle(batches)\n",
    "            \n",
    "            for batch_indices in batches:\n",
    "                # Get batch data\n",
    "                batch_states = states[batch_indices]\n",
    "                batch_actions = actions[batch_indices]\n",
    "                batch_advantages = advantages[batch_indices]\n",
    "                batch_returns = returns[batch_indices]\n",
    "                batch_old_log_probs = old_log_probs[batch_indices]\n",
    "                \n",
    "                batch_beliefs = None\n",
    "                if self.use_belief and beliefs is not None:\n",
    "                    batch_beliefs = beliefs[batch_indices]\n",
    "                \n",
    "                # Evaluate actions again\n",
    "                new_log_probs, entropy, values = self.actor_critic.evaluate_actions(\n",
    "                    batch_states,\n",
    "                    batch_actions,\n",
    "                    batch_beliefs\n",
    "                )\n",
    "                \n",
    "                # Compute ratio\n",
    "                ratio = torch.exp(new_log_probs - batch_old_log_probs)\n",
    "                \n",
    "                # Clamp ratio to avoid numerical instability\n",
    "                ratio = torch.clamp(ratio, 0.0, 10.0)\n",
    "                \n",
    "                # Compute surrogate losses\n",
    "                surrogate1 = ratio * batch_advantages\n",
    "                surrogate2 = torch.clamp(ratio, 1.0 - self.clip_epsilon, 1.0 + self.clip_epsilon) * batch_advantages\n",
    "                \n",
    "                # Value loss\n",
    "                value_loss = F.mse_loss(values, batch_returns)\n",
    "                \n",
    "                # Policy loss\n",
    "                policy_loss = -torch.min(surrogate1, surrogate2).mean()\n",
    "                \n",
    "                # Entropy bonus\n",
    "                entropy_loss = -entropy.mean()\n",
    "                \n",
    "                # Total loss\n",
    "                loss = policy_loss + self.value_coef * value_loss + self.entropy_coef * entropy_loss\n",
    "                \n",
    "                # Store losses for logging\n",
    "                policy_losses.append(policy_loss.item())\n",
    "                value_losses.append(value_loss.item())\n",
    "                entropy_losses.append(entropy_loss.item())\n",
    "                \n",
    "                # Update parameters\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                # Gradient clipping for stability\n",
    "                nn.utils.clip_grad_norm_(self.actor_critic.parameters(), max_norm=0.5)\n",
    "                if self.use_belief and self.belief_module:\n",
    "                    nn.utils.clip_grad_norm_(self.belief_module.parameters(), max_norm=0.5)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # Check KL divergence for early stopping\n",
    "                with torch.no_grad():\n",
    "                    # Compute approx KL for early stopping\n",
    "                    log_ratio = new_log_probs - batch_old_log_probs\n",
    "                    approx_kl = ((log_ratio.exp() - 1) - log_ratio).mean().item()\n",
    "                    kl_divs.append(approx_kl)\n",
    "                \n",
    "                if approx_kl > self.target_kl:\n",
    "                    break\n",
    "            \n",
    "            # Early stopping based on average KL divergence\n",
    "            if np.mean(kl_divs) > self.target_kl:\n",
    "                break\n",
    "        \n",
    "        # Update learning rate periodically\n",
    "        self.scheduler.step()\n",
    "        \n",
    "        # Increment epoch count\n",
    "        self.epochs_trained += 1\n",
    "        self.update_count += 1\n",
    "        \n",
    "        # Compute average losses for logging\n",
    "        avg_policy_loss = np.mean(policy_losses) if policy_losses else 0\n",
    "        avg_value_loss = np.mean(value_losses) if value_losses else 0\n",
    "        avg_entropy_loss = np.mean(entropy_losses) if entropy_losses else 0\n",
    "        avg_kl = np.mean(kl_divs) if kl_divs else 0\n",
    "        \n",
    "        # Reset memory buffer\n",
    "        self.clear_memory()\n",
    "        \n",
    "        # Return loss components for logging\n",
    "        loss_info = {\n",
    "            'total_loss': total_loss / (len(batches) * PPO_EPOCHS),\n",
    "            'policy_loss': avg_policy_loss,\n",
    "            'value_loss': avg_value_loss,\n",
    "            'entropy_loss': avg_entropy_loss,\n",
    "            'approx_kl': avg_kl\n",
    "        }\n",
    "        \n",
    "        return loss_info\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        \"\"\"Clear memory buffer after updates.\"\"\"\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.values = []\n",
    "        self.log_probs = []\n",
    "        self.beliefs = []\n",
    "        self.dones = []\n",
    "        self.steps_since_update = 0\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset agent state between episodes.\"\"\"\n",
    "        self.current_belief = None if self.use_belief else None\n",
    "        self.prev_action = None if self.use_belief else None\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"Save model parameters.\"\"\"\n",
    "        save_dict = {\n",
    "            'actor_critic_state_dict': self.actor_critic.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'epochs_trained': self.epochs_trained\n",
    "        }\n",
    "        \n",
    "        if self.use_belief and self.belief_module:\n",
    "            save_dict['belief_module_state_dict'] = self.belief_module.state_dict()\n",
    "            \n",
    "        torch.save(save_dict, path)\n",
    "    \n",
    "    def load(self, path):\n",
    "        \"\"\"Load model parameters.\"\"\"\n",
    "        checkpoint = torch.load(path, map_location=self.device)\n",
    "        self.actor_critic.load_state_dict(checkpoint['actor_critic_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        self.epochs_trained = checkpoint.get('epochs_trained', 0)\n",
    "        \n",
    "        if self.use_belief and self.belief_module and 'belief_module_state_dict' in checkpoint:\n",
    "            self.belief_module.load_state_dict(checkpoint['belief_module_state_dict'])\n",
    "\n",
    "\n",
    "# Performance tracking\n",
    "class PerformanceTracker:\n",
    "    def __init__(self, log_dir=\"results\"):\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.success_rates = []\n",
    "        self.collision_rates = []\n",
    "        self.belief_accuracy = []\n",
    "        self.losses = []\n",
    "        \n",
    "        # Training metrics\n",
    "        self.policy_losses = []\n",
    "        self.value_losses = []\n",
    "        self.entropy_losses = []\n",
    "        self.kl_divs = []\n",
    "        \n",
    "        # For visualization\n",
    "        self.frames = []\n",
    "        self.log_dir = log_dir\n",
    "        os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    def add_episode_metrics(self, episode_reward, episode_length, success, collision, belief_accuracy=None):\n",
    "        self.episode_rewards.append(episode_reward)\n",
    "        self.episode_lengths.append(episode_length)\n",
    "        self.success_rates.append(1 if success else 0)\n",
    "        self.collision_rates.append(1 if collision else 0)\n",
    "        if belief_accuracy is not None:\n",
    "            self.belief_accuracy.append(belief_accuracy)\n",
    "    \n",
    "    def add_training_metrics(self, loss_dict):\n",
    "        \"\"\"Add training metrics, handling both dictionary and float loss formats.\"\"\"\n",
    "        if loss_dict is None:\n",
    "            return\n",
    "            \n",
    "        # Handle the case where loss_dict is actually a float (total loss only)\n",
    "        if isinstance(loss_dict, (int, float)):\n",
    "            self.losses.append(loss_dict)\n",
    "            self.policy_losses.append(0)\n",
    "            self.value_losses.append(0)\n",
    "            self.entropy_losses.append(0)\n",
    "            self.kl_divs.append(0)\n",
    "        else:\n",
    "            # Normal case where we have a dictionary with detailed metrics\n",
    "            self.losses.append(loss_dict.get('total_loss', 0))\n",
    "            self.policy_losses.append(loss_dict.get('policy_loss', 0))\n",
    "            self.value_losses.append(loss_dict.get('value_loss', 0))\n",
    "            self.entropy_losses.append(loss_dict.get('entropy_loss', 0))\n",
    "            self.kl_divs.append(loss_dict.get('approx_kl', 0))\n",
    "    \n",
    "    def add_frame(self, frame):\n",
    "        self.frames.append(frame)\n",
    "    \n",
    "    def get_recent_metrics(self, window=100):\n",
    "        \"\"\"Get metrics from recent episodes.\"\"\"\n",
    "        reward = np.mean(self.episode_rewards[-window:]) if self.episode_rewards else 0\n",
    "        success = np.mean(self.success_rates[-window:]) if self.success_rates else 0\n",
    "        belief = np.mean(self.belief_accuracy[-window:]) if self.belief_accuracy else 0\n",
    "        collision = np.mean(self.collision_rates[-window:]) if self.collision_rates else 0\n",
    "        length = np.mean(self.episode_lengths[-window:]) if self.episode_lengths else 0\n",
    "        \n",
    "        return {\n",
    "            'reward': reward,\n",
    "            'success_rate': success,\n",
    "            'belief_accuracy': belief,\n",
    "            'collision_rate': collision,\n",
    "            'episode_length': length\n",
    "        }\n",
    "    \n",
    "    def plot_learning_curves(self, window=15):\n",
    "        \"\"\"Plot learning curves with moving averages.\"\"\"\n",
    "        sns.set(style=\"darkgrid\")\n",
    "        \n",
    "        # Create a figure with subplots\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        \n",
    "        # Episode rewards\n",
    "        self._plot_smoothed_curve(axes[0, 0], self.episode_rewards, window, \n",
    "                                 \"Episode Rewards\", \"Episode\", \"Reward\")\n",
    "        \n",
    "        # Success rate\n",
    "        self._plot_smoothed_curve(axes[0, 1], self.success_rates, window,\n",
    "                                 \"Success Rate\", \"Episode\", \"Success Rate\", \n",
    "                                 is_rate=True)\n",
    "        \n",
    "        # Collision rate\n",
    "        self._plot_smoothed_curve(axes[0, 2], self.collision_rates, window,\n",
    "                                 \"Collision Rate\", \"Episode\", \"Collision Rate\", \n",
    "                                 is_rate=True)\n",
    "        \n",
    "        # Episode length\n",
    "        self._plot_smoothed_curve(axes[1, 0], self.episode_lengths, window,\n",
    "                                 \"Episode Length\", \"Episode\", \"Steps\")\n",
    "        \n",
    "        # Training loss\n",
    "        self._plot_smoothed_curve(axes[1, 1], self.losses, window,\n",
    "                                 \"Training Loss\", \"Update\", \"Loss\")\n",
    "        \n",
    "        # Belief accuracy (if available)\n",
    "        if self.belief_accuracy:\n",
    "            self._plot_smoothed_curve(axes[1, 2], self.belief_accuracy, window,\n",
    "                                     \"Belief Accuracy\", \"Episode\", \"Accuracy\")\n",
    "        else:\n",
    "            axes[1, 2].set_title(\"Belief Accuracy (N/A)\")\n",
    "            axes[1, 2].set_xlabel(\"Episode\")\n",
    "            axes[1, 2].set_ylabel(\"Accuracy\")\n",
    "        \n",
    "        # Adjust layout\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{self.log_dir}/learning_curves.png\", dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        # Plot detailed training metrics\n",
    "        self._plot_training_metrics(window)\n",
    "    \n",
    "    def _smooth_data(self, data, window):\n",
    "        \"\"\"Apply moving average smoothing to data.\"\"\"\n",
    "        if len(data) < window:\n",
    "            return np.array(data)\n",
    "        \n",
    "        smoothed = np.convolve(data, np.ones(window)/window, mode='valid')\n",
    "        return smoothed\n",
    "    \n",
    "    def _plot_smoothed_curve(self, ax, data, window, title, xlabel, ylabel, is_rate=False):\n",
    "        \"\"\"Helper to plot smoothed curve with confidence interval.\"\"\"\n",
    "        if not data:\n",
    "            return\n",
    "            \n",
    "        x = np.arange(len(data))\n",
    "        y = np.array(data)\n",
    "        \n",
    "        # Plot raw data with low alpha\n",
    "        ax.plot(x, y, alpha=0.2, color='blue', label='Raw')\n",
    "        \n",
    "        # Apply smoothing for moving average if sufficient data\n",
    "        if len(x) > window:\n",
    "            smoothed_y = self._smooth_data(y, window)\n",
    "            smoothed_x = np.arange(window-1, len(x))\n",
    "            \n",
    "            # Plot moving average\n",
    "            ax.plot(smoothed_x, smoothed_y, alpha=1.0, color='blue', linewidth=2, label=f'Moving Avg (window={window})')\n",
    "            \n",
    "            # Compute rolling std for confidence interval\n",
    "            rolling_std = [np.std(y[max(0, i-window):i+1]) for i in range(window-1, len(y))]\n",
    "            rolling_std = np.array(rolling_std)\n",
    "            \n",
    "            # Plot confidence intervals\n",
    "            ax.fill_between(smoothed_x, \n",
    "                           np.maximum(0, smoothed_y - rolling_std), \n",
    "                           np.minimum(1 if is_rate else float('inf'), smoothed_y + rolling_std), \n",
    "                           alpha=0.2, color='blue')\n",
    "        \n",
    "        # Add title and labels\n",
    "        ax.set_title(title, fontsize=14)\n",
    "        ax.set_xlabel(xlabel, fontsize=12)\n",
    "        ax.set_ylabel(ylabel, fontsize=12)\n",
    "        \n",
    "        # Set y-axis limits for rate plots\n",
    "        if is_rate:\n",
    "            ax.set_ylim([-0.05, 1.05])\n",
    "        \n",
    "        # Add legend if we have both raw and smoothed data\n",
    "        if len(x) > window:\n",
    "            ax.legend(loc='best')\n",
    "    \n",
    "    def _plot_training_metrics(self, window=15):\n",
    "        \"\"\"Plot additional training metrics.\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "        \n",
    "        # Policy loss\n",
    "        self._plot_smoothed_curve(axes[0, 0], self.policy_losses, window,\n",
    "                                 \"Policy Loss\", \"Update\", \"Loss\")\n",
    "        \n",
    "        # Value loss\n",
    "        self._plot_smoothed_curve(axes[0, 1], self.value_losses, window,\n",
    "                                 \"Value Loss\", \"Update\", \"Loss\")\n",
    "        \n",
    "        # Entropy loss\n",
    "        self._plot_smoothed_curve(axes[1, 0], self.entropy_losses, window,\n",
    "                                 \"Entropy Loss\", \"Update\", \"Loss\")\n",
    "        \n",
    "        # KL divergence\n",
    "        self._plot_smoothed_curve(axes[1, 1], self.kl_divs, window,\n",
    "                                 \"Approx. KL Divergence\", \"Update\", \"KL\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{self.log_dir}/training_metrics.png\", dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_comparison(self, results_dict):\n",
    "        \"\"\"Plot comparison of different methods.\"\"\"\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "        \n",
    "        # Process data for plotting\n",
    "        methods = list(results_dict.keys())\n",
    "        success_rates = [results_dict[m]['success_rate'] for m in methods]\n",
    "        episode_rewards = [results_dict[m]['episode_reward'] for m in methods]\n",
    "        completion_times = [results_dict[m]['completion_time'] for m in methods]\n",
    "        \n",
    "        # Plot success rates\n",
    "        axes[0].bar(methods, success_rates, color='skyblue')\n",
    "        axes[0].set_title('Success Rate by Method', fontsize=14)\n",
    "        axes[0].set_ylabel('Success Rate', fontsize=12)\n",
    "        axes[0].set_ylim([0, 1.0])\n",
    "        \n",
    "        # Plot episode rewards\n",
    "        axes[1].bar(methods, episode_rewards, color='lightgreen')\n",
    "        axes[1].set_title('Average Episode Reward by Method', fontsize=14)\n",
    "        axes[1].set_ylabel('Reward', fontsize=12)\n",
    "        \n",
    "        # Plot completion times\n",
    "        axes[2].bar(methods, completion_times, color='salmon')\n",
    "        axes[2].set_title('Average Completion Time by Method', fontsize=14)\n",
    "        axes[2].set_ylabel('Steps', fontsize=12)\n",
    "        \n",
    "        # Add values on top of bars\n",
    "        for ax, data in zip(axes, [success_rates, episode_rewards, completion_times]):\n",
    "            for i, v in enumerate(data):\n",
    "                ax.text(i, v, f'{v:.2f}', ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{self.log_dir}/method_comparison.png\", dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    def save_video(self, filename=\"episode\", fps=30):\n",
    "        \"\"\"Save frames as a video.\"\"\"\n",
    "        if not self.frames:\n",
    "            return\n",
    "            \n",
    "        # Create writer\n",
    "        frames = [frame for frame in self.frames]\n",
    "        \n",
    "        # Check if frames are valid\n",
    "        if not frames or frames[0] is None:\n",
    "            print(\"No valid frames to save.\")\n",
    "            return\n",
    "            \n",
    "        height, width, _ = frames[0].shape\n",
    "        \n",
    "        # Create video writer\n",
    "        video_path = f\"{self.log_dir}/{filename}.mp4\"\n",
    "        \n",
    "        # Save using matplotlib animation\n",
    "        fig = plt.figure(figsize=(width/100, height/100), dpi=100)\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.set_axis_off()\n",
    "        \n",
    "        images = [[ax.imshow(frame)] for frame in frames]\n",
    "        anim = animation.ArtistAnimation(fig, images, interval=1000/fps, blit=True)\n",
    "        \n",
    "        try:\n",
    "            # Use Pillow writer\n",
    "            anim.save(video_path, writer='pillow', fps=fps)\n",
    "            print(f\"Video saved to {video_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving video: {e}\")\n",
    "            # Try to save individual frames instead\n",
    "            try:\n",
    "                os.makedirs(f\"{self.log_dir}/{filename}_frames\", exist_ok=True)\n",
    "                for i, frame in enumerate(frames):\n",
    "                    plt.imsave(f\"{self.log_dir}/{filename}_frames/frame_{i:04d}.png\", frame)\n",
    "                print(f\"Saved {len(frames)} individual frames to {self.log_dir}/{filename}_frames folder\")\n",
    "            except Exception as e2:\n",
    "                print(f\"Error saving frames: {e2}\")\n",
    "        \n",
    "        plt.close()\n",
    "    \n",
    "    def clear_frames(self):\n",
    "        \"\"\"Clear saved frames to free memory.\"\"\"\n",
    "        self.frames = []\n",
    "\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(env, agent, num_episodes=20, max_steps=1000, visualize=False, render_mode=None):\n",
    "    \"\"\"Evaluate the agent's performance.\"\"\"\n",
    "    eval_rewards = []\n",
    "    eval_lengths = []\n",
    "    eval_successes = []\n",
    "    eval_collisions = []\n",
    "    eval_belief_accuracy = []\n",
    "    \n",
    "    tracker = PerformanceTracker(log_dir=\"results/eval\")\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        agent.reset()\n",
    "        \n",
    "        episode_reward = 0\n",
    "        episode_length = 0\n",
    "        episode_beliefs = []\n",
    "        \n",
    "        # Extract goals from the environment\n",
    "        goals = []\n",
    "        for i in range(NUM_GOALS):\n",
    "            goal_idx = state.shape[0] - 2 * NUM_GOALS + 2 * i\n",
    "            goals.append(state[goal_idx:goal_idx + 2])\n",
    "        \n",
    "        # Episode loop\n",
    "        for step in range(max_steps):\n",
    "            # Record video if requested\n",
    "            if visualize:\n",
    "                frame = env.render()\n",
    "                if frame is not None:\n",
    "                    tracker.add_frame(frame)\n",
    "            \n",
    "            # Select action (deterministic for evaluation)\n",
    "            action, belief = agent.select_action(state, goals, deterministic=True)\n",
    "            \n",
    "            # Safety check\n",
    "            if np.isnan(action).any():\n",
    "                action = np.zeros_like(action)\n",
    "            \n",
    "            # Take action\n",
    "            next_state, reward, done, truncated, info = env.step(action)\n",
    "            \n",
    "            # Update state and counters\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            episode_length += 1\n",
    "            \n",
    "            # Track belief accuracy if using beliefs\n",
    "            if belief is not None:\n",
    "                true_goal_idx = env.true_goal_idx\n",
    "                predicted_goal_idx = np.argmax(belief)\n",
    "                episode_beliefs.append(predicted_goal_idx == true_goal_idx)\n",
    "            \n",
    "            if done or truncated:\n",
    "                break\n",
    "        \n",
    "        # Calculate episode metrics\n",
    "        success = info.get('goal_reached', False)\n",
    "        collision = info.get('collision', False)\n",
    "        belief_accuracy = np.mean(episode_beliefs) if episode_beliefs else None\n",
    "        \n",
    "        # Store metrics\n",
    "        eval_rewards.append(episode_reward)\n",
    "        eval_lengths.append(episode_length)\n",
    "        eval_successes.append(1 if success else 0)\n",
    "        eval_collisions.append(1 if collision else 0)\n",
    "        if belief_accuracy is not None:\n",
    "            eval_belief_accuracy.append(belief_accuracy)\n",
    "    \n",
    "    # Save video of the evaluation\n",
    "    if visualize and tracker.frames:\n",
    "        tracker.save_video(\"evaluation\")\n",
    "        tracker.clear_frames()\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    success_rate = np.mean(eval_successes)\n",
    "    successful_episode_lengths = [l for l, s in zip(eval_lengths, eval_successes) if s]\n",
    "    \n",
    "    results = {\n",
    "        'episode_reward': np.mean(eval_rewards),\n",
    "        'episode_length': np.mean(eval_lengths),\n",
    "        'success_rate': success_rate,\n",
    "        'collision_rate': np.mean(eval_collisions),\n",
    "        'completion_time': np.mean(successful_episode_lengths) if successful_episode_lengths else float('inf'),\n",
    "    }\n",
    "    \n",
    "    if eval_belief_accuracy:\n",
    "        results['belief_accuracy'] = np.mean(eval_belief_accuracy)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Human simulation for assistance evaluation\n",
    "class HumanSimulator:\n",
    "    def __init__(self, noise_level=0.2, random_action_prob=0.05):\n",
    "        self.noise_level = noise_level  # Noise in human control\n",
    "        self.random_action_prob = random_action_prob  # Probability of random action\n",
    "        \n",
    "    def get_action(self, state, true_goal):\n",
    "        \"\"\"Generate realistic simulated human action toward the goal.\"\"\"\n",
    "                # Extract end effector position# Get end effector position - corrected to account for goal information\n",
    "        ee_pos = state[-2-2*self.num_goals:-2*self.num_goals]  # Position right before goal info\n",
    "        \n",
    "        # Calculate direction to goal\n",
    "        goal_dir = true_goal - ee_pos\n",
    "        goal_dist = np.linalg.norm(goal_dir)\n",
    "        \n",
    "        # Random action with small probability\n",
    "        if np.random.random() < self.random_action_prob:\n",
    "            return np.random.uniform(-1, 1, size=2)\n",
    "        \n",
    "        # Normalize direction if non-zero\n",
    "        if goal_dist > 0:\n",
    "            optimal_dir = goal_dir / goal_dist\n",
    "        else:\n",
    "            optimal_dir = np.zeros(2)\n",
    "        \n",
    "        # Scale action magnitude based on distance\n",
    "        optimal_magnitude = min(1.0, goal_dist * 2.0)\n",
    "        optimal_action = optimal_dir * optimal_magnitude\n",
    "        \n",
    "        # Add noise to simulate human variability\n",
    "        noise = np.random.normal(0, self.noise_level, size=2)\n",
    "        \n",
    "        # Add some tremor when close to goal (simulating precision difficulty)\n",
    "        if goal_dist < 0.1:\n",
    "            tremor = np.random.normal(0, 0.1 + 0.3 * (1 - goal_dist/0.1), size=2)\n",
    "            noise += tremor\n",
    "        \n",
    "        # Final action with noise\n",
    "        action = optimal_action + noise\n",
    "        \n",
    "        # Clip to action space\n",
    "        return np.clip(action, -1, 1)\n",
    "\n",
    "\n",
    "# Assistance blending methods\n",
    "class AssistanceMethod:\n",
    "    def __init__(self, agent, human):\n",
    "        self.agent = agent\n",
    "        self.human = human\n",
    "        self.device = agent.device\n",
    "    \n",
    "    def reset(self):\n",
    "        self.agent.reset()\n",
    "    \n",
    "    def select_action(self, state, goals, true_goal, deterministic=False):\n",
    "        # This method should be overridden by subclasses\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class NoAssistance(AssistanceMethod):\n",
    "    def select_action(self, state, goals, true_goal, deterministic=False):\n",
    "        # Just return human action, ignore agent\n",
    "        human_action = self.human.get_action(state, true_goal)\n",
    "        \n",
    "        # Get belief for tracking purposes only\n",
    "        _, belief = self.agent.select_action(state, goals, deterministic)\n",
    "        \n",
    "        return human_action, belief\n",
    "\n",
    "\n",
    "class FixedBlending(AssistanceMethod):\n",
    "    def __init__(self, agent, human, blend_ratio=0.5):\n",
    "        super().__init__(agent, human)\n",
    "        self.blend_ratio = blend_ratio  # Fixed blend ratio\n",
    "    \n",
    "    def select_action(self, state, goals, true_goal, deterministic=False):\n",
    "        # Get agent action\n",
    "        agent_action, belief = self.agent.select_action(state, goals, deterministic)\n",
    "        \n",
    "        # Get human action\n",
    "        human_action = self.human.get_action(state, true_goal)\n",
    "        \n",
    "        # Blend with fixed ratio\n",
    "        gamma = self.blend_ratio\n",
    "        action = (1 - gamma) * human_action + gamma * agent_action\n",
    "        \n",
    "        # Safety check\n",
    "        if np.isnan(action).any():\n",
    "            action = human_action\n",
    "        \n",
    "        return action, belief\n",
    "\n",
    "\n",
    "class AdaptiveBlending(AssistanceMethod):\n",
    "    def __init__(self, agent, human, min_gamma=0.1, max_gamma=0.8):\n",
    "        super().__init__(agent, human)\n",
    "        self.min_gamma = min_gamma\n",
    "        self.max_gamma = max_gamma\n",
    "        \n",
    "        # Maintain a history of actions and goal distances\n",
    "        self.action_history = []\n",
    "        self.max_history = 10\n",
    "        \n",
    "        # Performance metrics for adaptation\n",
    "        self.success_counter = 0\n",
    "        self.failure_counter = 0\n",
    "    \n",
    "    def select_action(self, state, goals, true_goal, deterministic=False):\n",
    "        # Get agent action\n",
    "        agent_action, belief = self.agent.select_action(state, goals, deterministic)\n",
    "        \n",
    "        # Get human action\n",
    "        human_action = self.human.get_action(state, true_goal)\n",
    "        \n",
    "        # Extract end effector position\n",
    "                # Get end effector position - corrected to account for goal information\n",
    "        ee_pos = state[-2-2*self.num_goals:-2*self.num_goals]  # Position right before goal info\n",
    "                \n",
    "        # Calculate distance to true goal\n",
    "        dist_to_goal = np.linalg.norm(ee_pos - true_goal)\n",
    "        \n",
    "        # Store history for action consistency\n",
    "        self.action_history.append((human_action, agent_action, dist_to_goal))\n",
    "        if len(self.action_history) > self.max_history:\n",
    "            self.action_history.pop(0)\n",
    "        \n",
    "        # Calculate adaptive blending ratio based on multiple factors\n",
    "        if belief is not None:\n",
    "            # 1. Confidence-based component\n",
    "            max_belief_idx = np.argmax(belief)\n",
    "            confidence = belief[max_belief_idx]\n",
    "            confidence_factor = confidence\n",
    "            \n",
    "            # 2. Correct goal component (if agent believes in correct goal)\n",
    "            goal_correctness = 1.0 if max_belief_idx == np.where(goals == true_goal)[0][0] else 0.3\n",
    "            \n",
    "            # 3. Distance-based component (more assistance when closer to goal)\n",
    "            proximity_factor = np.clip(0.3 / (dist_to_goal + 0.3), 0, 1.0)\n",
    "            \n",
    "            # 4. Performance-based component\n",
    "            success_rate = self.success_counter / max(1, self.success_counter + self.failure_counter)\n",
    "            performance_factor = np.clip(success_rate, 0.3, 1.0)\n",
    "            \n",
    "            # 5. Action consistency component (prevent sudden changes)\n",
    "            consistency_factor = 1.0\n",
    "            if len(self.action_history) > 2:\n",
    "                # Check if human actions are consistent\n",
    "                recent_human = np.array([h for h, _, _ in self.action_history[-3:]])\n",
    "                human_std = np.std(recent_human, axis=0).mean()\n",
    "                if human_std < 0.2:  # Human is consistent, reduce AI intervention\n",
    "                    consistency_factor = 0.7\n",
    "            \n",
    "            # Combine all factors\n",
    "            gamma_factors = [\n",
    "                confidence_factor * 0.3,\n",
    "                goal_correctness * 0.2,\n",
    "                proximity_factor * 0.2,\n",
    "                performance_factor * 0.2,\n",
    "                consistency_factor * 0.1\n",
    "            ]\n",
    "            \n",
    "            gamma_raw = sum(gamma_factors)\n",
    "            \n",
    "            # Scale to desired range\n",
    "            gamma = self.min_gamma + (self.max_gamma - self.min_gamma) * gamma_raw\n",
    "            \n",
    "            # Special case: detect if stuck or making no progress\n",
    "            if len(self.action_history) > 5:\n",
    "                recent_distances = [d for _, _, d in self.action_history[-5:]]\n",
    "                if all(abs(d - recent_distances[0]) < 0.01 for d in recent_distances):\n",
    "                    # If stuck, alternate between more human and more AI\n",
    "                    if np.random.random() < 0.7:\n",
    "                        gamma = max(0.05, gamma * 0.5)  # Let human try more\n",
    "                    else:\n",
    "                        gamma = min(0.95, gamma * 1.5)  # Let AI try more\n",
    "        else:\n",
    "            # Default to minimum assistance if no belief\n",
    "            gamma = self.min_gamma\n",
    "        \n",
    "        # Safety checks before blending\n",
    "        agent_magnitude = np.linalg.norm(agent_action)\n",
    "        human_magnitude = np.linalg.norm(human_action)\n",
    "        \n",
    "        # If agent action is much larger than human, reduce its influence\n",
    "        if agent_magnitude > 2.0 * human_magnitude and human_magnitude > 0.1:\n",
    "            gamma *= 0.7\n",
    "        \n",
    "        # If agent and human actions point in opposite directions, reduce AI influence\n",
    "        dot_product = np.dot(agent_action, human_action)\n",
    "        agent_norm = np.linalg.norm(agent_action)\n",
    "        human_norm = np.linalg.norm(human_action)\n",
    "        if agent_norm > 0.1 and human_norm > 0.1:\n",
    "            cosine = dot_product / (agent_norm * human_norm)\n",
    "            if cosine < -0.5:  # Actions disagree significantly\n",
    "                gamma *= 0.5\n",
    "        \n",
    "        # Blend with adaptive ratio\n",
    "        action = (1 - gamma) * human_action + gamma * agent_action\n",
    "        \n",
    "        # Safety check\n",
    "        if np.isnan(action).any():\n",
    "            action = human_action\n",
    "        \n",
    "        return action, belief\n",
    "    \n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "        self.action_history = []\n",
    "        \n",
    "    def record_outcome(self, success):\n",
    "        \"\"\"Record outcome for adaptation.\"\"\"\n",
    "        if success:\n",
    "            self.success_counter += 1\n",
    "        else:\n",
    "            self.failure_counter += 1\n",
    "\n",
    "\n",
    "class PhasedTraining:\n",
    "    def __init__(self, env, state_dim, action_dim, log_dir=\"results\"):\n",
    "        self.env = env\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.log_dir = log_dir\n",
    "        \n",
    "        # Create separate directories for phase logs\n",
    "        self.phase1_dir = f\"{log_dir}/phase1\"\n",
    "        self.phase2_dir = f\"{log_dir}/phase2\"\n",
    "        self.phase3_dir = f\"{log_dir}/phase3\"\n",
    "        self.comparison_dir = f\"{log_dir}/comparison\"\n",
    "        \n",
    "        for dir_path in [self.phase1_dir, self.phase2_dir, self.phase3_dir, self.comparison_dir]:\n",
    "            os.makedirs(dir_path, exist_ok=True)\n",
    "    \n",
    "    def train_phase1(self, num_episodes=2000):\n",
    "        \"\"\"Phase 1: Train pure PPO without beliefs to establish a baseline.\"\"\"\n",
    "        print(\"\\n=== Phase 1: Training Pure PPO ===\")\n",
    "        \n",
    "        # Create pure PPO agent without belief module\n",
    "        agent = PPOAgent(\n",
    "            state_dim=self.state_dim,\n",
    "            action_dim=self.action_dim,\n",
    "            hidden_dim=256,\n",
    "            num_goals=NUM_GOALS,\n",
    "            lr=LR,\n",
    "            gamma=GAMMA,\n",
    "            gae_lambda=GAE_LAMBDA,\n",
    "            clip_epsilon=CLIP_EPSILON,\n",
    "            value_coef=VALUE_COEF,\n",
    "            entropy_coef=ENTROPY_COEF,\n",
    "            target_kl=TARGET_KL,\n",
    "            use_belief=False\n",
    "        )\n",
    "        \n",
    "        # Train the pure PPO agent - ensure it learns the basic task first\n",
    "        tracker = self._train_agent(\n",
    "            agent=agent,\n",
    "            num_episodes=num_episodes,\n",
    "            early_stop_target=0.75,  # Target 75% success rate\n",
    "            log_dir=self.phase1_dir\n",
    "        )\n",
    "        \n",
    "        # Final evaluation to verify performance\n",
    "        print(\"Performing detailed evaluation of Phase 1 agent...\")\n",
    "        eval_results = evaluate(self.env, agent, num_episodes=100, max_steps=500)\n",
    "        print(f\"Pure PPO Success Rate: {eval_results['success_rate']:.4f}\")\n",
    "        \n",
    "        # Save the final model\n",
    "        agent.save(f\"{self.phase1_dir}/final_model.pt\")\n",
    "        \n",
    "        return agent\n",
    "    \n",
    "    def train_phase2(self, base_agent, num_episodes=2000):\n",
    "        \"\"\"Phase 2: Add belief module jointly trained with PPO.\"\"\"\n",
    "        print(\"\\n=== Phase 2: Training with Belief Module ===\")\n",
    "        \n",
    "        # Create agent with belief module (transfer learning from base agent)\n",
    "        belief_agent = PPOAgent(\n",
    "            state_dim=self.state_dim,\n",
    "            action_dim=self.action_dim,\n",
    "            hidden_dim=256,\n",
    "            num_goals=NUM_GOALS,\n",
    "            lr=LR * 0.5,  # Lower learning rate for fine-tuning\n",
    "            gamma=GAMMA,\n",
    "            gae_lambda=GAE_LAMBDA,\n",
    "            clip_epsilon=CLIP_EPSILON * 0.8,  # Tighter clips for fine-tuning\n",
    "            value_coef=VALUE_COEF,\n",
    "            entropy_coef=ENTROPY_COEF * 0.7,  # Lower entropy to exploit learned policy\n",
    "            target_kl=TARGET_KL,\n",
    "            use_belief=True  # Now with belief module\n",
    "        )\n",
    "        \n",
    "        # Copy parameters from base agent to new agent\n",
    "        belief_agent.actor_critic.feature_network.load_state_dict(\n",
    "            base_agent.actor_critic.feature_network.state_dict()\n",
    "        )\n",
    "        belief_agent.actor_critic.actor_mean.load_state_dict(\n",
    "            base_agent.actor_critic.actor_mean.state_dict()\n",
    "        )\n",
    "        belief_agent.actor_critic.actor_log_std.data = base_agent.actor_critic.actor_log_std.data.clone()\n",
    "        belief_agent.actor_critic.critic.load_state_dict(\n",
    "            base_agent.actor_critic.critic.state_dict()\n",
    "        )\n",
    "        \n",
    "        # Phase 2A: Train belief module first with a higher learning rate\n",
    "        print(\"Phase 2A: Training belief module...\")\n",
    "        for param in belief_agent.actor_critic.parameters():\n",
    "            param.requires_grad = False  # Freeze actor-critic\n",
    "            \n",
    "        # Create a separate optimizer for the belief module\n",
    "        belief_optimizer = optim.Adam(belief_agent.belief_module.parameters(), lr=LR*2)\n",
    "        \n",
    "        # Train for a few hundred episodes with only belief module learning\n",
    "        belief_pretraining_episodes = 300\n",
    "        \n",
    "        # Custom training loop for belief module only\n",
    "        tracker = self._train_agent(\n",
    "            agent=belief_agent,\n",
    "            num_episodes=belief_pretraining_episodes,\n",
    "            early_stop_target=0.5,  # Lower goal for this phase\n",
    "            log_dir=f\"{self.phase2_dir}/belief_only\",\n",
    "            custom_optimizer=belief_optimizer\n",
    "        )\n",
    "        \n",
    "        # Phase 2B: Unfreeze actor-critic and train jointly\n",
    "        print(\"Phase 2B: Joint training of belief and policy...\")\n",
    "        for param in belief_agent.actor_critic.parameters():\n",
    "            param.requires_grad = True  # Unfreeze actor-critic\n",
    "            \n",
    "        # Reset the regular optimizer\n",
    "        belief_agent.optimizer = optim.Adam(\n",
    "            list(belief_agent.actor_critic.parameters()) + list(belief_agent.belief_module.parameters()),\n",
    "            lr=LR * 0.5\n",
    "        )\n",
    "        \n",
    "        # Continue training with everything unfrozen\n",
    "        tracker = self._train_agent(\n",
    "            agent=belief_agent,\n",
    "            num_episodes=num_episodes - belief_pretraining_episodes,\n",
    "            early_stop_target=0.65,  # Target for full agent\n",
    "            log_dir=self.phase2_dir\n",
    "        )\n",
    "        \n",
    "        # Final detailed evaluation\n",
    "        print(\"Performing detailed evaluation of Phase 2 agent...\")\n",
    "        eval_results = evaluate(self.env, belief_agent, num_episodes=100, max_steps=500)\n",
    "        print(f\"Belief-Enabled Agent Success Rate: {eval_results['success_rate']:.4f}\")\n",
    "        print(f\"Belief Accuracy: {eval_results.get('belief_accuracy', 'N/A')}\")\n",
    "        \n",
    "        # Save the final model\n",
    "        belief_agent.save(f\"{self.phase2_dir}/final_model.pt\")\n",
    "        \n",
    "        return belief_agent\n",
    "    \n",
    "    def evaluate_assistance_methods(self, agent, num_episodes=50):\n",
    "        \"\"\"Compare different assistance methods using the trained agent.\"\"\"\n",
    "        print(\"\\n=== Evaluating Assistance Methods ===\")\n",
    "        \n",
    "        # Create human simulator\n",
    "        human = HumanSimulator(noise_level=0.2)\n",
    "        \n",
    "        # Create evaluation environment\n",
    "        eval_env = MultiGoalReacherEnv(num_goals=NUM_GOALS)\n",
    "        \n",
    "        # Define assistance methods to evaluate\n",
    "        methods = {\n",
    "            'No Assistance': NoAssistance(agent, human),\n",
    "            'Fixed 50-50': FixedBlending(agent, human, blend_ratio=0.5),\n",
    "            'Adaptive': AdaptiveBlending(agent, human, min_gamma=0.1, max_gamma=0.8)\n",
    "        }\n",
    "        \n",
    "        # Evaluate each method\n",
    "        results = {}\n",
    "        for name, method in methods.items():\n",
    "            print(f\"\\nEvaluating: {name}\")\n",
    "            method_results = self._evaluate_assistance(\n",
    "                eval_env, method, num_episodes=num_episodes\n",
    "            )\n",
    "            results[name] = method_results\n",
    "            \n",
    "            # Print key metrics\n",
    "            print(f\"  Success rate: {method_results['success_rate']:.4f}\")\n",
    "            print(f\"  Episode reward: {method_results['episode_reward']:.4f}\")\n",
    "            print(f\"  Completion time: {method_results['completion_time']:.4f}\")\n",
    "            if 'belief_accuracy' in method_results:\n",
    "                print(f\"  Belief accuracy: {method_results['belief_accuracy']:.4f}\")\n",
    "        \n",
    "        # Plot comparison\n",
    "        tracker = PerformanceTracker(log_dir=self.comparison_dir)\n",
    "        tracker.plot_comparison(results)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _train_agent(self, agent, num_episodes, early_stop_target, log_dir, custom_optimizer=None):\n",
    "        \"\"\"Train an agent with early stopping based on success rate.\"\"\"\n",
    "        tracker = PerformanceTracker(log_dir=log_dir)\n",
    "        \n",
    "        # Define parameters\n",
    "        max_steps = 500\n",
    "        eval_interval = EVAL_INTERVAL\n",
    "        patience = EARLY_STOP_PATIENCE\n",
    "        \n",
    "        best_success_rate = 0\n",
    "        best_reward = -float('inf')\n",
    "        episodes_without_improvement = 0\n",
    "        \n",
    "        # For tracking progress\n",
    "        total_steps = 0\n",
    "        \n",
    "        # Minimum episodes to train before considering early stopping\n",
    "        min_episodes = MIN_TRAINING_EPISODES  # Using constant from parameters\n",
    "        # Minimum number of evaluation episodes to trust the results\n",
    "        min_eval_episodes = 50  # Increased for more reliable evaluation\n",
    "        # Number of consecutive evaluations above target to consider it reached\n",
    "        consecutive_target_reached = 0\n",
    "        required_consecutive = 3  # Require meeting target 3 times in a row\n",
    "        \n",
    "        # Use custom optimizer if provided (for specialized training phases)\n",
    "        if custom_optimizer is not None:\n",
    "            optimizer = custom_optimizer\n",
    "        else:\n",
    "            optimizer = agent.optimizer\n",
    "        \n",
    "        # Training loop\n",
    "        for episode in tqdm(range(1, num_episodes + 1), desc=\"Training\"):\n",
    "            state, _ = self.env.reset()\n",
    "            agent.reset()\n",
    "            \n",
    "            episode_reward = 0\n",
    "            episode_length = 0\n",
    "            episode_beliefs = []\n",
    "            \n",
    "            # Extract goals from the environment\n",
    "            goals = []\n",
    "            for i in range(NUM_GOALS):\n",
    "                goal_idx = state.shape[0] - 2 * NUM_GOALS + 2 * i\n",
    "                goals.append(state[goal_idx:goal_idx + 2])\n",
    "            \n",
    "            # Episode loop\n",
    "            for step in range(max_steps):\n",
    "                # Select action\n",
    "                action, belief = agent.select_action(state, goals)\n",
    "                \n",
    "                # Check for NaN values in action\n",
    "                if np.isnan(action).any():\n",
    "                    action = np.zeros_like(action)\n",
    "                \n",
    "                # Get action log probability and value\n",
    "                with torch.no_grad():\n",
    "                    state_tensor = torch.FloatTensor(state).unsqueeze(0).to(agent.device)\n",
    "                    belief_tensor = None\n",
    "                    if belief is not None:\n",
    "                        belief_tensor = torch.FloatTensor(belief).unsqueeze(0).to(agent.device)\n",
    "                    action_tensor = torch.FloatTensor(action).unsqueeze(0).to(agent.device)\n",
    "                    \n",
    "                    log_prob, _, value = agent.actor_critic.evaluate_actions(\n",
    "                        state_tensor, action_tensor, belief_tensor\n",
    "                    )\n",
    "                    log_prob = log_prob.squeeze().cpu().item()\n",
    "                    value = value.squeeze().cpu().item()\n",
    "                \n",
    "                # Take action in environment\n",
    "                next_state, reward, done, truncated, info = self.env.step(action)\n",
    "                \n",
    "                # Store experience for learning\n",
    "                agent.remember(state, action, reward, value, log_prob, belief, done)\n",
    "                \n",
    "                # Update state and counters\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "                episode_length += 1\n",
    "                total_steps += 1\n",
    "                \n",
    "                # Store belief accuracy\n",
    "                if belief is not None:\n",
    "                    true_goal_idx = self.env.true_goal_idx\n",
    "                    predicted_goal_idx = np.argmax(belief)\n",
    "                    episode_beliefs.append(predicted_goal_idx == true_goal_idx)\n",
    "                \n",
    "                # Update policy if enough steps have been taken\n",
    "                if agent.steps_since_update >= STEPS_PER_UPDATE:\n",
    "                    # If using custom optimizer\n",
    "                    if custom_optimizer is not None:\n",
    "                        # Manual update for specialized training (e.g., belief module only)\n",
    "                        with torch.no_grad():\n",
    "                            if len(agent.states) > 0:\n",
    "                                state = torch.FloatTensor(agent.states[-1]).unsqueeze(0).to(agent.device)\n",
    "                                belief = None\n",
    "                                if agent.use_belief and len(agent.beliefs) > 0:\n",
    "                                    belief = torch.FloatTensor(agent.beliefs[-1]).unsqueeze(0).to(agent.device)\n",
    "                                _, _, next_value = agent.actor_critic(state, belief)\n",
    "                                next_value = next_value.squeeze().cpu().item()\n",
    "                            else:\n",
    "                                next_value = 0\n",
    "                        \n",
    "                        # Compute returns\n",
    "                        returns = agent.compute_gae(next_value)\n",
    "                        \n",
    "                        # Convert lists to tensors\n",
    "                        states = torch.FloatTensor(np.array(agent.states)).to(agent.device)\n",
    "                        actions = torch.FloatTensor(np.array(agent.actions)).to(agent.device)\n",
    "                        returns = torch.FloatTensor(returns).unsqueeze(1).to(agent.device)\n",
    "                        \n",
    "                        # Prepare beliefs if used\n",
    "                        beliefs = None\n",
    "                        if agent.use_belief and len(agent.beliefs) > 0:\n",
    "                            beliefs = torch.FloatTensor(np.array(agent.beliefs)).to(agent.device)\n",
    "                        \n",
    "                        # Custom update for belief module\n",
    "                        if agent.use_belief and agent.belief_module:\n",
    "                            # Batch size for updates\n",
    "                            batch_size = min(BATCH_SIZE, len(agent.states))\n",
    "                            indices = np.arange(len(agent.states))\n",
    "                            np.random.shuffle(indices)\n",
    "                            \n",
    "                            # Mini-batch training for belief module\n",
    "                            total_loss = 0\n",
    "                            policy_loss = 0\n",
    "                            belief_loss = 0\n",
    "                            \n",
    "                            for idx in range(0, len(indices), batch_size):\n",
    "                                batch_indices = indices[idx:idx + batch_size]\n",
    "                                batch_states = states[batch_indices]\n",
    "                                batch_actions = actions[batch_indices]\n",
    "                                \n",
    "                                if beliefs is not None:\n",
    "                                    batch_beliefs = beliefs[batch_indices]\n",
    "                                    \n",
    "                                    # Extract end effector positions and goals\n",
    "                                    ee_positions = batch_states[:, -4:-2]\n",
    "                                    goal_indices = []\n",
    "                                    for i in range(NUM_GOALS):\n",
    "                                        goal_idx = batch_states.shape[1] - 2 * NUM_GOALS + 2 * i\n",
    "                                        goal_indices.append(goal_idx)\n",
    "                                    \n",
    "                                    # Collect belief module losses\n",
    "                                    belief_loss_total = 0\n",
    "                                    \n",
    "                                    # Update belief module here\n",
    "                                    custom_optimizer.zero_grad()\n",
    "                                    \n",
    "                                    # Special belief training code\n",
    "                                    batch_size = batch_states.shape[0]\n",
    "                                    for i in range(batch_size):\n",
    "                                        state = ee_positions[i].view(-1)\n",
    "                                        action = batch_actions[i].view(-1)\n",
    "                                        \n",
    "                                        # Extract goals for this sample\n",
    "                                        goals = []\n",
    "                                        for g_idx in range(NUM_GOALS):\n",
    "                                            goal_start = goal_indices[g_idx]\n",
    "                                            goals.append(batch_states[i, goal_start:goal_start+2])\n",
    "                                        \n",
    "                                        # Assuming the belief network always outputs higher probability for correct goal\n",
    "                                        # This is a simple auxiliary loss that may not be optimal\n",
    "                                        if random.random() < 0.5:  # Only use half the samples for efficiency\n",
    "                                            updated_belief = agent.belief_module(state, action, goals, batch_beliefs[i])\n",
    "                                            \n",
    "                                            # Create target distribution (one-hot for true goal)\n",
    "                                            # In a real scenario, this would come from human feedback\n",
    "                                            # For now, we'll use a heuristic based on rewards or progress\n",
    "                                            target_belief = torch.zeros_like(updated_belief)\n",
    "                                            \n",
    "                                            # Find goal closest to the action direction\n",
    "                                            action_dir = action / (torch.norm(action) + 1e-6)\n",
    "                                            goal_costs = []\n",
    "                                            \n",
    "                                            for goal in goals:\n",
    "                                                goal_dir = goal - state\n",
    "                                                goal_dist = torch.norm(goal_dir) + 1e-6\n",
    "                                                goal_dir = goal_dir / goal_dist\n",
    "                                                alignment = torch.dot(action_dir, goal_dir)\n",
    "                                                goal_costs.append(-alignment)  # Negative cost for better alignment\n",
    "                                            \n",
    "                                            # Select most likely goal based on action alignment\n",
    "                                            best_goal_idx = torch.argmin(torch.tensor(goal_costs)).item()\n",
    "                                            target_belief[best_goal_idx] = 1.0\n",
    "                                            \n",
    "                                            # Cross entropy loss\n",
    "                                            loss = F.kl_div(updated_belief.log(), target_belief, reduction='batchmean')\n",
    "                                            belief_loss_total += loss\n",
    "                                    \n",
    "                                    if belief_loss_total > 0:\n",
    "                                        belief_loss_total.backward()\n",
    "                                        torch.nn.utils.clip_grad_norm_(agent.belief_module.parameters(), 1.0)\n",
    "                                        custom_optimizer.step()\n",
    "                                        \n",
    "                                        belief_loss += belief_loss_total.item()\n",
    "                                    \n",
    "                            # Report metrics\n",
    "                            loss_info = {\n",
    "                                'total_loss': belief_loss,\n",
    "                                'belief_loss': belief_loss,\n",
    "                                'policy_loss': 0,\n",
    "                                'value_loss': 0,\n",
    "                                'entropy_loss': 0,\n",
    "                                'approx_kl': 0\n",
    "                            }\n",
    "                            \n",
    "                            tracker.add_training_metrics(loss_info)\n",
    "                        \n",
    "                        # Clear memory\n",
    "                        agent.clear_memory()\n",
    "                    else:\n",
    "                        # Normal PPO update\n",
    "                        loss_info = agent.update()\n",
    "                        tracker.add_training_metrics(loss_info)\n",
    "                \n",
    "                # Check if episode is done\n",
    "                if done or truncated:\n",
    "                    break\n",
    "            \n",
    "            # Calculate episode metrics\n",
    "            success = info.get('goal_reached', False)\n",
    "            collision = info.get('collision', False)\n",
    "            belief_accuracy = np.mean(episode_beliefs) if episode_beliefs else None\n",
    "            \n",
    "            # Add episode metrics to tracker\n",
    "            tracker.add_episode_metrics(\n",
    "                episode_reward, episode_length, success, collision, belief_accuracy\n",
    "            )\n",
    "            \n",
    "            # Log progress\n",
    "            if episode % 50 == 0:\n",
    "                metrics = tracker.get_recent_metrics(window=min(50, episode))\n",
    "                print(f\"Episode {episode}, Steps: {total_steps}, Reward: {metrics['reward']:.2f}, \"\n",
    "                      f\"Success: {metrics['success_rate']:.2f}, \"\n",
    "                      f\"Length: {metrics['episode_length']:.2f}\")\n",
    "                \n",
    "                # Plot learning curves\n",
    "                tracker.plot_learning_curves()\n",
    "            \n",
    "            # Evaluate agent periodically - but only after minimum training\n",
    "            if episode % eval_interval == 0 and episode >= min_episodes:\n",
    "                eval_results = evaluate(self.env, agent, num_episodes=min_eval_episodes, max_steps=max_steps)\n",
    "                \n",
    "                # Check if target is reached consistently\n",
    "                if eval_results['success_rate'] >= early_stop_target:\n",
    "                    consecutive_target_reached += 1\n",
    "                    print(f\"Target success rate reached ({consecutive_target_reached}/{required_consecutive})\")\n",
    "                else:\n",
    "                    consecutive_target_reached = 0\n",
    "                \n",
    "                # Check for improvement\n",
    "                improved = False\n",
    "                if eval_results['success_rate'] > best_success_rate + IMPROVEMENT_THRESHOLD:\n",
    "                    best_success_rate = eval_results['success_rate']\n",
    "                    agent.save(f\"{log_dir}/best_model_success.pt\")\n",
    "                    print(f\"New best success rate: {best_success_rate:.4f}\")\n",
    "                    improved = True\n",
    "                \n",
    "                if eval_results['episode_reward'] > best_reward + IMPROVEMENT_THRESHOLD * 100:\n",
    "                    best_reward = eval_results['episode_reward']\n",
    "                    if not improved:  # Only save if not already saved for success rate\n",
    "                        agent.save(f\"{log_dir}/best_model_reward.pt\")\n",
    "                        print(f\"New best reward: {best_reward:.4f}\")\n",
    "                    improved = True\n",
    "                \n",
    "                if not improved:\n",
    "                    episodes_without_improvement += 1\n",
    "                    print(f\"No improvement for {episodes_without_improvement} evaluations\")\n",
    "                else:\n",
    "                    episodes_without_improvement = 0\n",
    "                \n",
    "                # Early stopping based on patience\n",
    "                if episodes_without_improvement >= patience:\n",
    "                    print(f\"Early stopping after {episode} episodes due to no improvement\")\n",
    "                    break\n",
    "                \n",
    "                # Early stopping based on consistently reaching target\n",
    "                if consecutive_target_reached >= required_consecutive:\n",
    "                    print(f\"Target success rate {early_stop_target} reached consistently!\")\n",
    "                    break\n",
    "        \n",
    "        # Final evaluation with more episodes for reliable results\n",
    "        final_eval = evaluate(self.env, agent, num_episodes=50, max_steps=max_steps)\n",
    "        \n",
    "        # Save final model\n",
    "        agent.save(f\"{log_dir}/final_model.pt\")\n",
    "        \n",
    "        # Print final evaluation results\n",
    "        print(\"\\nFinal Evaluation Results:\")\n",
    "        for key, value in final_eval.items():\n",
    "            print(f\"{key}: {value:.4f}\")\n",
    "        \n",
    "        return tracker\n",
    "    \n",
    "    def _evaluate_assistance(self, env, method, num_episodes=50, max_steps=500):\n",
    "        \"\"\"Evaluate an assistance method.\"\"\"\n",
    "        rewards = []\n",
    "        lengths = []\n",
    "        successes = []\n",
    "        collisions = []\n",
    "        belief_accuracies = []\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            state, _ = env.reset()\n",
    "            method.reset()\n",
    "            \n",
    "            episode_reward = 0\n",
    "            episode_length = 0\n",
    "            episode_beliefs = []\n",
    "            \n",
    "            # Extract goals\n",
    "            goals = []\n",
    "            for i in range(NUM_GOALS):\n",
    "                goal_idx = state.shape[0] - 2 * NUM_GOALS + 2 * i\n",
    "                goals.append(state[goal_idx:goal_idx + 2])\n",
    "            \n",
    "            # Episode loop\n",
    "            for step in range(max_steps):\n",
    "                # Get assisted action\n",
    "                action, belief = method.select_action(\n",
    "                    state, goals, env.true_goal, deterministic=True\n",
    "                )\n",
    "                \n",
    "                # Take action\n",
    "                next_state, reward, done, truncated, info = env.step(action)\n",
    "                \n",
    "                # Update state and counters\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "                episode_length += 1\n",
    "                \n",
    "                # Track belief accuracy\n",
    "                if belief is not None:\n",
    "                    true_goal_idx = env.true_goal_idx\n",
    "                    predicted_goal_idx = np.argmax(belief)\n",
    "                    episode_beliefs.append(predicted_goal_idx == true_goal_idx)\n",
    "                \n",
    "                if done or truncated:\n",
    "                    break\n",
    "            \n",
    "            # Calculate episode metrics\n",
    "            success = info.get('goal_reached', False)\n",
    "            collision = info.get('collision', False)\n",
    "            \n",
    "            # Store metrics\n",
    "            rewards.append(episode_reward)\n",
    "            lengths.append(episode_length)\n",
    "            successes.append(1 if success else 0)\n",
    "            collisions.append(1 if collision else 0)\n",
    "            \n",
    "            if episode_beliefs:\n",
    "                belief_accuracies.append(np.mean(episode_beliefs))\n",
    "        \n",
    "        # Calculate aggregate metrics\n",
    "        success_rate = np.mean(successes)\n",
    "        successful_episode_lengths = [l for l, s in zip(lengths, successes) if s]\n",
    "        \n",
    "        results = {\n",
    "            'episode_reward': np.mean(rewards),\n",
    "            'episode_length': np.mean(lengths),\n",
    "            'success_rate': success_rate,\n",
    "            'collision_rate': np.mean(collisions),\n",
    "            'completion_time': np.mean(successful_episode_lengths) if successful_episode_lengths else float('inf')\n",
    "        }\n",
    "        \n",
    "        if belief_accuracies:\n",
    "            results['belief_accuracy'] = np.mean(belief_accuracies)\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Check if visualization is possible\n",
    "    render_mode = None\n",
    "    visualization_enabled = VISUALIZE\n",
    "    \n",
    "    if visualization_enabled:\n",
    "        try:\n",
    "            import pygame\n",
    "            pygame.init()\n",
    "            render_mode = \"rgb_array\"\n",
    "        except ImportError:\n",
    "            print(\"Warning: Pygame not found. Visualization will be disabled.\")\n",
    "            visualization_enabled = False\n",
    "    \n",
    "    # Create environment with reproducible seeds\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    random.seed(SEED)\n",
    "    \n",
    "    env = MultiGoalReacherEnv(num_goals=NUM_GOALS, render_mode=render_mode)\n",
    "    \n",
    "    # Get dimensions\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    \n",
    "    # Run phased training\n",
    "    trainer = PhasedTraining(env, state_dim, action_dim)\n",
    "    \n",
    "    # Phase 1: Train pure PPO without beliefs - longer training\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PHASE 1: Establishing solid PPO baseline\")\n",
    "    print(\"=\"*50)\n",
    "    ppo_agent = trainer.train_phase1(num_episodes=2000)\n",
    "    \n",
    "    # Verify baseline performance before proceeding\n",
    "    eval_results = evaluate(env, ppo_agent, num_episodes=100)\n",
    "    if eval_results['success_rate'] < 0.6:\n",
    "        print(\"\\nWARNING: Pure PPO agent performance is below target.\")\n",
    "        print(\"Consider running Phase 1 again with different hyperparameters.\")\n",
    "        print(\"Proceeding anyway, but results may be suboptimal.\")\n",
    "    else:\n",
    "        print(f\"\\nPure PPO baseline established successfully!\")\n",
    "        print(f\"Success rate: {eval_results['success_rate']:.2f}\")\n",
    "    \n",
    "    # Phase 2: Add belief module jointly trained with PPO\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PHASE 2: Training with belief module\")\n",
    "    print(\"=\"*50)\n",
    "    belief_agent = trainer.train_phase2(ppo_agent, num_episodes=2000)\n",
    "    \n",
    "    # Evaluate assistance methods with more episodes for reliable results\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"FINAL PHASE: Evaluating assistance methods\")\n",
    "    print(\"=\"*50)\n",
    "    results = trainer.evaluate_assistance_methods(belief_agent, num_episodes=100)\n",
    "    \n",
    "    # Print summary of results\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"SUMMARY OF RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Compare assistance methods\n",
    "    methods = sorted(results.keys(), key=lambda x: results[x]['success_rate'], reverse=True)\n",
    "    print(f\"Best method: {methods[0]} with {results[methods[0]]['success_rate']:.2f} success rate\")\n",
    "    \n",
    "    # Print all methods ordered by success rate\n",
    "    print(\"\\nAll methods ranked by success rate:\")\n",
    "    for i, method in enumerate(methods, 1):\n",
    "        print(f\"{i}. {method}: {results[method]['success_rate']:.2f} success, \" + \n",
    "              f\"{results[method]['episode_reward']:.1f} reward, \" +\n",
    "              f\"{results[method]['completion_time']:.1f} completion time\")\n",
    "    \n",
    "    # Report belief accuracy if available\n",
    "    if 'belief_accuracy' in results[methods[0]]:\n",
    "        print(f\"\\nBelief accuracy: {results[methods[0]]['belief_accuracy']:.2f}\")\n",
    "    \n",
    "    # Close environment\n",
    "    env.close()\n",
    "    \n",
    "    print(\"\\nTraining and evaluation completed!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
