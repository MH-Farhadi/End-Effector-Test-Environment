{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SHARED AUTONOMY EXPERIMENT\n",
      "======================================================================\n",
      "\n",
      "[1/3] Training Bayesian goal inference model...\n",
      "Training Bayesian goal inference model...\n",
      "Generating trajectory 50/50\n",
      "Generated synthetic dataset with 50 trajectories\n",
      "Saved synthetic dataset to datasets/synthetic_trajectories.pkl\n",
      "\n",
      "Training confidence calibration...\n",
      "Calibrator trained and saved\n",
      "Saved trained Bayesian model to models/bayesian_model.pkl\n",
      "\n",
      "Evaluating model on test set...\n",
      "\n",
      "Model performance at different path completion percentages:\n",
      "  25% completion: 0.800 accuracy\n",
      "  50% completion: 0.500 accuracy\n",
      "  75% completion: 0.600 accuracy\n",
      "  100% completion: 0.600 accuracy\n",
      "\n",
      "Visualizing model performance...\n",
      "\n",
      "[2/3] Training shared autonomy PPO model...\n",
      "Initializing shared autonomy environment...\n",
      "Loaded Bayesian model from models/bayesian_model.pkl\n",
      "Initializing PPO model...\n",
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mhfar\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\policies.py:460: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 10000 timesteps...\n",
      "Logging to ./tensorboard_logs/PPO_1\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 522  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 1024 |\n",
      "-----------------------------\n",
      "--------------------------------------\n",
      "| time/                   |          |\n",
      "|    fps                  | 485      |\n",
      "|    iterations           | 2        |\n",
      "|    time_elapsed         | 4        |\n",
      "|    total_timesteps      | 2048     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 90.83863 |\n",
      "|    clip_fraction        | 0.969    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -1.42    |\n",
      "|    explained_variance   | 0.93     |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 1.13     |\n",
      "|    n_updates            | 5        |\n",
      "|    policy_gradient_loss | 0.301    |\n",
      "|    std                  | 1        |\n",
      "|    value_loss           | 7.5      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 484       |\n",
      "|    iterations           | 3         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 3072      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 13.286042 |\n",
      "|    clip_fraction        | 0.736     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.43     |\n",
      "|    explained_variance   | 0.909     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.65      |\n",
      "|    n_updates            | 10        |\n",
      "|    policy_gradient_loss | 0.244     |\n",
      "|    std                  | 1.02      |\n",
      "|    value_loss           | 4.53      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                   |          |\n",
      "|    fps                  | 483      |\n",
      "|    iterations           | 4        |\n",
      "|    time_elapsed         | 8        |\n",
      "|    total_timesteps      | 4096     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 165.8281 |\n",
      "|    clip_fraction        | 0.904    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -1.44    |\n",
      "|    explained_variance   | 0.89     |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 1        |\n",
      "|    n_updates            | 15       |\n",
      "|    policy_gradient_loss | 0.327    |\n",
      "|    std                  | 1.02     |\n",
      "|    value_loss           | 5.87     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                   |          |\n",
      "|    fps                  | 484      |\n",
      "|    iterations           | 5        |\n",
      "|    time_elapsed         | 10       |\n",
      "|    total_timesteps      | 5120     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 91.52392 |\n",
      "|    clip_fraction        | 0.942    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -1.44    |\n",
      "|    explained_variance   | 0.93     |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 4.36     |\n",
      "|    n_updates            | 20       |\n",
      "|    policy_gradient_loss | 0.308    |\n",
      "|    std                  | 1.03     |\n",
      "|    value_loss           | 4.52     |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.22020082 |\n",
      "|    clip_fraction        | 0.551      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.44      |\n",
      "|    explained_variance   | 0.906      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.396      |\n",
      "|    n_updates            | 25         |\n",
      "|    policy_gradient_loss | 0.115      |\n",
      "|    std                  | 1.03       |\n",
      "|    value_loss           | 2.24       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 476         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 7168        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.047555894 |\n",
      "|    clip_fraction        | 0.511       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.834       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.58        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | 0.133       |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 2.73        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 478        |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 17         |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07937043 |\n",
      "|    clip_fraction        | 0.422      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.45      |\n",
      "|    explained_variance   | 0.936      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.39       |\n",
      "|    n_updates            | 35         |\n",
      "|    policy_gradient_loss | 0.058      |\n",
      "|    std                  | 1.04       |\n",
      "|    value_loss           | 1.2        |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 479       |\n",
      "|    iterations           | 9         |\n",
      "|    time_elapsed         | 19        |\n",
      "|    total_timesteps      | 9216      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8950318 |\n",
      "|    clip_fraction        | 0.527     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.46     |\n",
      "|    explained_variance   | 0.967     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.288     |\n",
      "|    n_updates            | 40        |\n",
      "|    policy_gradient_loss | 0.1       |\n",
      "|    std                  | 1.04      |\n",
      "|    value_loss           | 0.906     |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 479         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.059401587 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | 0.961       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.629       |\n",
      "|    n_updates            | 45          |\n",
      "|    policy_gradient_loss | 0.0368      |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 2.31        |\n",
      "-----------------------------------------\n",
      "Model saved to models\\shared_autonomy_model\n",
      "Metrics saved to 'training_metrics/'\n",
      "\n",
      "[3/3] Evaluating model...\n",
      "Evaluating model models/shared_autonomy_model...\n",
      "Loaded model from models/shared_autonomy_model\n",
      "Episode 10/10\n",
      "Evaluation Results:\n",
      "  Success Rate: 0.100\n",
      "  Collision Rate: 0.000\n",
      "  Timeout Rate: 0.900\n",
      "  Average Reward: 17.163\n",
      "  Average Steps: 194.4\n",
      "  Average Gamma: 1.000\n",
      "  Goal Inference Accuracy: 0.366\n",
      "Saved evaluation metrics to evaluation_results/metrics.json\n",
      "Generating gamma heatmap...\n",
      "Saved gamma heatmap to evaluation_results/gamma_heatmap.png\n",
      "\n",
      "Experiment complete!\n",
      "Results are saved in:\n",
      "- bayesian_results/ (Bayesian model performance)\n",
      "- training_metrics/ (training curves)\n",
      "- models/ (trained models)\n",
      "- evaluation_results/ (evaluation results and heatmaps)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import entropy\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "import pickle\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Optional visualization\n",
    "try:\n",
    "    import pygame\n",
    "except ImportError:\n",
    "    pygame = None\n",
    "\n",
    "###############################################################################\n",
    "# CONSTANTS & UTILS\n",
    "###############################################################################\n",
    "FULL_VIEW_SIZE = (800, 600)\n",
    "DOT_RADIUS = 15\n",
    "TARGET_RADIUS = 10\n",
    "OBSTACLE_RADIUS = 10\n",
    "COLLISION_BUFFER = 5\n",
    "MAX_SPEED = 3\n",
    "NOISE_MAGNITUDE = 2.5\n",
    "RENDER_FPS = 30\n",
    "\n",
    "START_POS = np.array([FULL_VIEW_SIZE[0]//2, FULL_VIEW_SIZE[1]//2], dtype=np.float32)\n",
    "\n",
    "# Colors\n",
    "WHITE = (255, 255, 255)\n",
    "GRAY = (128, 128, 128)\n",
    "YELLOW = (255, 255, 0)\n",
    "BLACK = (0, 0, 0)\n",
    "RED = (255, 0, 0)\n",
    "GREEN = (0, 255, 0)\n",
    "BLUE = (0, 0, 255)\n",
    "\n",
    "def distance(a, b):\n",
    "    \"\"\"Calculate Euclidean distance between two points.\"\"\"\n",
    "    return math.hypot(a[0] - b[0], a[1] - b[1])\n",
    "\n",
    "def check_line_collision(start, end, center, radius):\n",
    "    \"\"\"Check if a line segment intersects with a circle.\"\"\"\n",
    "    dx = end[0] - start[0]\n",
    "    dy = end[1] - start[1]\n",
    "    fx = center[0] - start[0]\n",
    "    fy = center[1] - start[1]\n",
    "    l2 = dx*dx + dy*dy\n",
    "    if l2 < 1e-9:\n",
    "        return distance(start, center) <= radius\n",
    "    t = max(0, min(1, (fx*dx + fy*dy) / l2))\n",
    "    px = start[0] + t*dx\n",
    "    py = start[1] + t*dy\n",
    "    return distance((px, py), center) <= radius\n",
    "\n",
    "def line_collision(pos, new_pos, obstacles):\n",
    "    \"\"\"Check if a line segment collides with any obstacle.\"\"\"\n",
    "    for obs in obstacles:\n",
    "        if check_line_collision(pos, new_pos, obs, OBSTACLE_RADIUS + COLLISION_BUFFER):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def inside_obstacle(pos, obstacles):\n",
    "    \"\"\"Check if a point is inside any obstacle.\"\"\"\n",
    "    for obs in obstacles:\n",
    "        if distance(pos, obs) <= (OBSTACLE_RADIUS + DOT_RADIUS):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def potential_field_dir(pos, goal, obstacles):\n",
    "    \"\"\"Get a normalized direction vector using potential field approach.\"\"\"\n",
    "    # Attractive force toward goal\n",
    "    gx = goal[0] - pos[0]\n",
    "    gy = goal[1] - pos[1]\n",
    "    dg = math.hypot(gx, gy)\n",
    "    if dg < 1e-6:\n",
    "        return np.zeros(2, dtype=np.float32)\n",
    "    att = np.array([gx / dg, gy / dg], dtype=np.float32)\n",
    "\n",
    "    # Repulsive force from obstacles\n",
    "    repulse_x = 0.0\n",
    "    repulse_y = 0.0\n",
    "    repulsion_radius = 50.0\n",
    "    repulsion_gain = 30000.0\n",
    "\n",
    "    for obs in obstacles:\n",
    "        dx = pos[0] - obs[0]\n",
    "        dy = pos[1] - obs[1]\n",
    "        dobs = math.hypot(dx, dy)\n",
    "        if dobs < 1e-9:\n",
    "            continue\n",
    "        if dobs < repulsion_radius:\n",
    "            pushx = dx / dobs\n",
    "            pushy = dy / dobs\n",
    "            strength = repulsion_gain / (dobs**2)\n",
    "            repulse_x += pushx * strength\n",
    "            repulse_y += pushy * strength\n",
    "\n",
    "    # Combined direction\n",
    "    px = att[0] + repulse_x\n",
    "    py = att[1] + repulse_y\n",
    "    mg = math.hypot(px, py)\n",
    "    if mg < 1e-9:\n",
    "        return np.zeros(2, dtype=np.float32)\n",
    "    return np.array([px / mg, py / mg], dtype=np.float32)\n",
    "\n",
    "###############################################################################\n",
    "# BAYESIAN GOAL INFERENCE MODEL\n",
    "###############################################################################\n",
    "class BayesianGoalInference:\n",
    "    \"\"\"Bayesian goal inference model that infers goal from human inputs.\"\"\"\n",
    "    \n",
    "    def __init__(self, beta=10.0, w_theta=0.7, w_d=0.3, decay_rate=0.85):\n",
    "        \"\"\"\n",
    "        Initialize the Bayesian goal inference model.\n",
    "        \n",
    "        Args:\n",
    "            beta: Rationality parameter for human behavior\n",
    "            w_theta: Weight for angular deviation cost\n",
    "            w_d: Weight for distance deviation cost\n",
    "            decay_rate: Temporal smoothing parameter\n",
    "        \"\"\"\n",
    "        self.beta = beta\n",
    "        self.w_theta = w_theta\n",
    "        self.w_d = w_d\n",
    "        self.decay_rate = decay_rate\n",
    "        self.goals = []\n",
    "        self.priors = None\n",
    "        self.goal_probs = None\n",
    "        self.calibrator = None\n",
    "        self.history = []\n",
    "        self.max_hist_len = 30\n",
    "    \n",
    "    def initialize_goals(self, goals):\n",
    "        \"\"\"Set up potential goals and initialize uniform priors.\"\"\"\n",
    "        self.goals = goals\n",
    "        n_goals = len(goals)\n",
    "        # Uniform prior\n",
    "        self.priors = np.ones(n_goals) / n_goals\n",
    "        self.goal_probs = self.priors.copy()\n",
    "        self.history = []\n",
    "    \n",
    "    def load_calibrator(self, calibrator_path):\n",
    "        \"\"\"Load a pre-trained confidence calibrator.\"\"\"\n",
    "        try:\n",
    "            with open(calibrator_path, 'rb') as f:\n",
    "                self.calibrator = pickle.load(f)\n",
    "            print(f\"Loaded calibrator from {calibrator_path}\")\n",
    "        except:\n",
    "            print(f\"Could not load calibrator from {calibrator_path}\")\n",
    "            self.calibrator = None\n",
    "    \n",
    "    def train_calibrator(self, confidences, accuracies):\n",
    "        \"\"\"Train a confidence calibrator using isotonic regression.\"\"\"\n",
    "        if len(confidences) < 10:\n",
    "            print(\"Not enough data to train calibrator\")\n",
    "            return\n",
    "        \n",
    "        self.calibrator = IsotonicRegression(out_of_bounds='clip')\n",
    "        self.calibrator.fit(confidences, accuracies)\n",
    "        \n",
    "        # Save the calibrator\n",
    "        os.makedirs(\"models\", exist_ok=True)\n",
    "        with open(\"models/calibrator.pkl\", 'wb') as f:\n",
    "            pickle.dump(self.calibrator, f)\n",
    "        print(\"Calibrator trained and saved\")\n",
    "    \n",
    "    def compute_cost(self, human_input, agent_pos, goal_pos):\n",
    "        \"\"\"Compute cost of human input based on deviation from optimal.\"\"\"\n",
    "        # Optimal direction to goal\n",
    "        goal_dir = goal_pos - agent_pos\n",
    "        dist = np.linalg.norm(goal_dir)\n",
    "        if dist < 1e-6:\n",
    "            return 0.0\n",
    "        \n",
    "        goal_dir = goal_dir / dist  # Normalize\n",
    "        \n",
    "        # Angular deviation (radians)\n",
    "        dot_product = np.clip(np.dot(human_input, goal_dir), -1.0, 1.0)\n",
    "        theta_dev = abs(np.arccos(dot_product))\n",
    "        \n",
    "        # Distance magnitude deviation\n",
    "        h_magnitude = np.linalg.norm(human_input)\n",
    "        \n",
    "        # Optimal magnitude (slow down when approaching target)\n",
    "        min_dist = TARGET_RADIUS + DOT_RADIUS\n",
    "        if dist < min_dist:\n",
    "            opt_magnitude = 0.0\n",
    "        else:\n",
    "            dist_factor = min(1.0, (dist - min_dist) / 200)\n",
    "            opt_magnitude = dist_factor * 1.0\n",
    "        \n",
    "        d_dev = abs(1.0 - h_magnitude / max(opt_magnitude, 1e-6))\n",
    "        \n",
    "        # Combined cost\n",
    "        cost = self.w_theta * theta_dev + self.w_d * d_dev\n",
    "        return cost\n",
    "    \n",
    "    def update(self, agent_pos, human_input, obstacles=None):\n",
    "        \"\"\"Update goal probabilities based on observed human input.\"\"\"\n",
    "        if len(self.goals) == 0:\n",
    "            return np.array([])\n",
    "        \n",
    "        # Calculate likelihoods for each goal\n",
    "        likelihoods = np.zeros(len(self.goals))\n",
    "        costs = np.zeros(len(self.goals))\n",
    "        \n",
    "        for i, goal in enumerate(self.goals):\n",
    "            cost = self.compute_cost(human_input, agent_pos, goal)\n",
    "            costs[i] = cost\n",
    "            # Noisy rational model: P(action|goal) ∝ exp(-β * cost)\n",
    "            likelihoods[i] = np.exp(-self.beta * cost)\n",
    "        \n",
    "        # Normalize likelihoods\n",
    "        if np.sum(likelihoods) > 0:\n",
    "            likelihoods = likelihoods / np.sum(likelihoods)\n",
    "        else:\n",
    "            likelihoods = np.ones_like(likelihoods) / len(likelihoods)\n",
    "        \n",
    "        # Bayesian update: P(goal|action) ∝ P(action|goal) * P(goal)\n",
    "        raw_posteriors = likelihoods * self.goal_probs\n",
    "        \n",
    "        # Normalize\n",
    "        if np.sum(raw_posteriors) > 0:\n",
    "            posteriors = raw_posteriors / np.sum(raw_posteriors)\n",
    "        else:\n",
    "            posteriors = self.priors.copy()\n",
    "        \n",
    "        # Apply temporal smoothing\n",
    "        self.goal_probs = self.decay_rate * self.goal_probs + (1 - self.decay_rate) * posteriors\n",
    "        \n",
    "        # Normalize again after smoothing\n",
    "        if np.sum(self.goal_probs) > 0:\n",
    "            self.goal_probs = self.goal_probs / np.sum(self.goal_probs)\n",
    "        \n",
    "        # Apply calibration if available\n",
    "        if self.calibrator is not None:\n",
    "            max_prob = np.max(self.goal_probs)\n",
    "            calibrated_max = self.calibrator.predict([max_prob])[0]\n",
    "            \n",
    "            # Adjust probabilities proportionally\n",
    "            if max_prob > 0:\n",
    "                scale_factor = calibrated_max / max_prob\n",
    "                self.goal_probs = self.goal_probs * scale_factor\n",
    "                remainder = 1.0 - np.sum(self.goal_probs)\n",
    "                if remainder > 0:\n",
    "                    # Distribute remainder proportionally\n",
    "                    indices = np.arange(len(self.goal_probs))\n",
    "                    max_idx = np.argmax(self.goal_probs)\n",
    "                    other_indices = indices[indices != max_idx]\n",
    "                    if len(other_indices) > 0:\n",
    "                        other_sum = np.sum(self.goal_probs[other_indices])\n",
    "                        if other_sum > 0:\n",
    "                            for idx in other_indices:\n",
    "                                self.goal_probs[idx] += remainder * (self.goal_probs[idx] / other_sum)\n",
    "                        else:\n",
    "                            for idx in other_indices:\n",
    "                                self.goal_probs[idx] += remainder / len(other_indices)\n",
    "        \n",
    "        # Track history\n",
    "        self.history.append(self.goal_probs.copy())\n",
    "        if len(self.history) > self.max_hist_len:\n",
    "            self.history.pop(0)\n",
    "        \n",
    "        return self.goal_probs\n",
    "    \n",
    "    def get_goal_probs(self):\n",
    "        \"\"\"Get current goal probabilities.\"\"\"\n",
    "        return self.goal_probs\n",
    "    \n",
    "    def get_most_likely_goal(self):\n",
    "        \"\"\"Get the most likely goal and its probability.\"\"\"\n",
    "        if len(self.goals) == 0:\n",
    "            return None, 0.0\n",
    "        \n",
    "        max_idx = np.argmax(self.goal_probs)\n",
    "        return self.goals[max_idx], self.goal_probs[max_idx]\n",
    "    \n",
    "    def get_entropy(self):\n",
    "        \"\"\"Calculate entropy of the goal distribution.\"\"\"\n",
    "        return entropy(self.goal_probs)\n",
    "    \n",
    "    def compute_expert_recommendation(self, agent_pos, obstacles):\n",
    "        \"\"\"Compute expert recommendation based on goal probabilities.\"\"\"\n",
    "        if len(self.goals) == 0:\n",
    "            return np.zeros(2, dtype=np.float32)\n",
    "        \n",
    "        # Compute weighted direction based on goal probabilities\n",
    "        weighted_dir = np.zeros(2, dtype=np.float32)\n",
    "        \n",
    "        for i, goal in enumerate(self.goals):\n",
    "            # Get expert direction for this goal\n",
    "            expert_dir = potential_field_dir(agent_pos, goal, obstacles)\n",
    "            # Weight by goal probability\n",
    "            weighted_dir += self.goal_probs[i] * expert_dir\n",
    "        \n",
    "        # Normalize\n",
    "        magnitude = np.linalg.norm(weighted_dir)\n",
    "        if magnitude > 1e-6:\n",
    "            weighted_dir = weighted_dir / magnitude\n",
    "        \n",
    "        return weighted_dir\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset to initial state with uniform priors.\"\"\"\n",
    "        if len(self.goals) > 0:\n",
    "            self.goal_probs = self.priors.copy()\n",
    "        self.history = []\n",
    "\n",
    "###############################################################################\n",
    "# METRICS CALLBACK\n",
    "###############################################################################\n",
    "class MetricsCallback(BaseCallback):\n",
    "    \"\"\"Callback to track training metrics.\"\"\"\n",
    "    \n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        # Episode metrics\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.episode_gammas = []\n",
    "        self.goal_inference_accuracy = []\n",
    "        self.goal_entropy = []\n",
    "        \n",
    "        # Current episode tracking\n",
    "        self.total_reward = 0.0\n",
    "        self.ep_length = 0\n",
    "        self.current_gammas = []\n",
    "        self.current_goal_probs = []\n",
    "        self.current_true_goal_idx = None\n",
    "        \n",
    "        # Collision tracking\n",
    "        self.n_collisions = 0\n",
    "        self.n_episodes = 0\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"Called at each step of training.\"\"\"\n",
    "        actions = self.locals['actions']\n",
    "        rewards = self.locals['rewards']\n",
    "        dones = self.locals['dones']\n",
    "        infos = self.locals['infos']\n",
    "        \n",
    "        # Get environment\n",
    "        env = self.model.env.envs[0]\n",
    "        \n",
    "        # Compute gamma from action (mapping [-1,1] -> [0,1])\n",
    "        gamma_val = 0.5 * (actions[0][0] + 1.0)\n",
    "        self.current_gammas.append(gamma_val)\n",
    "        \n",
    "        # Track reward and episode length\n",
    "        r = float(rewards[0])\n",
    "        self.total_reward += r\n",
    "        self.ep_length += 1\n",
    "        \n",
    "        # Track goal inference\n",
    "        if hasattr(env, 'bayesian_model') and hasattr(env, 'true_goal_idx'):\n",
    "            goal_probs = env.bayesian_model.get_goal_probs()\n",
    "            self.current_goal_probs.append(goal_probs.copy())\n",
    "            \n",
    "            if self.current_true_goal_idx is None:\n",
    "                self.current_true_goal_idx = env.true_goal_idx\n",
    "        \n",
    "        if dones[0]:\n",
    "            # Calculate goal inference accuracy\n",
    "            if self.current_goal_probs and self.current_true_goal_idx is not None:\n",
    "                accuracies = []\n",
    "                entropies = []\n",
    "                \n",
    "                for probs in self.current_goal_probs:\n",
    "                    pred_goal = np.argmax(probs)\n",
    "                    accuracies.append(int(pred_goal == self.current_true_goal_idx))\n",
    "                    entropies.append(entropy(probs))\n",
    "                \n",
    "                self.goal_inference_accuracy.append(np.mean(accuracies))\n",
    "                self.goal_entropy.append(np.mean(entropies))\n",
    "            \n",
    "            # Store episode statistics\n",
    "            self.episode_rewards.append(self.total_reward)\n",
    "            self.episode_lengths.append(self.ep_length)\n",
    "            \n",
    "            if self.current_gammas:\n",
    "                self.episode_gammas.append(np.mean(self.current_gammas))\n",
    "            \n",
    "            # Reset episode tracking\n",
    "            self.total_reward = 0.0\n",
    "            self.ep_length = 0\n",
    "            self.current_gammas = []\n",
    "            self.current_goal_probs = []\n",
    "            self.current_true_goal_idx = None\n",
    "            \n",
    "            # Increment episode counter\n",
    "            self.n_episodes += 1\n",
    "            if 'terminal_reason' in infos[0] and infos[0]['terminal_reason'] == 'collision':\n",
    "                self.n_collisions += 1\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def _moving_average(self, data, window=10):\n",
    "        \"\"\"Calculate moving average of data.\"\"\"\n",
    "        if len(data) < window:\n",
    "            return np.array(data)\n",
    "        return np.convolve(data, np.ones(window)/window, mode='valid')\n",
    "\n",
    "    def save_metrics(self, save_dir=\"training_metrics\"):\n",
    "        \"\"\"Save metrics plots to the specified directory.\"\"\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        # Episode Rewards\n",
    "        if self.episode_rewards:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(self.episode_rewards, label=\"Episode Reward\", alpha=0.6)\n",
    "            ma_rewards = self._moving_average(self.episode_rewards, 10)\n",
    "            if len(ma_rewards):\n",
    "                plt.plot(range(10 - 1, 10 - 1 + len(ma_rewards)), \n",
    "                         ma_rewards, label=\"MA(10)\", color='red', linewidth=2)\n",
    "            plt.xlabel(\"Episode\")\n",
    "            plt.ylabel(\"Reward\")\n",
    "            plt.title(\"Episode Rewards\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.savefig(os.path.join(save_dir, \"episode_rewards.png\"))\n",
    "            plt.close()\n",
    "        \n",
    "        # Average Gamma per Episode\n",
    "        if self.episode_gammas:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(self.episode_gammas, label=\"Average Gamma\", alpha=0.6)\n",
    "            ma_gamma = self._moving_average(self.episode_gammas, 10)\n",
    "            if len(ma_gamma):\n",
    "                plt.plot(range(10 - 1, 10 - 1 + len(ma_gamma)), \n",
    "                         ma_gamma, label=\"MA(10)\", color='red', linewidth=2)\n",
    "            plt.xlabel(\"Episode\")\n",
    "            plt.ylabel(\"Gamma (avg)\")\n",
    "            plt.title(\"Average Gamma per Episode\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.savefig(os.path.join(save_dir, \"average_gamma.png\"))\n",
    "            plt.close()\n",
    "        \n",
    "        # Goal Inference Accuracy\n",
    "        if self.goal_inference_accuracy:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(self.goal_inference_accuracy, label=\"Goal Inference Accuracy\", alpha=0.6)\n",
    "            ma_acc = self._moving_average(self.goal_inference_accuracy, 10)\n",
    "            if len(ma_acc):\n",
    "                plt.plot(range(10 - 1, 10 - 1 + len(ma_acc)), \n",
    "                         ma_acc, label=\"MA(10)\", color='red', linewidth=2)\n",
    "            plt.xlabel(\"Episode\")\n",
    "            plt.ylabel(\"Accuracy\")\n",
    "            plt.title(\"Goal Inference Accuracy per Episode\")\n",
    "            plt.ylim(0, 1.05)\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.savefig(os.path.join(save_dir, \"goal_inference_accuracy.png\"))\n",
    "            plt.close()\n",
    "        \n",
    "        # Goal Distribution Entropy\n",
    "        if self.goal_entropy:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(self.goal_entropy, label=\"Goal Distribution Entropy\", alpha=0.6)\n",
    "            ma_ent = self._moving_average(self.goal_entropy, 10)\n",
    "            if len(ma_ent):\n",
    "                plt.plot(range(10 - 1, 10 - 1 + len(ma_ent)), \n",
    "                         ma_ent, label=\"MA(10)\", color='red', linewidth=2)\n",
    "            plt.xlabel(\"Episode\")\n",
    "            plt.ylabel(\"Entropy\")\n",
    "            plt.title(\"Goal Distribution Entropy per Episode\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.savefig(os.path.join(save_dir, \"goal_entropy.png\"))\n",
    "            plt.close()\n",
    "        \n",
    "        # Summary text\n",
    "        with open(os.path.join(save_dir, \"summary.txt\"), \"w\") as f:\n",
    "            f.write(f\"Total Episodes: {len(self.episode_rewards)}\\n\")\n",
    "            if self.episode_rewards:\n",
    "                f.write(f\"Mean Episode Reward: {np.mean(self.episode_rewards):.3f}\\n\")\n",
    "            f.write(f\"Collisions Rate: {self.n_collisions/max(1, self.n_episodes):.3f}\\n\")\n",
    "            if self.episode_gammas:\n",
    "                f.write(f\"Mean Gamma: {np.mean(self.episode_gammas):.3f}\\n\")\n",
    "            if self.goal_inference_accuracy:\n",
    "                f.write(f\"Mean Goal Inference Accuracy: {np.mean(self.goal_inference_accuracy):.3f}\\n\")\n",
    "\n",
    "###############################################################################\n",
    "# SHARED AUTONOMY ENVIRONMENT\n",
    "###############################################################################\n",
    "class SharedAutonomyEnv(gym.Env):\n",
    "    \"\"\"Environment for training shared autonomy.\"\"\"\n",
    "    \n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": RENDER_FPS}\n",
    "    \n",
    "    def __init__(self, visualize=False, use_joint_optimization=True):\n",
    "        \"\"\"Initialize the environment.\"\"\"\n",
    "        super().__init__()\n",
    "        self.visualize = visualize\n",
    "        self.use_joint_optimization = use_joint_optimization\n",
    "        \n",
    "        # Action space: gamma ∈ [-1, 1] (mapped to [0, 1])\n",
    "        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(1,), dtype=np.float32)\n",
    "        \n",
    "        # Default observation space (will be updated in reset)\n",
    "        # Assuming a default of 8 goals for initial observation space\n",
    "        n_goals = 8\n",
    "        low = np.array(\n",
    "            [0, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0] + [0] * n_goals,\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        high = np.array(\n",
    "            [FULL_VIEW_SIZE[0], FULL_VIEW_SIZE[1], 1, 1,\n",
    "            FULL_VIEW_SIZE[0], FULL_VIEW_SIZE[1], 1, 1, 1, 1, np.log(n_goals)] + [1] * n_goals,\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        self.observation_space = spaces.Box(low=low, high=high, shape=(11 + n_goals,), dtype=np.float32)\n",
    "        \n",
    "        # Initialize state\n",
    "        self.dot_pos = None\n",
    "        self.goal_pos = None\n",
    "        self.obstacles = []\n",
    "        self.goals = []\n",
    "        self.true_goal_idx = None\n",
    "        self.step_count = 0\n",
    "        self.max_steps = 200\n",
    "        self.episode_reward = 0.0\n",
    "        self.max_dist = math.hypot(FULL_VIEW_SIZE[0], FULL_VIEW_SIZE[1])\n",
    "        \n",
    "        # Bayesian model for goal inference\n",
    "        self.bayesian_model = BayesianGoalInference(beta=10.0, w_theta=0.7, w_d=0.3)\n",
    "        \n",
    "        # Set up visualization\n",
    "        if self.visualize and pygame is not None:\n",
    "            pygame.init()\n",
    "            self.window = pygame.display.set_mode(FULL_VIEW_SIZE)\n",
    "            pygame.display.set_caption(\"Shared Autonomy\")\n",
    "            self.clock = pygame.time.Clock()\n",
    "        else:\n",
    "            self.window = None\n",
    "            self.clock = None\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"Reset the environment.\"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        # Create the environment layout\n",
    "        self.randomize_env(seed if seed is not None else random.randint(0, 9999))\n",
    "        \n",
    "        # Reset internal state\n",
    "        self.step_count = 0\n",
    "        self.episode_reward = 0.0\n",
    "        self.dot_pos = START_POS.copy()\n",
    "        \n",
    "        # Choose a random goal\n",
    "        if self.goals:\n",
    "            self.true_goal_idx = random.randint(0, len(self.goals) - 1)\n",
    "            self.goal_pos = self.goals[self.true_goal_idx].copy()\n",
    "        else:\n",
    "            self.goal_pos = np.array([random.uniform(0.2*FULL_VIEW_SIZE[0], 0.8*FULL_VIEW_SIZE[0]),\n",
    "                                      random.uniform(0.2*FULL_VIEW_SIZE[1], 0.8*FULL_VIEW_SIZE[1])],\n",
    "                                     dtype=np.float32)\n",
    "            self.true_goal_idx = 0\n",
    "        \n",
    "        # Reset Bayesian model\n",
    "        self.bayesian_model.initialize_goals(self.goals)\n",
    "        self.bayesian_model.reset()\n",
    "        \n",
    "        # Update observation space based on number of goals\n",
    "        n_goals = len(self.goals)\n",
    "        low = np.array(\n",
    "            [0, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0] + [0] * n_goals,\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        high = np.array(\n",
    "            [FULL_VIEW_SIZE[0], FULL_VIEW_SIZE[1], 1, 1,\n",
    "             FULL_VIEW_SIZE[0], FULL_VIEW_SIZE[1], 1, 1, 1, 1, np.log(n_goals)] + [1] * n_goals,\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        self.observation_space = spaces.Box(low=low, high=high, shape=(11 + n_goals,), dtype=np.float32)\n",
    "        \n",
    "        return self._get_obs(), {}\n",
    "    \n",
    "    def randomize_env(self, seed):\n",
    "        \"\"\"Create a randomized environment layout.\"\"\"\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        margin = 50\n",
    "        n_goals = 8\n",
    "        n_obstacles = 5\n",
    "        min_goal_distance = 200\n",
    "        \n",
    "        # Generate goals\n",
    "        new_goals = []\n",
    "        attempts = 0\n",
    "        while len(new_goals) < n_goals and attempts < 1000:\n",
    "            x = random.uniform(margin, FULL_VIEW_SIZE[0] - margin)\n",
    "            y = random.uniform(margin, FULL_VIEW_SIZE[1] - margin)\n",
    "            candidate = np.array([x, y], dtype=np.float32)\n",
    "            \n",
    "            # Ensure goal is far enough from start position\n",
    "            if distance(candidate, START_POS) >= min_goal_distance:\n",
    "                new_goals.append(candidate)\n",
    "            attempts += 1\n",
    "        \n",
    "        self.goals = new_goals[:n_goals]\n",
    "        \n",
    "        # Generate obstacles\n",
    "        new_obstacles = []\n",
    "        for _ in range(n_obstacles):\n",
    "            x = random.uniform(margin, FULL_VIEW_SIZE[0] - margin)\n",
    "            y = random.uniform(margin, FULL_VIEW_SIZE[1] - margin)\n",
    "            candidate = np.array([x, y], dtype=np.float32)\n",
    "            \n",
    "            # Check validity\n",
    "            valid = True\n",
    "            if distance(candidate, START_POS) < (DOT_RADIUS + OBSTACLE_RADIUS + 10):\n",
    "                valid = False\n",
    "            for goal in self.goals:\n",
    "                if distance(candidate, goal) < (TARGET_RADIUS + OBSTACLE_RADIUS + 20):\n",
    "                    valid = False\n",
    "            for obs in new_obstacles:\n",
    "                if distance(candidate, obs) < (2*OBSTACLE_RADIUS + 10):\n",
    "                    valid = False\n",
    "            \n",
    "            if valid:\n",
    "                new_obstacles.append(candidate)\n",
    "        \n",
    "        self.obstacles = new_obstacles\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Execute one environment step.\"\"\"\n",
    "        # Map action to gamma\n",
    "        gamma_val = 0.5 * (action[0] + 1.0)\n",
    "        \n",
    "        self.step_count += 1\n",
    "        \n",
    "        # Compute world direction using potential field\n",
    "        w_dir = potential_field_dir(self.dot_pos, self.goal_pos, self.obstacles)\n",
    "        \n",
    "        # Add noise for human direction (simulating imperfect human input)\n",
    "        noise = np.random.normal(0, NOISE_MAGNITUDE, size=2)\n",
    "        h_dir = w_dir + noise\n",
    "        hm = np.hypot(h_dir[0], h_dir[1])\n",
    "        if hm > 1e-6:\n",
    "            h_dir /= hm\n",
    "        \n",
    "        # Update Bayesian model with human input\n",
    "        goal_probs = self.bayesian_model.update(self.dot_pos, h_dir, self.obstacles)\n",
    "        \n",
    "        # Get expert recommendation\n",
    "        if self.use_joint_optimization:\n",
    "            # Use weighted expert recommendation\n",
    "            w_dir = self.bayesian_model.compute_expert_recommendation(self.dot_pos, self.obstacles)\n",
    "        \n",
    "        # Combine directions using gamma\n",
    "        c_dir = gamma_val * w_dir + (1 - gamma_val) * h_dir\n",
    "        cm = np.hypot(c_dir[0], c_dir[1])\n",
    "        if cm > 1e-6:\n",
    "            c_dir /= cm\n",
    "        \n",
    "        # Compute new position\n",
    "        move_vec = c_dir * MAX_SPEED\n",
    "        new_pos = self.dot_pos + move_vec\n",
    "        \n",
    "        # Check for collision\n",
    "        if not line_collision(self.dot_pos, new_pos, self.obstacles):\n",
    "            new_pos[0] = np.clip(new_pos[0], 0, FULL_VIEW_SIZE[0])\n",
    "            new_pos[1] = np.clip(new_pos[1], 0, FULL_VIEW_SIZE[1])\n",
    "            self.dot_pos = new_pos\n",
    "        \n",
    "        # Check if inside obstacle\n",
    "        collided = inside_obstacle(self.dot_pos, self.obstacles)\n",
    "        info = {}\n",
    "        \n",
    "        if collided:\n",
    "            reward = -2.0\n",
    "            done = True\n",
    "            info[\"terminal_reason\"] = \"collision\"\n",
    "        else:\n",
    "            # Check if reached goal\n",
    "            if distance(self.dot_pos, self.goal_pos) <= (TARGET_RADIUS + DOT_RADIUS):\n",
    "                reward = 5.0\n",
    "                done = True\n",
    "                info[\"terminal_reason\"] = \"goal_reached\"\n",
    "            else:\n",
    "                # Progress reward\n",
    "                prev_pos = self.dot_pos - move_vec\n",
    "                prev_dist = distance(prev_pos, self.goal_pos)\n",
    "                curr_dist = distance(self.dot_pos, self.goal_pos)\n",
    "                progress = prev_dist - curr_dist\n",
    "                \n",
    "                reward = 0.1 * progress\n",
    "                done = False\n",
    "                info[\"terminal_reason\"] = None\n",
    "        \n",
    "        # Check for timeout\n",
    "        truncated = (self.step_count >= self.max_steps)\n",
    "        if truncated and not done:\n",
    "            info[\"terminal_reason\"] = \"timeout\"\n",
    "        \n",
    "        # Get observation\n",
    "        obs = self._get_obs()\n",
    "        \n",
    "        return obs, float(reward), done, truncated, info\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        \"\"\"Get the current observation.\"\"\"\n",
    "        # Distance to goal\n",
    "        to_g = self.goal_pos - self.dot_pos\n",
    "        d = math.hypot(to_g[0], to_g[1])\n",
    "        dist_ratio = d / self.max_dist\n",
    "        \n",
    "        # Compute directions\n",
    "        w_dir = potential_field_dir(self.dot_pos, self.goal_pos, self.obstacles)\n",
    "        noise = np.random.normal(0, NOISE_MAGNITUDE, size=2)\n",
    "        h_dir = w_dir + noise\n",
    "        hm = np.hypot(h_dir[0], h_dir[1])\n",
    "        if hm > 1e-6:\n",
    "            h_dir /= hm\n",
    "        \n",
    "        # Distance to closest obstacle\n",
    "        if self.obstacles:\n",
    "            min_obs_distance = min(distance(self.dot_pos, obs) for obs in self.obstacles)\n",
    "        else:\n",
    "            min_obs_distance = self.max_dist\n",
    "        obs_dist_ratio = min_obs_distance / self.max_dist\n",
    "        \n",
    "        # Get goal probabilities and entropy\n",
    "        goal_probs = self.bayesian_model.get_goal_probs()\n",
    "        goal_ent = entropy(goal_probs) if len(goal_probs) > 0 else np.log(len(self.goals))\n",
    "        \n",
    "        # Combine all features\n",
    "        base_obs = np.array([\n",
    "            self.dot_pos[0], self.dot_pos[1],\n",
    "            h_dir[0], h_dir[1],\n",
    "            self.goal_pos[0], self.goal_pos[1],\n",
    "            w_dir[0], w_dir[1],\n",
    "            dist_ratio, obs_dist_ratio,\n",
    "            goal_ent\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        # Append goal probabilities\n",
    "        obs = np.concatenate([base_obs, goal_probs]).astype(np.float32)\n",
    "        \n",
    "        return obs\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"Render the environment.\"\"\"\n",
    "        if not self.visualize or (pygame is None):\n",
    "            return\n",
    "        \n",
    "        # Handle user events\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                return\n",
    "        \n",
    "        # Clear the screen\n",
    "        self.window.fill(WHITE)\n",
    "        \n",
    "        # Draw obstacles\n",
    "        for obs in self.obstacles:\n",
    "            pygame.draw.circle(self.window, GRAY, (int(obs[0]), int(obs[1])), OBSTACLE_RADIUS)\n",
    "        \n",
    "        # Draw all potential targets\n",
    "        for i, gpos in enumerate(self.goals):\n",
    "            # Color intensity based on goal probability\n",
    "            goal_probs = self.bayesian_model.get_goal_probs()\n",
    "            if i < len(goal_probs):\n",
    "                prob = goal_probs[i]\n",
    "                # Interpolate color from yellow to green based on probability\n",
    "                color = (int(255 * (1 - prob)), int(255), 0)\n",
    "            else:\n",
    "                color = YELLOW\n",
    "            \n",
    "            # Draw the target\n",
    "            pygame.draw.circle(self.window, color, (int(gpos[0]), int(gpos[1])), TARGET_RADIUS)\n",
    "            \n",
    "            # Label the target\n",
    "            font = pygame.font.SysFont(None, 24)\n",
    "            label = font.render(str(i), True, BLACK)\n",
    "            self.window.blit(label, (int(gpos[0]) - 5, int(gpos[1]) - 8))\n",
    "        \n",
    "        # Draw the true goal with a black outline\n",
    "        pygame.draw.circle(self.window, BLACK, \n",
    "                          (int(self.goal_pos[0]), int(self.goal_pos[1])), \n",
    "                          TARGET_RADIUS+2, width=2)\n",
    "        \n",
    "        # Draw the agent\n",
    "        pygame.draw.circle(self.window, RED, \n",
    "                          (int(self.dot_pos[0]), int(self.dot_pos[1])), \n",
    "                          DOT_RADIUS)\n",
    "        \n",
    "        # Display gamma value\n",
    "        if hasattr(self, 'last_gamma'):\n",
    "            font = pygame.font.SysFont(None, 30)\n",
    "            gamma_text = font.render(f\"γ: {self.last_gamma:.2f}\", True, BLACK)\n",
    "            self.window.blit(gamma_text, (10, 10))\n",
    "        \n",
    "        # Update the display\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(RENDER_FPS)\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close the environment.\"\"\"\n",
    "        if self.visualize and pygame is not None:\n",
    "            pygame.quit()\n",
    "        super().close()\n",
    "\n",
    "###############################################################################\n",
    "# TRAINING FUNCTIONS\n",
    "###############################################################################\n",
    "def train_bayesian_model(visualize=False):\n",
    "    \"\"\"Train and evaluate the Bayesian goal inference model.\"\"\"\n",
    "    print(\"Training Bayesian goal inference model...\")\n",
    "    \n",
    "    # Create a model with default parameters\n",
    "    model = BayesianGoalInference(beta=10.0, w_theta=0.7, w_d=0.3)\n",
    "    \n",
    "    # Create a synthetic dataset\n",
    "    n_trajectories = 50\n",
    "    trajectories = []\n",
    "    env = SharedAutonomyEnv(visualize=False)\n",
    "    \n",
    "    for i in range(n_trajectories):\n",
    "        print(f\"Generating trajectory {i+1}/{n_trajectories}\", end=\"\\r\")\n",
    "        \n",
    "        # Reset environment\n",
    "        obs, _ = env.reset()\n",
    "        \n",
    "        # Extract information\n",
    "        goal_pos = env.goal_pos\n",
    "        true_goal_idx = env.true_goal_idx\n",
    "        goals = env.goals\n",
    "        \n",
    "        # Generate a trajectory\n",
    "        trajectory = {\n",
    "            \"positions\": [env.dot_pos.copy()],\n",
    "            \"inputs\": [],\n",
    "            \"goal_pos\": goal_pos.copy(),\n",
    "            \"true_goal_idx\": true_goal_idx,\n",
    "            \"goals\": [g.copy() for g in goals],\n",
    "        }\n",
    "        \n",
    "        # Simple policy to generate trajectories\n",
    "        done = False\n",
    "        truncated = False\n",
    "        while not (done or truncated):\n",
    "            # Direction to goal with noise\n",
    "            to_goal = goal_pos - env.dot_pos\n",
    "            dist = np.linalg.norm(to_goal)\n",
    "            if dist > 0:\n",
    "                direction = to_goal / dist\n",
    "            else:\n",
    "                direction = np.zeros(2)\n",
    "            \n",
    "            # Add noise\n",
    "            noise_scale = 0.2\n",
    "            noise = np.random.normal(0, noise_scale, size=2)\n",
    "            noisy_direction = direction + noise\n",
    "            \n",
    "            # Normalize\n",
    "            norm = np.linalg.norm(noisy_direction)\n",
    "            if norm > 0:\n",
    "                noisy_direction = noisy_direction / norm\n",
    "            \n",
    "            # Store human input\n",
    "            trajectory[\"inputs\"].append(noisy_direction.copy())\n",
    "            \n",
    "            # Step environment with full human control\n",
    "            obs, _, done, truncated, _ = env.step([-1.0])  # -1.0 maps to gamma=0\n",
    "            \n",
    "            # Store position\n",
    "            trajectory[\"positions\"].append(env.dot_pos.copy())\n",
    "            \n",
    "            if len(trajectory[\"positions\"]) > 200:  # Prevent infinite loops\n",
    "                break\n",
    "        \n",
    "        trajectories.append(trajectory)\n",
    "    \n",
    "    env.close()\n",
    "    print(\"\\nGenerated synthetic dataset with\", len(trajectories), \"trajectories\")\n",
    "    \n",
    "    # Save dataset\n",
    "    os.makedirs(\"datasets\", exist_ok=True)\n",
    "    with open(\"datasets/synthetic_trajectories.pkl\", \"wb\") as f:\n",
    "        pickle.dump(trajectories, f)\n",
    "    print(\"Saved synthetic dataset to datasets/synthetic_trajectories.pkl\")\n",
    "    \n",
    "    # Train confidence calibration\n",
    "    print(\"\\nTraining confidence calibration...\")\n",
    "    confidences = []\n",
    "    accuracies = []\n",
    "    \n",
    "    n_calib = min(40, len(trajectories))\n",
    "    for i in range(n_calib):\n",
    "        traj = trajectories[i]\n",
    "        goals = traj[\"goals\"]\n",
    "        true_idx = traj[\"true_goal_idx\"]\n",
    "        \n",
    "        # Initialize model with goals\n",
    "        model.initialize_goals(goals)\n",
    "        \n",
    "        # Process trajectory\n",
    "        positions = traj[\"positions\"]\n",
    "        inputs = traj[\"inputs\"]\n",
    "        n_steps = min(len(inputs), len(positions) - 1)\n",
    "        \n",
    "        for t in range(n_steps):\n",
    "            pos = positions[t]\n",
    "            h_input = inputs[t]\n",
    "            \n",
    "            # Update model\n",
    "            goal_probs = model.update(pos, h_input)\n",
    "            \n",
    "            # Record confidence and accuracy\n",
    "            if len(goal_probs) > 0:\n",
    "                max_prob = np.max(goal_probs)\n",
    "                pred_idx = np.argmax(goal_probs)\n",
    "                is_correct = int(pred_idx == true_idx)\n",
    "                \n",
    "                confidences.append(max_prob)\n",
    "                accuracies.append(is_correct)\n",
    "    \n",
    "    # Train calibrator\n",
    "    model.train_calibrator(confidences, accuracies)\n",
    "    \n",
    "    # Save the trained model\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    with open(\"models/bayesian_model.pkl\", \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(\"Saved trained Bayesian model to models/bayesian_model.pkl\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"\\nEvaluating model on test set...\")\n",
    "    eval_results = {}\n",
    "    completion_points = [0.25, 0.5, 0.75, 1.0]\n",
    "    for cp in completion_points:\n",
    "        eval_results[cp] = []\n",
    "    \n",
    "    n_test = min(10, len(trajectories))\n",
    "    for i in range(n_test):\n",
    "        traj = trajectories[i]\n",
    "        goals = traj[\"goals\"]\n",
    "        true_idx = traj[\"true_goal_idx\"]\n",
    "        \n",
    "        # Initialize model with goals\n",
    "        model.initialize_goals(goals)\n",
    "        \n",
    "        # Process trajectory\n",
    "        positions = traj[\"positions\"]\n",
    "        inputs = traj[\"inputs\"]\n",
    "        n_steps = min(len(inputs), len(positions) - 1)\n",
    "        \n",
    "        for cp in completion_points:\n",
    "            step_idx = int(cp * n_steps) - 1\n",
    "            if step_idx < 0:\n",
    "                continue\n",
    "            \n",
    "            # Reset model\n",
    "            model.reset()\n",
    "            \n",
    "            # Process up to completion point\n",
    "            for t in range(step_idx + 1):\n",
    "                pos = positions[t]\n",
    "                h_input = inputs[t]\n",
    "                goal_probs = model.update(pos, h_input)\n",
    "            \n",
    "            # Check prediction\n",
    "            if len(goal_probs) > 0:\n",
    "                pred_idx = np.argmax(goal_probs)\n",
    "                is_correct = int(pred_idx == true_idx)\n",
    "                eval_results[cp].append(is_correct)\n",
    "    \n",
    "    # Print evaluation results\n",
    "    print(\"\\nModel performance at different path completion percentages:\")\n",
    "    for cp in completion_points:\n",
    "        if eval_results[cp]:\n",
    "            accuracy = np.mean(eval_results[cp])\n",
    "            print(f\"  {int(cp*100)}% completion: {accuracy:.3f} accuracy\")\n",
    "    \n",
    "    # Plot accuracy by path completion\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    x = []\n",
    "    y = []\n",
    "    for cp in completion_points:\n",
    "        if eval_results[cp]:\n",
    "            x.append(int(cp*100))\n",
    "            y.append(np.mean(eval_results[cp]))\n",
    "    \n",
    "    if x and y:\n",
    "        plt.bar(x, y)\n",
    "        plt.xlabel('Path Completion (%)')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.title('Goal Inference Accuracy vs. Path Completion')\n",
    "        plt.ylim(0, 1.1)\n",
    "        plt.grid(True, axis='y')\n",
    "        os.makedirs(\"bayesian_results\", exist_ok=True)\n",
    "        plt.savefig(\"bayesian_results/accuracy_vs_completion.png\")\n",
    "        plt.close()\n",
    "    \n",
    "    # Visualize model if requested\n",
    "    if visualize and pygame is not None:\n",
    "        print(\"\\nVisualizing model performance...\")\n",
    "        \n",
    "        # Create visualization environment\n",
    "        vis_env = SharedAutonomyEnv(visualize=True)\n",
    "        \n",
    "        for _ in range(2):  # Show 2 trajectories\n",
    "            # Reset environment\n",
    "            obs, _ = vis_env.reset()\n",
    "            \n",
    "            done = False\n",
    "            truncated = False\n",
    "            while not (done or truncated):\n",
    "                # Get noisy human input\n",
    "                to_goal = vis_env.goal_pos - vis_env.dot_pos\n",
    "                dist = np.linalg.norm(to_goal)\n",
    "                if dist > 0:\n",
    "                    direction = to_goal / dist\n",
    "                else:\n",
    "                    direction = np.zeros(2)\n",
    "                \n",
    "                # Add noise\n",
    "                noise_scale = 0.2\n",
    "                noise = np.random.normal(0, noise_scale, size=2)\n",
    "                h_input = direction + noise\n",
    "                \n",
    "                # Normalize\n",
    "                norm = np.linalg.norm(h_input)\n",
    "                if norm > 0:\n",
    "                    h_input = h_input / norm\n",
    "                \n",
    "                # Step environment with full human control\n",
    "                obs, _, done, truncated, _ = vis_env.step([-1.0])\n",
    "                \n",
    "                # Visualize\n",
    "                vis_env.render()\n",
    "                import time\n",
    "                time.sleep(0.05)\n",
    "        \n",
    "        vis_env.close()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_ppo_model(total_timesteps=10000, visualize=False, use_joint_optimization=True):\n",
    "    \"\"\"Train the PPO shared autonomy model.\"\"\"\n",
    "    print(\"Initializing shared autonomy environment...\")\n",
    "    env = SharedAutonomyEnv(visualize=visualize, use_joint_optimization=use_joint_optimization)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    \n",
    "    # Set up callback\n",
    "    metrics_callback = MetricsCallback()\n",
    "    \n",
    "    # Load pre-trained Bayesian model if available\n",
    "    bayesian_model_path = \"models/bayesian_model.pkl\"\n",
    "    if os.path.exists(bayesian_model_path):\n",
    "        try:\n",
    "            with open(bayesian_model_path, \"rb\") as f:\n",
    "                bayesian_model = pickle.load(f)\n",
    "            print(f\"Loaded Bayesian model from {bayesian_model_path}\")\n",
    "            \n",
    "            # Set the model in the environment\n",
    "            env.envs[0].bayesian_model = bayesian_model\n",
    "        except:\n",
    "            print(f\"Could not load Bayesian model from {bayesian_model_path}\")\n",
    "    \n",
    "    print(\"Initializing PPO model...\")\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        learning_rate=3e-4,\n",
    "        n_steps=1024,\n",
    "        batch_size=64,\n",
    "        n_epochs=5,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        clip_range=0.2,\n",
    "        verbose=1,\n",
    "        tensorboard_log=\"./tensorboard_logs/\",\n",
    "        policy_kwargs={\n",
    "            \"net_arch\": [dict(pi=[128, 128], vf=[128, 128])],\n",
    "            \"activation_fn\": nn.ReLU\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(f\"Starting training for {total_timesteps} timesteps...\")\n",
    "    model.learn(total_timesteps=total_timesteps, callback=metrics_callback)\n",
    "    \n",
    "    # Save the trained model\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    model_path = os.path.join(\"models\", \"shared_autonomy_model\")\n",
    "    model.save(model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "    \n",
    "    # Save metrics\n",
    "    metrics_callback.save_metrics(save_dir=\"training_metrics\")\n",
    "    print(\"Metrics saved to 'training_metrics/'\")\n",
    "    \n",
    "    env.close()\n",
    "    return model\n",
    "\n",
    "def generate_gamma_heatmap(env, model, output_path=\"gamma_heatmap.png\", resolution=20):\n",
    "    \"\"\"Generate a heatmap showing gamma values across the environment space.\"\"\"\n",
    "    print(\"Generating gamma heatmap...\")\n",
    "    \n",
    "    # Create grid\n",
    "    x_grid = np.linspace(0, FULL_VIEW_SIZE[0], resolution)\n",
    "    y_grid = np.linspace(0, FULL_VIEW_SIZE[1], resolution)\n",
    "    gamma_values = np.zeros((resolution, resolution))\n",
    "    \n",
    "    # Cache some data\n",
    "    goal_pos = env.goal_pos\n",
    "    obstacles = env.obstacles\n",
    "    original_pos = env.dot_pos.copy()\n",
    "    \n",
    "    # Compute gamma values at each grid point\n",
    "    for i, x in enumerate(x_grid):\n",
    "        for j, y in enumerate(y_grid):\n",
    "            # Temporarily set agent position\n",
    "            env.dot_pos = np.array([x, y], dtype=np.float32)\n",
    "            \n",
    "            # Get observation at this position\n",
    "            obs = env._get_obs()\n",
    "            \n",
    "            # Predict action\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            \n",
    "            # Map action to gamma\n",
    "            gamma = 0.5 * (action[0] + 1.0)\n",
    "            \n",
    "            # Store gamma value\n",
    "            gamma_values[j, i] = gamma  # Note: j, i for correct orientation\n",
    "    \n",
    "    # Restore original position\n",
    "    env.dot_pos = original_pos\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(gamma_values, origin='lower', extent=[0, FULL_VIEW_SIZE[0], 0, FULL_VIEW_SIZE[1]], \n",
    "              cmap='viridis', vmin=0, vmax=1)\n",
    "    plt.colorbar(label='Gamma')\n",
    "    \n",
    "    # Plot obstacles and goals\n",
    "    for obs_pos in obstacles:\n",
    "        circle = plt.Circle((obs_pos[0], obs_pos[1]), OBSTACLE_RADIUS, color='gray')\n",
    "        plt.gcf().gca().add_artist(circle)\n",
    "    \n",
    "    for i, g_pos in enumerate(env.goals):\n",
    "        circle = plt.Circle((g_pos[0], g_pos[1]), TARGET_RADIUS, color='yellow')\n",
    "        plt.gcf().gca().add_artist(circle)\n",
    "    \n",
    "    # Mark true goal\n",
    "    circle = plt.Circle((goal_pos[0], goal_pos[1]), TARGET_RADIUS+2, color='black', fill=False)\n",
    "    plt.gcf().gca().add_artist(circle)\n",
    "    \n",
    "    plt.title('Gamma Values Across Environment')\n",
    "    plt.xlabel('X Position')\n",
    "    plt.ylabel('Y Position')\n",
    "    plt.grid(False)\n",
    "    \n",
    "    # Save heatmap\n",
    "    os.makedirs(os.path.dirname(output_path) or \".\", exist_ok=True)\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Saved gamma heatmap to {output_path}\")\n",
    "\n",
    "def evaluate_model(model_path=\"models/shared_autonomy_model\", n_episodes=10, visualize=True):\n",
    "    \"\"\"Evaluate a trained model.\"\"\"\n",
    "    print(f\"Evaluating model {model_path}...\")\n",
    "    \n",
    "    # Create evaluation environment\n",
    "    env = SharedAutonomyEnv(visualize=visualize)\n",
    "    \n",
    "    try:\n",
    "        # Load the model\n",
    "        model = PPO.load(model_path)\n",
    "        print(f\"Loaded model from {model_path}\")\n",
    "    except:\n",
    "        print(f\"Could not load model from {model_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Initialize metrics\n",
    "    metrics = {\n",
    "        \"success_rate\": 0.0,\n",
    "        \"collision_rate\": 0.0,\n",
    "        \"timeout_rate\": 0.0,\n",
    "        \"avg_reward\": 0.0,\n",
    "        \"avg_steps\": 0.0,\n",
    "        \"avg_gamma\": 0.0,\n",
    "        \"goal_inference_accuracy\": 0.0\n",
    "    }\n",
    "    \n",
    "    # Run evaluation episodes\n",
    "    for episode in range(n_episodes):\n",
    "        print(f\"Episode {episode+1}/{n_episodes}\", end=\"\\r\")\n",
    "        \n",
    "        # Reset environment\n",
    "        obs, _ = env.reset()\n",
    "        \n",
    "        # Episode tracking\n",
    "        done = False\n",
    "        truncated = False\n",
    "        episode_reward = 0.0\n",
    "        episode_steps = 0\n",
    "        episode_gammas = []\n",
    "        goal_predictions = []\n",
    "        \n",
    "        while not (done or truncated):\n",
    "            # Predict action\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            \n",
    "            # Map action to gamma\n",
    "            gamma = 0.5 * (action[0] + 1.0)\n",
    "            episode_gammas.append(gamma)\n",
    "            \n",
    "            # Save for rendering\n",
    "            env.last_gamma = gamma\n",
    "            \n",
    "            # Step environment\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "            \n",
    "            # Update tracking\n",
    "            episode_reward += reward\n",
    "            episode_steps += 1\n",
    "            \n",
    "            # Track goal inference\n",
    "            if hasattr(env.bayesian_model, \"get_goal_probs\"):\n",
    "                probs = env.bayesian_model.get_goal_probs()\n",
    "                if len(probs) > 0:\n",
    "                    pred_goal = np.argmax(probs)\n",
    "                    goal_predictions.append(pred_goal)\n",
    "            \n",
    "            # Visualize\n",
    "            if visualize:\n",
    "                env.render()\n",
    "                import time\n",
    "                time.sleep(0.01)\n",
    "        \n",
    "        # Update metrics\n",
    "        terminal_reason = info.get(\"terminal_reason\", None)\n",
    "        \n",
    "        if terminal_reason == \"goal_reached\":\n",
    "            metrics[\"success_rate\"] += 1.0\n",
    "        elif terminal_reason == \"collision\":\n",
    "            metrics[\"collision_rate\"] += 1.0\n",
    "        elif terminal_reason == \"timeout\":\n",
    "            metrics[\"timeout_rate\"] += 1.0\n",
    "        \n",
    "        metrics[\"avg_reward\"] += episode_reward\n",
    "        metrics[\"avg_steps\"] += episode_steps\n",
    "        \n",
    "        if episode_gammas:\n",
    "            metrics[\"avg_gamma\"] += np.mean(episode_gammas)\n",
    "        \n",
    "        # Goal inference accuracy\n",
    "        if goal_predictions and hasattr(env, \"true_goal_idx\"):\n",
    "            accuracy = np.mean([1.0 if p == env.true_goal_idx else 0.0 for p in goal_predictions])\n",
    "            metrics[\"goal_inference_accuracy\"] += accuracy\n",
    "    \n",
    "    # Normalize metrics\n",
    "    for key in metrics:\n",
    "        metrics[key] /= n_episodes\n",
    "    \n",
    "    # Print evaluation results\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(f\"  Success Rate: {metrics['success_rate']:.3f}\")\n",
    "    print(f\"  Collision Rate: {metrics['collision_rate']:.3f}\")\n",
    "    print(f\"  Timeout Rate: {metrics['timeout_rate']:.3f}\")\n",
    "    print(f\"  Average Reward: {metrics['avg_reward']:.3f}\")\n",
    "    print(f\"  Average Steps: {metrics['avg_steps']:.1f}\")\n",
    "    print(f\"  Average Gamma: {metrics['avg_gamma']:.3f}\")\n",
    "    print(f\"  Goal Inference Accuracy: {metrics['goal_inference_accuracy']:.3f}\")\n",
    "    \n",
    "    # Save metrics to file\n",
    "    os.makedirs(\"evaluation_results\", exist_ok=True)\n",
    "    with open(\"evaluation_results/metrics.json\", \"w\") as f:\n",
    "        import json\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    print(\"Saved evaluation metrics to evaluation_results/metrics.json\")\n",
    "    \n",
    "    # Generate gamma heatmap\n",
    "    generate_gamma_heatmap(env, model, \"evaluation_results/gamma_heatmap.png\")\n",
    "    \n",
    "    env.close()\n",
    "    return metrics\n",
    "\n",
    "###############################################################################\n",
    "# MAIN EXPERIMENT FUNCTION\n",
    "###############################################################################\n",
    "def run_experiment(visualize=False):\n",
    "    \"\"\"Run the complete shared autonomy experiment.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"SHARED AUTONOMY EXPERIMENT\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Set up directories\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    os.makedirs(\"training_metrics\", exist_ok=True)\n",
    "    os.makedirs(\"evaluation_results\", exist_ok=True)\n",
    "    os.makedirs(\"bayesian_results\", exist_ok=True)\n",
    "    \n",
    "    # Step 1: Train Bayesian model\n",
    "    print(\"\\n[1/3] Training Bayesian goal inference model...\")\n",
    "    bayesian_model = train_bayesian_model(visualize=visualize)\n",
    "    \n",
    "    # Step 2: Train PPO model\n",
    "    print(\"\\n[2/3] Training shared autonomy PPO model...\")\n",
    "    # Smaller timesteps for faster execution, increase for better performance\n",
    "    ppo_model = train_ppo_model(\n",
    "        total_timesteps=10000,\n",
    "        visualize=visualize,\n",
    "        use_joint_optimization=True\n",
    "    )\n",
    "    \n",
    "    # Step 3: Evaluate model\n",
    "    print(\"\\n[3/3] Evaluating model...\")\n",
    "    metrics = evaluate_model(\n",
    "        model_path=\"models/shared_autonomy_model\",\n",
    "        n_episodes=10,\n",
    "        visualize=visualize\n",
    "    )\n",
    "    \n",
    "    print(\"\\nExperiment complete!\")\n",
    "    print(\"Results are saved in:\")\n",
    "    print(\"- bayesian_results/ (Bayesian model performance)\")\n",
    "    print(\"- training_metrics/ (training curves)\")\n",
    "    print(\"- models/ (trained models)\")\n",
    "    print(\"- evaluation_results/ (evaluation results and heatmaps)\")\n",
    "    \n",
    "    return {\n",
    "        \"bayesian_model\": bayesian_model,\n",
    "        \"ppo_model\": ppo_model,\n",
    "        \"evaluation_metrics\": metrics\n",
    "    }\n",
    "\n",
    "###############################################################################\n",
    "# MAIN SCRIPT\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the experiment with visualization (set to False to run headless)\n",
    "    run_experiment(visualize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
