{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in Jupyter environment\n",
      "To run the experiment, use the following in a new cell:\n",
      "results = run_full_experiment(visualize=False, timesteps=50000)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import entropy\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "import pickle\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback, CallbackList\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.utils import obs_as_tensor\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "\n",
    "# For rendering (optional):\n",
    "try:\n",
    "    import pygame\n",
    "except ImportError:\n",
    "    pygame = None\n",
    "\n",
    "###############################################################################\n",
    "# CONSTANTS & UTILS\n",
    "###############################################################################\n",
    "FULL_VIEW_SIZE = (1200, 800)\n",
    "SCALING_FACTOR_X = FULL_VIEW_SIZE[0] / 600.0\n",
    "SCALING_FACTOR_Y = FULL_VIEW_SIZE[1] / 600.0\n",
    "SCALING_FACTOR   = (SCALING_FACTOR_X + SCALING_FACTOR_Y) / 2\n",
    "\n",
    "DOT_RADIUS       = int(15 * SCALING_FACTOR)\n",
    "TARGET_RADIUS    = int(10 * SCALING_FACTOR)\n",
    "OBSTACLE_RADIUS  = int(10 * SCALING_FACTOR)\n",
    "COLLISION_BUFFER = int(5  * SCALING_FACTOR)\n",
    "MAX_SPEED        = 3 * SCALING_FACTOR\n",
    "NOISE_MAGNITUDE  = 2.5\n",
    "RENDER_FPS       = 30\n",
    "\n",
    "START_POS = np.array([FULL_VIEW_SIZE[0]//2, FULL_VIEW_SIZE[1]//2], dtype=np.float32)\n",
    "\n",
    "# Colors for visualization\n",
    "WHITE = (255, 255, 255)\n",
    "GRAY  = (128, 128, 128)\n",
    "YELLOW= (255, 255, 0)\n",
    "BLACK = (0, 0, 0)\n",
    "RED   = (255, 0, 0)\n",
    "GREEN = (0, 255, 0)\n",
    "BLUE  = (0, 0, 255)\n",
    "PURPLE= (128, 0, 128)\n",
    "\n",
    "def distance(a, b):\n",
    "    return math.hypot(a[0] - b[0], a[1] - b[1])\n",
    "\n",
    "def check_line_collision(start, end, center, radius):\n",
    "    dx = end[0] - start[0]\n",
    "    dy = end[1] - start[1]\n",
    "    fx = center[0] - start[0]\n",
    "    fy = center[1] - start[1]\n",
    "    l2 = dx*dx + dy*dy\n",
    "    if l2 < 1e-9:\n",
    "        return distance(start, center) <= radius\n",
    "    t = max(0, min(1, (fx*dx + fy*dy) / l2))\n",
    "    px = start[0] + t*dx\n",
    "    py = start[1] + t*dy\n",
    "    return distance((px, py), center) <= radius\n",
    "\n",
    "def line_collision(pos, new_pos, obstacles):\n",
    "    for obs in obstacles:\n",
    "        if check_line_collision(pos, new_pos, obs, OBSTACLE_RADIUS + COLLISION_BUFFER):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def inside_obstacle(pos, obstacles):\n",
    "    for obs in obstacles:\n",
    "        if distance(pos, obs) <= (OBSTACLE_RADIUS + DOT_RADIUS):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def potential_field_dir(pos, goal, obstacles):\n",
    "    \"\"\"\n",
    "    Returns a normalized direction from pos to goal,\n",
    "    plus repulsion from obstacles.\n",
    "    \"\"\"\n",
    "    gx = goal[0] - pos[0]\n",
    "    gy = goal[1] - pos[1]\n",
    "    dg = math.hypot(gx, gy)\n",
    "    if dg < 1e-6:\n",
    "        return np.zeros(2, dtype=np.float32)\n",
    "    att = np.array([gx / dg, gy / dg], dtype=np.float32)\n",
    "\n",
    "    repulse_x = 0.0\n",
    "    repulse_y = 0.0\n",
    "    repulsion_radius = 23.0 * SCALING_FACTOR\n",
    "    repulsion_gain   = 30000.0\n",
    "\n",
    "    for obs in obstacles:\n",
    "        dx = pos[0] - obs[0]\n",
    "        dy = pos[1] - obs[1]\n",
    "        dobs = math.hypot(dx, dy)\n",
    "        if dobs < 1e-9:\n",
    "            continue\n",
    "        if dobs < repulsion_radius:\n",
    "            pushx    = dx / dobs\n",
    "            pushy    = dy / dobs\n",
    "            strength = repulsion_gain / (dobs**2)\n",
    "            repulse_x += pushx * strength\n",
    "            repulse_y += pushy * strength\n",
    "\n",
    "    px = att[0] + repulse_x\n",
    "    py = att[1] + repulse_y\n",
    "    mg = math.hypot(px, py)\n",
    "    if mg < 1e-9:\n",
    "        return np.zeros(2, dtype=np.float32)\n",
    "    return np.array([px / mg, py / mg], dtype=np.float32)\n",
    "\n",
    "###############################################################################\n",
    "# BAYESIAN GOAL INFERENCE MODEL\n",
    "###############################################################################\n",
    "class BayesianGoalInference:\n",
    "    \"\"\"\n",
    "    Recursive Bayesian goal inference model that maintains and updates\n",
    "    a probability distribution over potential goals based on the human inputs.\n",
    "    \"\"\"\n",
    "    def __init__(self, beta=10.0, w_theta=0.7, w_d=0.3, decay_rate=0.85):\n",
    "        \"\"\"\n",
    "        Initialize the Bayesian goal inference model.\n",
    "        \n",
    "        Args:\n",
    "            beta (float): Rationality parameter that determines how closely\n",
    "                         the human follows the optimal policy\n",
    "            w_theta (float): Weight for angular deviation in the cost function\n",
    "            w_d (float): Weight for distance deviation in the cost function\n",
    "            decay_rate (float): Decay rate for temporal smoothing of probabilities\n",
    "        \"\"\"\n",
    "        self.beta = beta                # Rationality parameter\n",
    "        self.w_theta = w_theta          # Weight for angular deviation in cost\n",
    "        self.w_d = w_d                  # Weight for distance deviation in cost\n",
    "        self.decay_rate = decay_rate    # For temporal smoothing\n",
    "        self.goals = []                 # List of potential goals\n",
    "        self.priors = None              # Prior probabilities for goals\n",
    "        self.goal_probs = None          # Current goal probabilities\n",
    "        self.calibrator = None          # For confidence calibration\n",
    "        self.history = []               # History of probability updates\n",
    "        self.max_hist_len = 30          # Maximum history length to store\n",
    "    \n",
    "    def initialize_goals(self, goals):\n",
    "        \"\"\"\n",
    "        Initialize goal distribution with equal priors.\n",
    "        \n",
    "        Args:\n",
    "            goals (list): List of potential goal positions\n",
    "        \"\"\"\n",
    "        self.goals = goals\n",
    "        n_goals = len(goals)\n",
    "        # Uniform prior\n",
    "        self.priors = np.ones(n_goals) / n_goals\n",
    "        self.goal_probs = self.priors.copy()\n",
    "        self.history = []\n",
    "    \n",
    "    def load_calibrator(self, calibrator_path):\n",
    "        \"\"\"\n",
    "        Load a pre-trained confidence calibrator.\n",
    "        \n",
    "        Args:\n",
    "            calibrator_path (str): Path to the saved calibrator model\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(calibrator_path, 'rb') as f:\n",
    "                self.calibrator = pickle.load(f)\n",
    "            print(f\"Loaded calibrator from {calibrator_path}\")\n",
    "        except:\n",
    "            print(f\"Could not load calibrator from {calibrator_path}\")\n",
    "            self.calibrator = None\n",
    "    \n",
    "    def train_calibrator(self, confidences, accuracies):\n",
    "        \"\"\"\n",
    "        Train a confidence calibrator using isotonic regression.\n",
    "        \n",
    "        Args:\n",
    "            confidences (list): List of predicted confidence values\n",
    "            accuracies (list): List of binary accuracy values (0 or 1)\n",
    "        \"\"\"\n",
    "        if len(confidences) < 10:\n",
    "            print(\"Not enough data to train calibrator\")\n",
    "            return\n",
    "        \n",
    "        self.calibrator = IsotonicRegression(out_of_bounds='clip')\n",
    "        self.calibrator.fit(confidences, accuracies)\n",
    "        \n",
    "        # Save the calibrator\n",
    "        os.makedirs(\"models\", exist_ok=True)\n",
    "        with open(\"models/calibrator.pkl\", 'wb') as f:\n",
    "            pickle.dump(self.calibrator, f)\n",
    "        print(\"Calibrator trained and saved\")\n",
    "    \n",
    "    def compute_cost(self, human_input, agent_pos, goal_pos):\n",
    "        \"\"\"\n",
    "        Compute the cost of human input based on deviation from optimal action.\n",
    "        \n",
    "        Args:\n",
    "            human_input (numpy.ndarray): Normalized direction vector of human input\n",
    "            agent_pos (numpy.ndarray): Current agent position\n",
    "            goal_pos (numpy.ndarray): Goal position\n",
    "            \n",
    "        Returns:\n",
    "            float: Cost value\n",
    "        \"\"\"\n",
    "        # Compute optimal direction vector to goal\n",
    "        goal_dir = goal_pos - agent_pos\n",
    "        dist = np.linalg.norm(goal_dir)\n",
    "        if dist < 1e-6:\n",
    "            return 0.0\n",
    "        \n",
    "        goal_dir = goal_dir / dist  # Normalize\n",
    "        \n",
    "        # Compute angular deviation (radians)\n",
    "        dot_product = np.clip(np.dot(human_input, goal_dir), -1.0, 1.0)\n",
    "        theta_dev = abs(np.arccos(dot_product))\n",
    "        \n",
    "        # Compute distance (magnitude) deviation\n",
    "        h_magnitude = np.linalg.norm(human_input)\n",
    "        \n",
    "        # Optimal magnitude scales inversely with proximity to target\n",
    "        # (slow down as approaching the target)\n",
    "        min_dist = TARGET_RADIUS + DOT_RADIUS\n",
    "        if dist < min_dist:\n",
    "            opt_magnitude = 0.0\n",
    "        else:\n",
    "            dist_factor = min(1.0, (dist - min_dist) / (200 * SCALING_FACTOR))\n",
    "            opt_magnitude = dist_factor * 1.0  # Scale to [0,1]\n",
    "        \n",
    "        d_dev = abs(1.0 - h_magnitude / max(opt_magnitude, 1e-6))\n",
    "        \n",
    "        # Combine angular and distance deviation with weights\n",
    "        cost = self.w_theta * theta_dev + self.w_d * d_dev\n",
    "        return cost\n",
    "    \n",
    "    def update(self, agent_pos, human_input, obstacles=None):\n",
    "        \"\"\"\n",
    "        Update the goal probabilities based on the observed human input.\n",
    "        \n",
    "        Args:\n",
    "            agent_pos (numpy.ndarray): Current agent position\n",
    "            human_input (numpy.ndarray): Normalized direction vector of human input\n",
    "            obstacles (list, optional): List of obstacle positions\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Updated goal probabilities\n",
    "        \"\"\"\n",
    "        if len(self.goals) == 0:\n",
    "            return np.array([])\n",
    "        \n",
    "        # Process human input toward each goal\n",
    "        likelihoods = np.zeros(len(self.goals))\n",
    "        costs = np.zeros(len(self.goals))\n",
    "        \n",
    "        for i, goal in enumerate(self.goals):\n",
    "            cost = self.compute_cost(human_input, agent_pos, goal)\n",
    "            costs[i] = cost\n",
    "            # Noisy rational model: P(action|goal) ∝ exp(-β * cost)\n",
    "            likelihoods[i] = np.exp(-self.beta * cost)\n",
    "        \n",
    "        # Normalize likelihoods to prevent numerical issues\n",
    "        if np.sum(likelihoods) > 0:\n",
    "            likelihoods = likelihoods / np.sum(likelihoods)\n",
    "        else:\n",
    "            likelihoods = np.ones_like(likelihoods) / len(likelihoods)\n",
    "        \n",
    "        # Bayesian update: P(goal|action) ∝ P(action|goal) * P(goal)\n",
    "        raw_posteriors = likelihoods * self.goal_probs\n",
    "        \n",
    "        # Normalize\n",
    "        if np.sum(raw_posteriors) > 0:\n",
    "            posteriors = raw_posteriors / np.sum(raw_posteriors)\n",
    "        else:\n",
    "            # If all posteriors are zero, revert to prior\n",
    "            posteriors = self.priors.copy()\n",
    "        \n",
    "        # Apply temporal smoothing using exponential moving average\n",
    "        self.goal_probs = self.decay_rate * self.goal_probs + (1 - self.decay_rate) * posteriors\n",
    "        \n",
    "        # Normalize again after smoothing\n",
    "        if np.sum(self.goal_probs) > 0:\n",
    "            self.goal_probs = self.goal_probs / np.sum(self.goal_probs)\n",
    "        \n",
    "        # Apply calibration if available\n",
    "        if self.calibrator is not None:\n",
    "            max_prob = np.max(self.goal_probs)\n",
    "            calibrated_max = self.calibrator.predict([max_prob])[0]\n",
    "            # Adjust other probabilities proportionally\n",
    "            if max_prob > 0:\n",
    "                scale_factor = calibrated_max / max_prob\n",
    "                self.goal_probs = self.goal_probs * scale_factor\n",
    "                remainder = 1.0 - np.sum(self.goal_probs)\n",
    "                if remainder > 0:\n",
    "                    # Distribute remainder proportionally to other goals\n",
    "                    indices = np.arange(len(self.goal_probs))\n",
    "                    max_idx = np.argmax(self.goal_probs)\n",
    "                    other_indices = indices[indices != max_idx]\n",
    "                    if len(other_indices) > 0:\n",
    "                        other_sum = np.sum(self.goal_probs[other_indices])\n",
    "                        if other_sum > 0:\n",
    "                            for idx in other_indices:\n",
    "                                self.goal_probs[idx] += remainder * (self.goal_probs[idx] / other_sum)\n",
    "                        else:\n",
    "                            # If all other probs are zero, distribute uniformly\n",
    "                            for idx in other_indices:\n",
    "                                self.goal_probs[idx] += remainder / len(other_indices)\n",
    "        \n",
    "        # Keep track of probabilities history\n",
    "        self.history.append(self.goal_probs.copy())\n",
    "        if len(self.history) > self.max_hist_len:\n",
    "            self.history.pop(0)\n",
    "        \n",
    "        return self.goal_probs\n",
    "    \n",
    "    def get_goal_probs(self):\n",
    "        \"\"\"\n",
    "        Get the current goal probabilities.\n",
    "        \n",
    "        Returns:\n",
    "            numpy.ndarray: Current goal probabilities\n",
    "        \"\"\"\n",
    "        return self.goal_probs\n",
    "    \n",
    "    def get_most_likely_goal(self):\n",
    "        \"\"\"\n",
    "        Get the most likely goal and its probability.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (goal_position, probability)\n",
    "        \"\"\"\n",
    "        if len(self.goals) == 0:\n",
    "            return None, 0.0\n",
    "        \n",
    "        max_idx = np.argmax(self.goal_probs)\n",
    "        return self.goals[max_idx], self.goal_probs[max_idx]\n",
    "    \n",
    "    def get_entropy(self):\n",
    "        \"\"\"\n",
    "        Calculate the entropy of the current goal distribution.\n",
    "        \n",
    "        Returns:\n",
    "            float: Entropy value\n",
    "        \"\"\"\n",
    "        return entropy(self.goal_probs)\n",
    "    \n",
    "    def compute_expert_recommendation(self, agent_pos, obstacles):\n",
    "        \"\"\"\n",
    "        Compute the expert's recommended action based on goal probabilities.\n",
    "        \n",
    "        Args:\n",
    "            agent_pos (numpy.ndarray): Current agent position\n",
    "            obstacles (list): List of obstacle positions\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Normalized direction vector for expert recommendation\n",
    "        \"\"\"\n",
    "        if len(self.goals) == 0:\n",
    "            return np.zeros(2, dtype=np.float32)\n",
    "        \n",
    "        # Compute weighted direction based on goal probabilities\n",
    "        weighted_dir = np.zeros(2, dtype=np.float32)\n",
    "        \n",
    "        for i, goal in enumerate(self.goals):\n",
    "            # Get the expert direction for this goal using potential field\n",
    "            expert_dir = potential_field_dir(agent_pos, goal, obstacles)\n",
    "            # Weight by goal probability\n",
    "            weighted_dir += self.goal_probs[i] * expert_dir\n",
    "        \n",
    "        # Normalize\n",
    "        magnitude = np.linalg.norm(weighted_dir)\n",
    "        if magnitude > 1e-6:\n",
    "            weighted_dir = weighted_dir / magnitude\n",
    "        \n",
    "        return weighted_dir\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the model to initial state with uniform priors.\"\"\"\n",
    "        if len(self.goals) > 0:\n",
    "            self.goal_probs = self.priors.copy()\n",
    "        self.history = []\n",
    "\n",
    "###############################################################################\n",
    "# CUSTOM POLICY: DUAL-HEAD NETWORK ARCHITECTURE\n",
    "###############################################################################\n",
    "class DualHeadMlpPolicy(ActorCriticPolicy):\n",
    "    \"\"\"\n",
    "    Custom policy with a dual-head architecture:\n",
    "    - Shared encoder for feature extraction\n",
    "    - Separate heads for goal inference and assistance determination\n",
    "    \"\"\"\n",
    "    def __init__(self, observation_space, action_space, lr_schedule, **kwargs):\n",
    "        super(DualHeadMlpPolicy, self).__init__(\n",
    "            observation_space,\n",
    "            action_space,\n",
    "            lr_schedule,\n",
    "            **kwargs\n",
    "        )\n",
    "    \n",
    "    def _build_mlp_extractor(self, config=None):\n",
    "        \"\"\"\n",
    "        Build the feature extraction network.\n",
    "        \n",
    "        Overridden to create a specialized architecture with better feature extraction\n",
    "        for both goal inference and policy determination.\n",
    "        \"\"\"\n",
    "        # Create a custom MLP extractor with separated streams\n",
    "        # for goal inference and policy determination\n",
    "        class DualHeadExtractor(nn.Module):\n",
    "            def __init__(self, feature_dim, goal_features, policy_features):\n",
    "                super(DualHeadExtractor, self).__init__()\n",
    "                \n",
    "                # Shared encoder\n",
    "                self.shared = nn.Sequential(\n",
    "                    nn.Linear(feature_dim, 256),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(256, 256),\n",
    "                    nn.ReLU()\n",
    "                )\n",
    "                \n",
    "                # Goal inference branch - predicts goal probabilities\n",
    "                self.goal_branch = nn.Sequential(\n",
    "                    nn.Linear(256, 128),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(128, goal_features),\n",
    "                    nn.Softmax(dim=-1)\n",
    "                )\n",
    "                \n",
    "                # Policy determination branch\n",
    "                self.pi_branch = nn.Sequential(\n",
    "                    nn.Linear(256 + goal_features, 256),  # Includes goal probs\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(256, policy_features),\n",
    "                    nn.ReLU()\n",
    "                )\n",
    "                \n",
    "                # Value branch\n",
    "                self.vf_branch = nn.Sequential(\n",
    "                    nn.Linear(256 + goal_features, 256),  # Includes goal probs\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(256, policy_features),\n",
    "                    nn.ReLU()\n",
    "                )\n",
    "            \n",
    "            def forward(self, features):\n",
    "                shared_features = self.shared(features)\n",
    "                \n",
    "                # Goal inference prediction\n",
    "                goal_probs = self.goal_branch(shared_features)\n",
    "                \n",
    "                # Concatenate shared features with predicted goal probabilities\n",
    "                combined_features = torch.cat([shared_features, goal_probs], dim=-1)\n",
    "                \n",
    "                # Policy (pi) and value (vf) features\n",
    "                pi_features = self.pi_branch(combined_features)\n",
    "                vf_features = self.vf_branch(combined_features)\n",
    "                \n",
    "                return pi_features, vf_features, goal_probs\n",
    "        \n",
    "        # Number of potential goals - will be set by the environment\n",
    "        n_goals = 8  # Default, will be updated at runtime\n",
    "        \n",
    "        # Create the dual-head extractor\n",
    "        self.mlp_extractor = DualHeadExtractor(\n",
    "            feature_dim=self.features_dim,\n",
    "            goal_features=n_goals,\n",
    "            policy_features=256\n",
    "        )\n",
    "        \n",
    "        return self.mlp_extractor\n",
    "    \n",
    "    def forward(self, obs, deterministic=False):\n",
    "        \"\"\"\n",
    "        Forward pass of the neural network.\n",
    "        \n",
    "        Args:\n",
    "            obs: Observation tensor\n",
    "            deterministic: Whether to sample or take the most likely action\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (actions, values, log_probs, goal_probs)\n",
    "        \"\"\"\n",
    "        # Extract features\n",
    "        features = self.extract_features(obs)\n",
    "        \n",
    "        # Get policy features, value features, and goal probabilities\n",
    "        latent_pi, latent_vf, goal_probs = self.mlp_extractor(features)\n",
    "        \n",
    "        # Get action distribution parameters\n",
    "        mean_actions = self.action_net(latent_pi)\n",
    "        log_std = torch.clamp(self.log_std, -20, 2)  # Constrain for numerical stability\n",
    "        std = torch.exp(log_std)\n",
    "        \n",
    "        # Create action distribution\n",
    "        distribution = Normal(mean_actions, std)\n",
    "        \n",
    "        # Sample or take most likely action\n",
    "        if deterministic:\n",
    "            actions = torch.tanh(mean_actions)\n",
    "        else:\n",
    "            actions = torch.tanh(distribution.rsample())\n",
    "        \n",
    "        log_prob = distribution.log_prob(actions)\n",
    "        \n",
    "        # Get value estimate\n",
    "        values = self.value_net(latent_vf)\n",
    "        \n",
    "        return actions, values, log_prob, goal_probs\n",
    "\n",
    "###############################################################################\n",
    "# METRICS CALLBACK WITH GOAL INFERENCE TRACKING\n",
    "###############################################################################\n",
    "class EnhancedMetricsCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Enhanced callback that logs training metrics including goal inference performance.\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        # Episode metrics\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.episode_mean_gammas = []\n",
    "        self.episode_std_gammas = []\n",
    "        self.goal_inference_accuracy = []\n",
    "        self.goal_entropy = []\n",
    "        self.path_completion_accuracy = []  # Accuracy at different path completions\n",
    "        \n",
    "        # Current episode tracking\n",
    "        self.total_reward = 0.0\n",
    "        self.ep_length = 0\n",
    "        self.current_episode_gammas = []\n",
    "        self.current_goal_probs = []\n",
    "        self.current_true_goal_idx = None\n",
    "        self.trajectory_start = None\n",
    "        self.trajectory_positions = []\n",
    "        \n",
    "        # Collision tracking\n",
    "        self.n_collisions = 0\n",
    "        self.n_episodes = 0\n",
    "        \n",
    "        # Loss tracking\n",
    "        self.losses = []\n",
    "        self.value_losses = []\n",
    "        self.policy_losses = []\n",
    "        self.entropy_losses = []\n",
    "        self.training_steps = []\n",
    "        self.n_updates = 0\n",
    "        \n",
    "        # Goal inference performance at different path completion percentages\n",
    "        self.completion_thresholds = [0.25, 0.5, 0.75]\n",
    "        self.completion_accuracies = {t: [] for t in self.completion_thresholds}\n",
    "\n",
    "    def _on_training_start(self):\n",
    "        \"\"\"Called at the start of training.\"\"\"\n",
    "        self.episode_rewards.clear()\n",
    "        self.episode_lengths.clear()\n",
    "        self.episode_mean_gammas.clear()\n",
    "        self.episode_std_gammas.clear()\n",
    "        self.goal_inference_accuracy.clear()\n",
    "        self.goal_entropy.clear()\n",
    "        self.path_completion_accuracy.clear()\n",
    "        \n",
    "        self.total_reward = 0.0\n",
    "        self.ep_length = 0\n",
    "        self.current_episode_gammas.clear()\n",
    "        self.current_goal_probs.clear()\n",
    "        self.current_true_goal_idx = None\n",
    "        self.trajectory_start = None\n",
    "        self.trajectory_positions = []\n",
    "        \n",
    "        self.n_collisions = 0\n",
    "        self.n_episodes = 0\n",
    "        \n",
    "        for t in self.completion_thresholds:\n",
    "            self.completion_accuracies[t].clear()\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"\n",
    "        Called at each step of training.\n",
    "        \n",
    "        Returns:\n",
    "            bool: Whether training should continue\n",
    "        \"\"\"\n",
    "        actions = self.locals['actions']\n",
    "        rewards = self.locals['rewards']\n",
    "        dones = self.locals['dones']\n",
    "        infos = self.locals['infos']\n",
    "        obs = self.locals['new_obs']\n",
    "        \n",
    "        # Get the environment instance\n",
    "        env = self.model.env.envs[0]\n",
    "        \n",
    "        # Compute gamma from the action (mapping [-1,1] -> [0,1])\n",
    "        gamma_val = 0.5 * (actions[0][0] + 1.0)\n",
    "        self.current_episode_gammas.append(gamma_val)\n",
    "        \n",
    "        # Track reward and episode length\n",
    "        r = float(rewards[0])\n",
    "        self.total_reward += r\n",
    "        self.ep_length += 1\n",
    "        \n",
    "        # Store current position for trajectory analysis\n",
    "        if hasattr(env, 'dot_pos'):\n",
    "            current_pos = env.dot_pos.copy()\n",
    "            self.trajectory_positions.append(current_pos)\n",
    "            \n",
    "            # Record start position if this is first step\n",
    "            if self.trajectory_start is None:\n",
    "                self.trajectory_start = current_pos.copy()\n",
    "        \n",
    "        # If using the dual-head policy, get goal probabilities\n",
    "        if hasattr(env, 'bayesian_model') and hasattr(env, 'true_goal_idx'):\n",
    "            # Get goal probabilities from environment's Bayesian model\n",
    "            goal_probs = env.bayesian_model.get_goal_probs()\n",
    "            self.current_goal_probs.append(goal_probs.copy())\n",
    "            \n",
    "            # Store true goal index for accuracy calculation\n",
    "            if self.current_true_goal_idx is None:\n",
    "                self.current_true_goal_idx = env.true_goal_idx\n",
    "            \n",
    "            # Calculate current path completion\n",
    "            if hasattr(env, 'goal_pos') and len(self.trajectory_positions) > 1:\n",
    "                goal_pos = env.goal_pos\n",
    "                total_distance = distance(self.trajectory_start, goal_pos)\n",
    "                \n",
    "                if total_distance > 0:\n",
    "                    current_distance = distance(current_pos, goal_pos)\n",
    "                    path_completion = 1.0 - (current_distance / total_distance)\n",
    "                    \n",
    "                    # For each completion threshold, check if we've just crossed it\n",
    "                    for threshold in self.completion_thresholds:\n",
    "                        if path_completion >= threshold:\n",
    "                            # Check if prediction is correct\n",
    "                            pred_goal = np.argmax(goal_probs)\n",
    "                            is_correct = (pred_goal == self.current_true_goal_idx)\n",
    "                            \n",
    "                            # Store in appropriate bin if not already captured for this episode\n",
    "                            threshold_key = f\"{int(threshold*100)}%\"\n",
    "                            if not hasattr(self, f\"recorded_{threshold_key}\"):\n",
    "                                setattr(self, f\"recorded_{threshold_key}\", True)\n",
    "                                self.completion_accuracies[threshold].append(int(is_correct))\n",
    "        \n",
    "        if dones[0]:\n",
    "            # Calculate goal inference accuracy for this episode\n",
    "            if self.current_goal_probs and self.current_true_goal_idx is not None:\n",
    "                # Average accuracy across episode\n",
    "                accuracies = []\n",
    "                entropies = []\n",
    "                \n",
    "                for probs in self.current_goal_probs:\n",
    "                    pred_goal = np.argmax(probs)\n",
    "                    accuracies.append(int(pred_goal == self.current_true_goal_idx))\n",
    "                    entropies.append(entropy(probs))\n",
    "                \n",
    "                # Store average accuracy and entropy\n",
    "                self.goal_inference_accuracy.append(np.mean(accuracies))\n",
    "                self.goal_entropy.append(np.mean(entropies))\n",
    "            \n",
    "            # Store episode statistics\n",
    "            self.episode_rewards.append(self.total_reward)\n",
    "            self.episode_lengths.append(self.ep_length)\n",
    "            \n",
    "            if self.current_episode_gammas:\n",
    "                mean_gamma = np.mean(self.current_episode_gammas)\n",
    "                std_gamma = np.std(self.current_episode_gammas)\n",
    "                self.episode_mean_gammas.append(mean_gamma)\n",
    "                self.episode_std_gammas.append(std_gamma)\n",
    "            \n",
    "            # Reset episode tracking\n",
    "            self.total_reward = 0.0\n",
    "            self.ep_length = 0\n",
    "            self.current_episode_gammas.clear()\n",
    "            self.current_goal_probs.clear()\n",
    "            self.current_true_goal_idx = None\n",
    "            self.trajectory_start = None\n",
    "            self.trajectory_positions.clear()\n",
    "            \n",
    "            # Reset completion recording flags\n",
    "            for threshold in self.completion_thresholds:\n",
    "                threshold_key = f\"{int(threshold*100)}%\"\n",
    "                if hasattr(self, f\"recorded_{threshold_key}\"):\n",
    "                    delattr(self, f\"recorded_{threshold_key}\")\n",
    "            \n",
    "            # Increment episode and collision counters\n",
    "            self.n_episodes += 1\n",
    "            if 'terminal_reason' in infos[0] and infos[0]['terminal_reason'] == 'collision':\n",
    "                self.n_collisions += 1\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        \"\"\"Called at the end of a rollout.\"\"\"\n",
    "        self.n_updates += 1\n",
    "        logs = self.model.logger.name_to_value or {}\n",
    "        \n",
    "        # Record loss metrics\n",
    "        if \"train/loss\" in logs:\n",
    "            self.losses.append(logs[\"train/loss\"])\n",
    "            self.training_steps.append(self.n_updates)\n",
    "        if \"train/value_loss\" in logs:\n",
    "            self.value_losses.append(logs[\"train/value_loss\"])\n",
    "        if \"train/policy_gradient_loss\" in logs:\n",
    "            self.policy_losses.append(logs[\"train/policy_gradient_loss\"])\n",
    "        if \"train/entropy_loss\" in logs:\n",
    "            self.entropy_losses.append(logs[\"train/entropy_loss\"])\n",
    "\n",
    "    def _moving_average(self, data, window=10):\n",
    "        \"\"\"Calculate moving average of data.\"\"\"\n",
    "        if len(data) < window:\n",
    "            return np.array(data)\n",
    "        return np.convolve(data, np.ones(window)/window, mode='valid')\n",
    "\n",
    "    def save_metrics(self, save_dir=\"training_metrics\"):\n",
    "        \"\"\"\n",
    "        Save all metrics and plots to the specified directory.\n",
    "        \n",
    "        Args:\n",
    "            save_dir (str): Directory to save metrics\n",
    "        \"\"\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        # 1. Episode Rewards\n",
    "        if self.episode_rewards:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(self.episode_rewards, label=\"Episode Reward\", alpha=0.6)\n",
    "            ma_rewards = self._moving_average(self.episode_rewards, 10)\n",
    "            if len(ma_rewards):\n",
    "                plt.plot(range(10 - 1, 10 - 1 + len(ma_rewards)), \n",
    "                         ma_rewards, label=\"MA(10)\", color='red', linewidth=2)\n",
    "            plt.xlabel(\"Episode\")\n",
    "            plt.ylabel(\"Reward\")\n",
    "            plt.title(\"Episode Rewards\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.savefig(os.path.join(save_dir, \"episode_rewards.png\"))\n",
    "            plt.close()\n",
    "        \n",
    "        # 2. Average Gamma per Episode\n",
    "        if self.episode_mean_gammas:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(self.episode_mean_gammas, label=\"Average Gamma\", alpha=0.6)\n",
    "            ma_gamma = self._moving_average(self.episode_mean_gammas, 10)\n",
    "            if len(ma_gamma):\n",
    "                plt.plot(range(10 - 1, 10 - 1 + len(ma_gamma)), \n",
    "                         ma_gamma, label=\"MA(10)\", color='red', linewidth=2)\n",
    "            plt.xlabel(\"Episode\")\n",
    "            plt.ylabel(\"Gamma (avg)\")\n",
    "            plt.title(\"Average Gamma per Episode\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.savefig(os.path.join(save_dir, \"average_gamma.png\"))\n",
    "            plt.close()\n",
    "        \n",
    "        # 3. Gamma Std per Episode\n",
    "        if self.episode_std_gammas:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(self.episode_std_gammas, label=\"Gamma Std\", alpha=0.6)\n",
    "            ma_gstd = self._moving_average(self.episode_std_gammas, 10)\n",
    "            if len(ma_gstd):\n",
    "                plt.plot(range(10 - 1, 10 - 1 + len(ma_gstd)), \n",
    "                         ma_gstd, label=\"MA(10)\", color='red', linewidth=2)\n",
    "            plt.xlabel(\"Episode\")\n",
    "            plt.ylabel(\"Gamma Std\")\n",
    "            plt.title(\"Gamma Std per Episode\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.savefig(os.path.join(save_dir, \"gamma_std.png\"))\n",
    "            plt.close()\n",
    "        \n",
    "        # 4. Goal Inference Accuracy\n",
    "        if self.goal_inference_accuracy:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(self.goal_inference_accuracy, label=\"Goal Inference Accuracy\", alpha=0.6)\n",
    "            ma_acc = self._moving_average(self.goal_inference_accuracy, 10)\n",
    "            if len(ma_acc):\n",
    "                plt.plot(range(10 - 1, 10 - 1 + len(ma_acc)), \n",
    "                         ma_acc, label=\"MA(10)\", color='red', linewidth=2)\n",
    "            plt.xlabel(\"Episode\")\n",
    "            plt.ylabel(\"Accuracy\")\n",
    "            plt.title(\"Goal Inference Accuracy per Episode\")\n",
    "            plt.ylim(0, 1.05)\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.savefig(os.path.join(save_dir, \"goal_inference_accuracy.png\"))\n",
    "            plt.close()\n",
    "        \n",
    "        # 5. Goal Distribution Entropy\n",
    "        if self.goal_entropy:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(self.goal_entropy, label=\"Goal Distribution Entropy\", alpha=0.6)\n",
    "            ma_ent = self._moving_average(self.goal_entropy, 10)\n",
    "            if len(ma_ent):\n",
    "                plt.plot(range(10 - 1, 10 - 1 + len(ma_ent)), \n",
    "                         ma_ent, label=\"MA(10)\", color='red', linewidth=2)\n",
    "            plt.xlabel(\"Episode\")\n",
    "            plt.ylabel(\"Entropy\")\n",
    "            plt.title(\"Goal Distribution Entropy per Episode\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.savefig(os.path.join(save_dir, \"goal_entropy.png\"))\n",
    "            plt.close()\n",
    "        \n",
    "        # 6. Path Completion Accuracy\n",
    "        accuracy_by_completion = {}\n",
    "        for threshold in self.completion_thresholds:\n",
    "            if self.completion_accuracies[threshold]:\n",
    "                accuracy_by_completion[threshold] = np.mean(self.completion_accuracies[threshold])\n",
    "        \n",
    "        if accuracy_by_completion:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            x = list(accuracy_by_completion.keys())\n",
    "            y = [accuracy_by_completion[t] for t in x]\n",
    "            plt.bar([str(int(t*100))+\"%\" for t in x], y)\n",
    "            plt.xlabel(\"Path Completion\")\n",
    "            plt.ylabel(\"Accuracy\")\n",
    "            plt.title(\"Goal Inference Accuracy vs. Path Completion\")\n",
    "            plt.ylim(0, 1.05)\n",
    "            plt.grid(True, axis='y')\n",
    "            plt.savefig(os.path.join(save_dir, \"completion_accuracy.png\"))\n",
    "            plt.close()\n",
    "        \n",
    "        # 7. Total Model Loss\n",
    "        if self.losses:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(self.training_steps, self.losses, label=\"Total Model Loss\", alpha=0.7)\n",
    "            if len(self.losses) >= 10:\n",
    "                ma_loss = self._moving_average(self.losses, 10)\n",
    "                plt.plot(range(self.training_steps[0] + (10 - 1),\n",
    "                               self.training_steps[0] + (10 - 1) + len(ma_loss)),\n",
    "                         ma_loss, label=\"MA(10)\", color='red', linewidth=2)\n",
    "            plt.xlabel(\"Training Updates\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.title(\"Total Model Loss Over Rollouts\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.savefig(os.path.join(save_dir, \"total_loss.png\"))\n",
    "            plt.close()\n",
    "        \n",
    "        # 8. Critic (Value) Loss\n",
    "        if self.value_losses:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(self.value_losses, label=\"Value Loss\", alpha=0.7)\n",
    "            if len(self.value_losses) >= 10:\n",
    "                ma_val_loss = self._moving_average(self.value_losses, 10)\n",
    "                plt.plot(range(10 - 1, 10 - 1 + len(ma_val_loss)), \n",
    "                         ma_val_loss, label=\"MA(10)\", color='red', linewidth=2)\n",
    "            plt.xlabel(\"Rollout End #\")\n",
    "            plt.ylabel(\"Value Loss\")\n",
    "            plt.title(\"Value (Critic) Loss\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.savefig(os.path.join(save_dir, \"value_loss.png\"))\n",
    "            plt.close()\n",
    "        \n",
    "        # 9. Actor (Policy) Loss\n",
    "        if self.policy_losses:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(self.policy_losses, label=\"Policy Loss\", alpha=0.7)\n",
    "            if len(self.policy_losses) >= 10:\n",
    "                ma_pol_loss = self._moving_average(self.policy_losses, 10)\n",
    "                plt.plot(range(10 - 1, 10 - 1 + len(ma_pol_loss)), \n",
    "                         ma_pol_loss, label=\"MA(10)\", color='red', linewidth=2)\n",
    "            plt.xlabel(\"Rollout End #\")\n",
    "            plt.ylabel(\"Policy Loss\")\n",
    "            plt.title(\"Policy (Actor) Loss\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.savefig(os.path.join(save_dir, \"policy_loss.png\"))\n",
    "            plt.close()\n",
    "        \n",
    "        # 10. Entropy Loss\n",
    "        if self.entropy_losses:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(self.entropy_losses, label=\"Entropy Loss\", alpha=0.7)\n",
    "            if len(self.entropy_losses) >= 10:\n",
    "                ma_ent_loss = self._moving_average(self.entropy_losses, 10)\n",
    "                plt.plot(range(10 - 1, 10 - 1 + len(ma_ent_loss)), \n",
    "                         ma_ent_loss, label=\"MA(10)\", color='red', linewidth=2)\n",
    "            plt.xlabel(\"Rollout End #\")\n",
    "            plt.ylabel(\"Entropy Loss\")\n",
    "            plt.title(\"Entropy Loss (Exploration)\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.savefig(os.path.join(save_dir, \"entropy_loss.png\"))\n",
    "            plt.close()\n",
    "        \n",
    "        # 11. Episode Length\n",
    "        if self.episode_lengths:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(self.episode_lengths, label=\"Episode Length\", alpha=0.6)\n",
    "            ma_length = self._moving_average(self.episode_lengths, 10)\n",
    "            if len(ma_length):\n",
    "                plt.plot(range(10 - 1, 10 - 1 + len(ma_length)), \n",
    "                         ma_length, label=\"MA(10)\", color='red', linewidth=2)\n",
    "            plt.xlabel(\"Episode\")\n",
    "            plt.ylabel(\"Length (# steps)\")\n",
    "            plt.title(\"Episode Length\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.savefig(os.path.join(save_dir, \"episode_length.png\"))\n",
    "            plt.close()\n",
    "        \n",
    "        # 12. Save summary text\n",
    "        with open(os.path.join(save_dir, \"summary.txt\"), \"w\") as f:\n",
    "            f.write(f\"Total Episodes: {len(self.episode_rewards)}\\n\")\n",
    "            if self.episode_rewards:\n",
    "                avg_reward = np.mean(self.episode_rewards)\n",
    "                f.write(f\"Mean Episode Reward: {avg_reward:.3f}\\n\")\n",
    "            f.write(f\"Collisions Count: {self.n_collisions}\\n\")\n",
    "            if self.episode_mean_gammas:\n",
    "                mean_gamma_all = np.mean(self.episode_mean_gammas)\n",
    "                f.write(f\"Mean of Average-Gamma: {mean_gamma_all:.3f}\\n\")\n",
    "            if self.goal_inference_accuracy:\n",
    "                f.write(f\"Mean Goal Inference Accuracy: {np.mean(self.goal_inference_accuracy):.3f}\\n\")\n",
    "            for threshold in sorted(self.completion_thresholds):\n",
    "                if self.completion_accuracies[threshold]:\n",
    "                    f.write(f\"Goal Accuracy @ {int(threshold*100)}% completion: {np.mean(self.completion_accuracies[threshold]):.3f}\\n\")\n",
    "\n",
    "###############################################################################\n",
    "# CONTEXT-ADAPTIVE SHARED AUTONOMY ENVIRONMENT\n",
    "###############################################################################\n",
    "class SharedAutonomyEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Environment for training context-adaptive shared autonomy.\n",
    "    \n",
    "    Features:\n",
    "    - Multiple potential targets with varying goal ambiguity\n",
    "    - Dynamic obstacle environments\n",
    "    - Recursive Bayesian goal inference\n",
    "    - Context-dependent blending of human and AI control\n",
    "    \"\"\"\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": RENDER_FPS}\n",
    "    \n",
    "    def __init__(self, visualize=False, use_joint_optimization=True):\n",
    "        \"\"\"\n",
    "        Initialize the shared autonomy environment.\n",
    "        \n",
    "        Args:\n",
    "            visualize (bool): Whether to visualize the environment\n",
    "            use_joint_optimization (bool): Whether to use joint optimization of\n",
    "                                          goal inference and assistance determination\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.visualize = visualize\n",
    "        self.use_joint_optimization = use_joint_optimization\n",
    "        \n",
    "        # State observation includes:\n",
    "        # [dot_x, dot_y, h_dir_x, h_dir_y, goal_x, goal_y, w_dir_x, w_dir_y, \n",
    "        #  dist_ratio, obs_dist_ratio, entropy, goal_probs (8)]\n",
    "        # Space dimensions will be determined when goals are created\n",
    "        \n",
    "        # Action space: blending parameter gamma ∈ [-1, 1] (mapped to [0, 1])\n",
    "        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(1,), dtype=np.float32)\n",
    "        \n",
    "        # Initialize environment state\n",
    "        self.dot_pos = None\n",
    "        self.goal_pos = None\n",
    "        self.obstacles = []\n",
    "        self.goals = []\n",
    "        self.true_goal_idx = None\n",
    "        self.step_count = 0\n",
    "        self.max_steps = 300\n",
    "        self.episode_reward = 0.0\n",
    "        self.max_dist = math.hypot(FULL_VIEW_SIZE[0], FULL_VIEW_SIZE[1])\n",
    "        \n",
    "        # Hyperparameters for human simulation\n",
    "        self.alpha = 3.0  # Directness of human control\n",
    "        self.beta = 3.0   # Precision of human control\n",
    "        \n",
    "        # Thresholds for context-based reward\n",
    "        self.goal_threshold = 100.0 * SCALING_FACTOR\n",
    "        self.obs_threshold = 100.0 * SCALING_FACTOR\n",
    "        \n",
    "        # Curriculum learning settings\n",
    "        self.SCENARIO_SEEDS = [0, 1, 2, 58, 487]\n",
    "        self.scenario_index = 0\n",
    "        self.episode_counter = 0\n",
    "        self.random_seed_probability = 0.3\n",
    "        \n",
    "        # Initialize Bayesian model for goal inference\n",
    "        self.bayesian_model = BayesianGoalInference(beta=10.0, w_theta=0.7, w_d=0.3)\n",
    "        \n",
    "        # Set up visualization\n",
    "        if self.visualize and pygame is not None:\n",
    "            pygame.init()\n",
    "            self.window = pygame.display.set_mode(FULL_VIEW_SIZE)\n",
    "            pygame.display.set_caption(\"Context-Adaptive Shared Autonomy\")\n",
    "            self.clock = pygame.time.Clock()\n",
    "        else:\n",
    "            self.window = None\n",
    "            self.clock = None\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"\n",
    "        Reset the environment.\n",
    "        \n",
    "        Args:\n",
    "            seed (int, optional): Random seed\n",
    "            options (dict, optional): Additional options\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (observation, info)\n",
    "        \"\"\"\n",
    "        # Reset the base environment\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        # Update episode counter\n",
    "        self.episode_counter += 1\n",
    "        \n",
    "        # Choose a scenario seed\n",
    "        use_random = (random.random() < self.random_seed_probability)\n",
    "        if use_random:\n",
    "            scenario_seed = random.randint(0, 9999999)\n",
    "        else:\n",
    "            scenario_seed = self.SCENARIO_SEEDS[self.scenario_index]\n",
    "            self.scenario_index = (self.scenario_index + 1) % len(self.SCENARIO_SEEDS)\n",
    "        \n",
    "        # Create the environment layout\n",
    "        self.randomize_env(scenario_seed)\n",
    "        \n",
    "        # Reset internal state\n",
    "        self.step_count = 0\n",
    "        self.episode_reward = 0.0\n",
    "        self.dot_pos = START_POS.copy()\n",
    "        \n",
    "        # Choose a random goal from the available targets\n",
    "        if self.goals:\n",
    "            self.true_goal_idx = random.randint(0, len(self.goals) - 1)\n",
    "            self.goal_pos = self.goals[self.true_goal_idx].copy()\n",
    "        else:\n",
    "            self.goal_pos = np.array([random.uniform(0.2*FULL_VIEW_SIZE[0], 0.8*FULL_VIEW_SIZE[0]),\n",
    "                                     random.uniform(0.2*FULL_VIEW_SIZE[1], 0.8*FULL_VIEW_SIZE[1])],\n",
    "                                    dtype=np.float32)\n",
    "            self.true_goal_idx = 0\n",
    "        \n",
    "        # Reset Bayesian goal inference model\n",
    "        self.bayesian_model.initialize_goals(self.goals)\n",
    "        self.bayesian_model.reset()\n",
    "        \n",
    "        # Set up observation space based on number of goals\n",
    "        n_goals = len(self.goals)\n",
    "        low = np.array(\n",
    "            [0, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0] + [0] * n_goals,\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        high = np.array(\n",
    "            [FULL_VIEW_SIZE[0], FULL_VIEW_SIZE[1], 1, 1,\n",
    "             FULL_VIEW_SIZE[0], FULL_VIEW_SIZE[1], 1, 1, 1, 1, np.log(n_goals)] + [1] * n_goals,\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        self.observation_space = spaces.Box(low=low, high=high, shape=(11 + n_goals,), dtype=np.float32)\n",
    "        \n",
    "        return self._get_obs(), {}\n",
    "    \n",
    "    def randomize_env(self, scenario_seed):\n",
    "        \"\"\"\n",
    "        Create a randomized environment layout.\n",
    "        \n",
    "        Args:\n",
    "            scenario_seed (int): Random seed for environment generation\n",
    "        \"\"\"\n",
    "        random.seed(scenario_seed)\n",
    "        np.random.seed(scenario_seed)\n",
    "        \n",
    "        margin = 50 * SCALING_FACTOR\n",
    "        N_GOALS = 8\n",
    "        N_OBSTACLES = 5\n",
    "        min_goal_distance = 300 * SCALING_FACTOR\n",
    "        \n",
    "        # Generate goals\n",
    "        new_goals = []\n",
    "        attempts = 0\n",
    "        while len(new_goals) < N_GOALS and attempts < 2000:\n",
    "            x = random.uniform(margin, FULL_VIEW_SIZE[0] - margin)\n",
    "            y = random.uniform(margin, FULL_VIEW_SIZE[1] - margin)\n",
    "            candidate = np.array([x, y], dtype=np.float32)\n",
    "            \n",
    "            # Ensure goal is far enough from start position\n",
    "            if distance(candidate, START_POS) >= min_goal_distance:\n",
    "                new_goals.append(candidate)\n",
    "            attempts += 1\n",
    "        \n",
    "        self.goals = new_goals[:N_GOALS]\n",
    "        \n",
    "        # Generate obstacles\n",
    "        new_obstacles = []\n",
    "        if len(self.goals) > 1:\n",
    "            obstacle_goals = random.sample(self.goals, k=min(min(N_GOALS-1, N_OBSTACLES), len(self.goals)-1))\n",
    "        else:\n",
    "            obstacle_goals = self.goals.copy()\n",
    "        \n",
    "        for goal in obstacle_goals:\n",
    "            # Place obstacle between start and goal\n",
    "            t = random.uniform(0.6, 0.8)\n",
    "            base_point = START_POS + t*(goal - START_POS)\n",
    "            \n",
    "            # Create perpendicular offset\n",
    "            vec = goal - START_POS\n",
    "            vec_norm = np.linalg.norm(vec)\n",
    "            if vec_norm < 1e-6:\n",
    "                perp = np.array([0, 0], dtype=np.float32)\n",
    "            else:\n",
    "                perp = np.array([-vec[1], vec[0]], dtype=np.float32)\n",
    "                perp /= np.linalg.norm(perp)\n",
    "            \n",
    "            # Apply random offset\n",
    "            offset_mag = random.uniform(20*SCALING_FACTOR, 40*SCALING_FACTOR)\n",
    "            offset = perp * offset_mag * random.choice([-1, 1])\n",
    "            candidate = base_point + offset\n",
    "            \n",
    "            # Keep within bounds\n",
    "            candidate[0] = np.clip(candidate[0], margin, FULL_VIEW_SIZE[0] - margin)\n",
    "            candidate[1] = np.clip(candidate[1], margin, FULL_VIEW_SIZE[1] - margin)\n",
    "            \n",
    "            # Check validity\n",
    "            valid = True\n",
    "            if distance(candidate, START_POS) < (DOT_RADIUS + OBSTACLE_RADIUS + 10):\n",
    "                valid = False\n",
    "            if distance(candidate, goal) < (TARGET_RADIUS + OBSTACLE_RADIUS + 20):\n",
    "                valid = False\n",
    "            for obs in new_obstacles:\n",
    "                if distance(candidate, obs) < (2*OBSTACLE_RADIUS + 10):\n",
    "                    valid = False\n",
    "            \n",
    "            if valid:\n",
    "                new_obstacles.append(candidate)\n",
    "        \n",
    "        self.obstacles = new_obstacles\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Execute one environment step.\n",
    "        \n",
    "        Args:\n",
    "            action (numpy.ndarray): Action to take (gamma value)\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (observation, reward, done, truncated, info)\n",
    "        \"\"\"\n",
    "        # Map the action from [-1,1] to gamma in [0,1]\n",
    "        raw_a = float(action[0])\n",
    "        gamma_val = 0.5 * (raw_a + 1.0)\n",
    "        \n",
    "        self.step_count += 1\n",
    "        \n",
    "        # Compute the world (expert) direction using the potential field\n",
    "        w_dir = potential_field_dir(self.dot_pos, self.goal_pos, self.obstacles)\n",
    "        \n",
    "        # Add noise for the human direction (simulating imperfect human input)\n",
    "        noise = np.random.normal(0, NOISE_MAGNITUDE, size=2)\n",
    "        h_dir = w_dir + noise\n",
    "        hm = np.hypot(h_dir[0], h_dir[1])\n",
    "        if hm > 1e-6:\n",
    "            h_dir /= hm\n",
    "        \n",
    "        # Update Bayesian model with latest human input\n",
    "        goal_probs = self.bayesian_model.update(self.dot_pos, h_dir, self.obstacles)\n",
    "        \n",
    "        # Get expert recommendation from Bayesian model\n",
    "        if self.use_joint_optimization:\n",
    "            # For joint optimization, use the weighted expert recommendation\n",
    "            w_dir = self.bayesian_model.compute_expert_recommendation(self.dot_pos, self.obstacles)\n",
    "        # else we use the potential_field_dir directly (already computed above)\n",
    "        \n",
    "        # Combine the directions using gamma as the blending parameter\n",
    "        c_dir = gamma_val * w_dir + (1 - gamma_val) * h_dir\n",
    "        cm = np.hypot(c_dir[0], c_dir[1])\n",
    "        if cm > 1e-6:\n",
    "            c_dir /= cm\n",
    "        \n",
    "        # Compute the new position\n",
    "        move_vec = c_dir * MAX_SPEED\n",
    "        new_pos = self.dot_pos + move_vec\n",
    "        \n",
    "        # Update the dot position if no collision occurs along the move\n",
    "        if not line_collision(self.dot_pos, new_pos, self.obstacles):\n",
    "            new_pos[0] = np.clip(new_pos[0], 0, FULL_VIEW_SIZE[0])\n",
    "            new_pos[1] = np.clip(new_pos[1], 0, FULL_VIEW_SIZE[1])\n",
    "            self.dot_pos = new_pos\n",
    "        \n",
    "        # Check if the dot is inside an obstacle\n",
    "        collided = inside_obstacle(self.dot_pos, self.obstacles)\n",
    "        info = {}\n",
    "        \n",
    "        if collided:\n",
    "            reward = -2.0\n",
    "            done = True\n",
    "            info[\"terminal_reason\"] = \"collision\"\n",
    "        else:\n",
    "            # Check if reached the goal\n",
    "            if distance(self.dot_pos, self.goal_pos) <= (TARGET_RADIUS + DOT_RADIUS):\n",
    "                reward = 5.0\n",
    "                done = True\n",
    "                info[\"terminal_reason\"] = \"goal_reached\"\n",
    "            else:\n",
    "                reward = 0.0\n",
    "                done = False\n",
    "                info[\"terminal_reason\"] = None\n",
    "        \n",
    "        # Check for timeout\n",
    "        truncated = (self.step_count >= self.max_steps)\n",
    "        if truncated and not done:\n",
    "            info[\"terminal_reason\"] = \"timeout\"\n",
    "        \n",
    "        # ------------------ Reward Shaping ------------------\n",
    "        # Compute distance to goal\n",
    "        d_goal = distance(self.dot_pos, self.goal_pos)\n",
    "        \n",
    "        # Compute distance to closest obstacle\n",
    "        if self.obstacles:\n",
    "            d_obs = min(distance(self.dot_pos, obs) for obs in self.obstacles)\n",
    "        else:\n",
    "            d_obs = 999999.0\n",
    "        \n",
    "        # Progress reward\n",
    "        prev_dist = distance(self.dot_pos - move_vec, self.goal_pos)\n",
    "        progress_reward = (prev_dist - d_goal) * 0.1\n",
    "        \n",
    "        # Get goal probability for the true goal\n",
    "        if len(goal_probs) > self.true_goal_idx:\n",
    "            p_true = goal_probs[self.true_goal_idx]\n",
    "        else:\n",
    "            p_true = 0.0\n",
    "        \n",
    "        # Get maximum probability\n",
    "        p_max = np.max(goal_probs) if len(goal_probs) > 0 else 0.0\n",
    "        \n",
    "        # Determine optimal gamma based on context\n",
    "        # Compute the contextual reward based on the environment situation\n",
    "        if d_goal < self.goal_threshold and d_obs < self.obs_threshold:\n",
    "            # Near both goal and obstacles: higher gamma for safety\n",
    "            desired_gamma = 0.6\n",
    "        elif d_goal < self.goal_threshold:\n",
    "            # Near goal: lower gamma (more human control for fine adjustment)\n",
    "            desired_gamma = 0.3\n",
    "        elif d_obs < self.obs_threshold:\n",
    "            # Near obstacle: higher gamma (more AI control for safety)\n",
    "            desired_gamma = 0.8\n",
    "        else:\n",
    "            # Open space: gamma depends on goal certainty\n",
    "            # Higher uncertainty = lower gamma (more human control)\n",
    "            goal_entropy = entropy(goal_probs) if len(goal_probs) > 0 else np.log(len(self.goals))\n",
    "            max_entropy = np.log(len(self.goals))\n",
    "            uncertainty = goal_entropy / max_entropy if max_entropy > 0 else 0.5\n",
    "            desired_gamma = 0.3 + 0.5 * (1 - uncertainty)  # Maps [0,1] uncertainty to [0.8,0.3] gamma\n",
    "        \n",
    "        # Compute reward components based on manuscript\n",
    "        # -c_collision · 𝟙_collision: penalty for collisions (already handled above)\n",
    "        # +c_near · γ · p_max · 𝟙_near: reward for being near the predicted target with appropriate assistance\n",
    "        # -c_far · γ · 𝟙_far: penalty for high assistance when far from all targets\n",
    "        # +c_progress · p_max · (d_{t-1} - d_t): reward for making progress toward the goal\n",
    "        # -c_γ · γ^2: quadratic penalty to minimize intervention\n",
    "        # +c_goal · log(p_true): reward for correctly identifying the true goal\n",
    "        \n",
    "        c_near = 0.2\n",
    "        c_far = 0.1\n",
    "        c_progress = 0.5\n",
    "        c_gamma = 0.5\n",
    "        c_goal = 0.3\n",
    "        \n",
    "        near_flag = d_goal < self.goal_threshold\n",
    "        far_flag = d_goal > 2 * self.goal_threshold\n",
    "        \n",
    "        near_reward = c_near * gamma_val * p_max * float(near_flag)\n",
    "        far_penalty = c_far * gamma_val * float(far_flag)\n",
    "        progress_term = c_progress * p_max * progress_reward\n",
    "        gamma_penalty = c_gamma * (gamma_val - desired_gamma) ** 2\n",
    "        goal_reward = c_goal * np.log(p_true + 1e-6)  # Add small epsilon to avoid log(0)\n",
    "        \n",
    "        # Combine all reward components\n",
    "        shaped_reward = near_reward - far_penalty + progress_term - gamma_penalty + goal_reward\n",
    "        \n",
    "        # Total reward\n",
    "        reward += shaped_reward\n",
    "        \n",
    "        self.episode_reward += reward\n",
    "        # ------------------ End Reward Shaping ------------------\n",
    "        \n",
    "        # Get observation\n",
    "        obs = self._get_obs()\n",
    "        \n",
    "        return obs, float(reward), done, truncated, info\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        \"\"\"\n",
    "        Get the current observation.\n",
    "        \n",
    "        Returns:\n",
    "            numpy.ndarray: Current observation\n",
    "        \"\"\"\n",
    "        # Basic state information\n",
    "        to_g = self.goal_pos - self.dot_pos\n",
    "        d = math.hypot(to_g[0], to_g[1])\n",
    "        dist_ratio = d / self.max_dist if self.max_dist > 1e-6 else 0.0\n",
    "        \n",
    "        # Compute directions\n",
    "        w_dir = potential_field_dir(self.dot_pos, self.goal_pos, self.obstacles)\n",
    "        noise = np.random.normal(0, NOISE_MAGNITUDE, size=2)\n",
    "        h_dir = w_dir + noise\n",
    "        hm = np.hypot(h_dir[0], h_dir[1])\n",
    "        if hm > 1e-6:\n",
    "            h_dir /= hm\n",
    "        \n",
    "        # Distance to closest obstacle\n",
    "        if self.obstacles:\n",
    "            min_obs_distance = min(distance(self.dot_pos, obs) for obs in self.obstacles)\n",
    "        else:\n",
    "            min_obs_distance = self.max_dist\n",
    "        obs_dist_ratio = min_obs_distance / self.max_dist\n",
    "        \n",
    "        # Get goal probabilities and entropy\n",
    "        goal_probs = self.bayesian_model.get_goal_probs()\n",
    "        goal_ent = entropy(goal_probs) if len(goal_probs) > 0 else np.log(len(self.goals))\n",
    "        \n",
    "        # Combine all features\n",
    "        base_obs = np.array([\n",
    "            self.dot_pos[0], self.dot_pos[1],\n",
    "            h_dir[0], h_dir[1],\n",
    "            self.goal_pos[0], self.goal_pos[1],\n",
    "            w_dir[0], w_dir[1],\n",
    "            dist_ratio, obs_dist_ratio,\n",
    "            goal_ent\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        # Append goal probabilities\n",
    "        obs = np.concatenate([base_obs, goal_probs]).astype(np.float32)\n",
    "        \n",
    "        return obs\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"Render the environment.\"\"\"\n",
    "        if not self.visualize or (pygame is None):\n",
    "            return\n",
    "        \n",
    "        # Handle user events (e.g., window close)\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                return\n",
    "        \n",
    "        # Clear the screen\n",
    "        self.window.fill(WHITE)\n",
    "        \n",
    "        # Draw obstacles\n",
    "        for obs in self.obstacles:\n",
    "            pygame.draw.circle(self.window, GRAY, (int(obs[0]), int(obs[1])), OBSTACLE_RADIUS)\n",
    "        \n",
    "        # Draw all potential targets\n",
    "        for i, gpos in enumerate(self.goals):\n",
    "            # Color intensity based on goal probability\n",
    "            goal_probs = self.bayesian_model.get_goal_probs()\n",
    "            if i < len(goal_probs):\n",
    "                prob = goal_probs[i]\n",
    "                # Interpolate color from yellow to green based on probability\n",
    "                color = (int(255 * (1 - prob)), int(255), 0)\n",
    "            else:\n",
    "                color = YELLOW\n",
    "            \n",
    "            # Draw the target\n",
    "            pygame.draw.circle(self.window, color, (int(gpos[0]), int(gpos[1])), TARGET_RADIUS)\n",
    "            \n",
    "            # Label the target\n",
    "            font = pygame.font.SysFont(None, 24)\n",
    "            label = font.render(str(i), True, BLACK)\n",
    "            self.window.blit(label, (int(gpos[0]) - 5, int(gpos[1]) - 8))\n",
    "        \n",
    "        # Draw the true goal with a black outline\n",
    "        pygame.draw.circle(self.window, BLACK, \n",
    "                          (int(self.goal_pos[0]), int(self.goal_pos[1])), \n",
    "                          TARGET_RADIUS+2, width=2)\n",
    "        \n",
    "        # Draw the agent\n",
    "        pygame.draw.circle(self.window, BLACK, \n",
    "                          (int(self.dot_pos[0]), int(self.dot_pos[1])), \n",
    "                          DOT_RADIUS, width=2)\n",
    "        \n",
    "        # Display gamma value\n",
    "        if hasattr(self, 'last_gamma'):\n",
    "            font = pygame.font.SysFont(None, 30)\n",
    "            gamma_text = font.render(f\"γ: {self.last_gamma:.2f}\", True, BLACK)\n",
    "            self.window.blit(gamma_text, (10, 10))\n",
    "        \n",
    "        # Update the display\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(RENDER_FPS)\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close the environment.\"\"\"\n",
    "        if self.visualize and pygame is not None:\n",
    "            pygame.quit()\n",
    "        super().close()\n",
    "\n",
    "###############################################################################\n",
    "# CURRICULUM CALLBACK\n",
    "###############################################################################\n",
    "class CurriculumCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for curriculum learning during training.\n",
    "    \n",
    "    Progressively increases the difficulty of the environment:\n",
    "    1. Basic goal-directed behavior\n",
    "    2. Basic collision avoidance\n",
    "    3. Challenging obstacle configurations\n",
    "    4. Goal ambiguity with multiple potential targets\n",
    "    5. Full complexity\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.stage = 1\n",
    "        self.success_count = 0\n",
    "        self.required_successes = 100\n",
    "        self.max_stage = 5\n",
    "        self.stage_rewards = []\n",
    "        self.stage_lengths = []\n",
    "        self.current_ep_reward = 0\n",
    "        self.stage_start_time = time.time()\n",
    "    \n",
    "    def _on_training_start(self):\n",
    "        \"\"\"Set initial curriculum stage.\"\"\"\n",
    "        print(f\"Starting curriculum stage {self.stage}: Basic goal-directed behavior\")\n",
    "        self.stage_start_time = time.time()\n",
    "    \n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"Check if curriculum stage should be updated.\"\"\"\n",
    "        # Get the environment\n",
    "        env = self.model.env.envs[0]\n",
    "        \n",
    "        # Track reward\n",
    "        rewards = self.locals.get('rewards', [0])\n",
    "        self.current_ep_reward += rewards[0]\n",
    "        \n",
    "        # Check episode termination\n",
    "        dones = self.locals.get('dones', [False])\n",
    "        infos = self.locals.get('infos', [{}])\n",
    "        \n",
    "        if dones[0]:\n",
    "            self.stage_rewards.append(self.current_ep_reward)\n",
    "            self.stage_lengths.append(self.locals.get('n_steps', 0))\n",
    "            self.current_ep_reward = 0\n",
    "            \n",
    "            # Check success (reached goal without collision)\n",
    "            info = infos[0]\n",
    "            terminal_reason = info.get('terminal_reason', None)\n",
    "            \n",
    "            if terminal_reason == 'goal_reached':\n",
    "                self.success_count += 1\n",
    "            else:\n",
    "                # Reset success counter if failure occurs\n",
    "                self.success_count = max(0, self.success_count - 1)\n",
    "            \n",
    "            # Check for stage progression\n",
    "            if self.success_count >= self.required_successes:\n",
    "                self._progress_curriculum()\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def _progress_curriculum(self):\n",
    "        \"\"\"Progress to the next curriculum stage.\"\"\"\n",
    "        elapsed_time = time.time() - self.stage_start_time\n",
    "        avg_reward = np.mean(self.stage_rewards[-100:]) if self.stage_rewards else 0\n",
    "        \n",
    "        print(f\"\\nCompleted curriculum stage {self.stage}:\")\n",
    "        print(f\"  - Time taken: {elapsed_time:.2f} seconds\")\n",
    "        print(f\"  - Average reward (last 100 episodes): {avg_reward:.2f}\")\n",
    "        \n",
    "        self.stage += 1\n",
    "        if self.stage > self.max_stage:\n",
    "            print(\"\\nCurriculum completed! Using full complexity environment.\")\n",
    "            return\n",
    "        \n",
    "        # Reset counters\n",
    "        self.success_count = 0\n",
    "        self.stage_rewards = []\n",
    "        self.stage_lengths = []\n",
    "        self.stage_start_time = time.time()\n",
    "        \n",
    "        # Update environment parameters based on curriculum stage\n",
    "        env = self.model.env.envs[0]\n",
    "        \n",
    "        if self.stage == 2:\n",
    "            print(\"\\nProgressing to stage 2: Basic collision avoidance\")\n",
    "            # Increase obstacle count but keep goal ambiguity low\n",
    "            env.random_seed_probability = 0.4\n",
    "        \n",
    "        elif self.stage == 3:\n",
    "            print(\"\\nProgressing to stage 3: Challenging obstacle configurations\")\n",
    "            # More complex obstacle patterns\n",
    "            env.random_seed_probability = 0.6\n",
    "        \n",
    "        elif self.stage == 4:\n",
    "            print(\"\\nProgressing to stage 4: Goal ambiguity with multiple targets\")\n",
    "            # Introduce more goal ambiguity\n",
    "            env.random_seed_probability = 0.8\n",
    "        \n",
    "        elif self.stage == 5:\n",
    "            print(\"\\nProgressing to stage 5: Full complexity\")\n",
    "            # Full complexity - random scenarios\n",
    "            env.random_seed_probability = 1.0\n",
    "    \n",
    "    def on_training_end(self):\n",
    "        \"\"\"Log final curriculum statistics.\"\"\"\n",
    "        print(\"\\nCurriculum learning completed.\")\n",
    "        print(f\"Final stage reached: {self.stage}/{self.max_stage}\")\n",
    "\n",
    "###############################################################################\n",
    "# RENDER CALLBACK FOR LIVE VIEW\n",
    "###############################################################################\n",
    "class RenderCallback(BaseCallback):\n",
    "    \"\"\"Simple callback to render the environment during training.\"\"\"\n",
    "    def __init__(self, render_freq=1, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.render_freq = render_freq\n",
    "    \n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"Render the environment periodically.\"\"\"\n",
    "        if self.n_calls % self.render_freq == 0:\n",
    "            self.model.env.envs[0].render()\n",
    "        return True\n",
    "\n",
    "###############################################################################\n",
    "# BAYESIAN MODEL TRAINING FUNCTION\n",
    "###############################################################################\n",
    "def train_bayesian_model(dataset_path=None, epochs=5, visualize=False):\n",
    "    \"\"\"\n",
    "    Pre-train the Bayesian goal inference model on a dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset_path (str): Path to trajectory dataset\n",
    "        epochs (int): Number of epochs for training\n",
    "        visualize (bool): Whether to visualize the training process\n",
    "        \n",
    "    Returns:\n",
    "        BayesianGoalInference: Trained model\n",
    "    \"\"\"\n",
    "    print(\"Training Bayesian goal inference model...\")\n",
    "    \n",
    "    # Create a model with default parameters\n",
    "    model = BayesianGoalInference(beta=10.0, w_theta=0.7, w_d=0.3)\n",
    "    \n",
    "    # If no dataset is provided, create a synthetic one\n",
    "    if dataset_path is None:\n",
    "        print(\"No dataset provided. Creating synthetic data...\")\n",
    "        \n",
    "        # Create a synthetic dataset\n",
    "        n_trajectories = 100\n",
    "        trajectories = []\n",
    "        env = SharedAutonomyEnv(visualize=False)\n",
    "        \n",
    "        for i in range(n_trajectories):\n",
    "            print(f\"Generating trajectory {i+1}/{n_trajectories}\", end=\"\\r\")\n",
    "            \n",
    "            # Reset environment\n",
    "            obs, _ = env.reset()\n",
    "            \n",
    "            # Extract relevant information\n",
    "            goal_pos = env.goal_pos\n",
    "            true_goal_idx = env.true_goal_idx\n",
    "            goals = env.goals\n",
    "            \n",
    "            # Generate a trajectory\n",
    "            trajectory = {\n",
    "                \"positions\": [env.dot_pos.copy()],\n",
    "                \"inputs\": [],\n",
    "                \"goal_pos\": goal_pos.copy(),\n",
    "                \"true_goal_idx\": true_goal_idx,\n",
    "                \"goals\": [g.copy() for g in goals],\n",
    "            }\n",
    "            \n",
    "            # Simple policy to generate human-like trajectories\n",
    "            done = False\n",
    "            while not done:\n",
    "                # Compute direction to goal with noise\n",
    "                to_goal = goal_pos - env.dot_pos\n",
    "                dist = np.linalg.norm(to_goal)\n",
    "                if dist > 0:\n",
    "                    direction = to_goal / dist\n",
    "                else:\n",
    "                    direction = np.zeros(2)\n",
    "                \n",
    "                # Add noise to direction\n",
    "                noise_scale = 0.2\n",
    "                noise = np.random.normal(0, noise_scale, size=2)\n",
    "                noisy_direction = direction + noise\n",
    "                \n",
    "                # Normalize\n",
    "                norm = np.linalg.norm(noisy_direction)\n",
    "                if norm > 0:\n",
    "                    noisy_direction = noisy_direction / norm\n",
    "                \n",
    "                # Store human input\n",
    "                trajectory[\"inputs\"].append(noisy_direction.copy())\n",
    "                \n",
    "                # Step environment with full human control (gamma=0)\n",
    "                obs, _, done, _, _ = env.step([-1.0])  # -1.0 maps to gamma=0\n",
    "                \n",
    "                # Store position\n",
    "                trajectory[\"positions\"].append(env.dot_pos.copy())\n",
    "                \n",
    "                if len(trajectory[\"positions\"]) > 300:  # Prevent infinite loops\n",
    "                    break\n",
    "            \n",
    "            trajectories.append(trajectory)\n",
    "        \n",
    "        env.close()\n",
    "        print(\"\\nGenerated synthetic dataset with\", len(trajectories), \"trajectories\")\n",
    "        \n",
    "        # Save dataset\n",
    "        os.makedirs(\"datasets\", exist_ok=True)\n",
    "        with open(\"datasets/synthetic_trajectories.pkl\", \"wb\") as f:\n",
    "            pickle.dump(trajectories, f)\n",
    "        print(\"Saved synthetic dataset to datasets/synthetic_trajectories.pkl\")\n",
    "        \n",
    "        dataset = trajectories\n",
    "    else:\n",
    "        # Load real dataset\n",
    "        try:\n",
    "            with open(dataset_path, \"rb\") as f:\n",
    "                dataset = pickle.load(f)\n",
    "            print(f\"Loaded dataset from {dataset_path} with {len(dataset)} trajectories\")\n",
    "        except:\n",
    "            raise ValueError(f\"Could not load dataset from {dataset_path}\")\n",
    "    \n",
    "    # Perform grid search for hyperparameter optimization\n",
    "    print(\"\\nPerforming hyperparameter grid search...\")\n",
    "    best_params = None\n",
    "    best_accuracy = 0.0\n",
    "    \n",
    "    # Define parameter search space\n",
    "    beta_values = [5.0, 10.0, 15.0]\n",
    "    w_theta_values = [0.5, 0.7, 0.9]\n",
    "    decay_values = [0.8, 0.85, 0.9]\n",
    "    \n",
    "    for beta in beta_values:\n",
    "        for w_theta in w_theta_values:\n",
    "            for decay in decay_values:\n",
    "                w_d = 1.0 - w_theta  # Ensure weights sum to 1\n",
    "                \n",
    "                # Set model parameters\n",
    "                model.beta = beta\n",
    "                model.w_theta = w_theta\n",
    "                model.w_d = w_d\n",
    "                model.decay_rate = decay\n",
    "                \n",
    "                # Evaluate on validation set\n",
    "                total_acc = 0.0\n",
    "                n_valid = min(30, len(dataset))  # Use a subset for validation\n",
    "                \n",
    "                for i in range(n_valid):\n",
    "                    traj = dataset[i]\n",
    "                    goals = traj[\"goals\"]\n",
    "                    true_idx = traj[\"true_goal_idx\"]\n",
    "                    \n",
    "                    # Initialize model with trajectory goals\n",
    "                    model.initialize_goals(goals)\n",
    "                    \n",
    "                    # Process trajectory\n",
    "                    positions = traj[\"positions\"]\n",
    "                    inputs = traj[\"inputs\"]\n",
    "                    n_steps = min(len(inputs), len(positions) - 1)\n",
    "                    n_correct = 0\n",
    "                    \n",
    "                    for t in range(n_steps):\n",
    "                        pos = positions[t]\n",
    "                        h_input = inputs[t]\n",
    "                        \n",
    "                        # Update model\n",
    "                        goal_probs = model.update(pos, h_input)\n",
    "                        \n",
    "                        # Check if prediction is correct\n",
    "                        if len(goal_probs) > 0:\n",
    "                            pred_idx = np.argmax(goal_probs)\n",
    "                            if pred_idx == true_idx:\n",
    "                                n_correct += 1\n",
    "                    \n",
    "                    # Calculate accuracy for this trajectory\n",
    "                    accuracy = n_correct / n_steps if n_steps > 0 else 0\n",
    "                    total_acc += accuracy\n",
    "                \n",
    "                # Average accuracy across trajectories\n",
    "                avg_acc = total_acc / n_valid\n",
    "                \n",
    "                print(f\"  beta={beta}, w_theta={w_theta}, decay={decay}: accuracy={avg_acc:.3f}\")\n",
    "                \n",
    "                # Update best parameters\n",
    "                if avg_acc > best_accuracy:\n",
    "                    best_accuracy = avg_acc\n",
    "                    best_params = (beta, w_theta, 1.0-w_theta, decay)\n",
    "    \n",
    "    if best_params:\n",
    "        print(f\"\\nBest parameters: beta={best_params[0]}, w_theta={best_params[1]}, w_d={best_params[2]}, decay={best_params[3]}\")\n",
    "        print(f\"Best accuracy: {best_accuracy:.3f}\")\n",
    "        \n",
    "        # Set model to best parameters\n",
    "        model.beta = best_params[0]\n",
    "        model.w_theta = best_params[1]\n",
    "        model.w_d = best_params[2]\n",
    "        model.decay_rate = best_params[3]\n",
    "    \n",
    "    # Train confidence calibration\n",
    "    print(\"\\nTraining confidence calibration...\")\n",
    "    confidences = []\n",
    "    accuracies = []\n",
    "    \n",
    "    n_calib = min(50, len(dataset))\n",
    "    for i in range(n_calib):\n",
    "        traj = dataset[i]\n",
    "        goals = traj[\"goals\"]\n",
    "        true_idx = traj[\"true_goal_idx\"]\n",
    "        \n",
    "        # Initialize model with trajectory goals\n",
    "        model.initialize_goals(goals)\n",
    "        \n",
    "        # Process trajectory\n",
    "        positions = traj[\"positions\"]\n",
    "        inputs = traj[\"inputs\"]\n",
    "        n_steps = min(len(inputs), len(positions) - 1)\n",
    "        \n",
    "        for t in range(n_steps):\n",
    "            pos = positions[t]\n",
    "            h_input = inputs[t]\n",
    "            \n",
    "            # Update model\n",
    "            goal_probs = model.update(pos, h_input)\n",
    "            \n",
    "            # Record confidence and accuracy\n",
    "            if len(goal_probs) > 0:\n",
    "                max_prob = np.max(goal_probs)\n",
    "                pred_idx = np.argmax(goal_probs)\n",
    "                is_correct = int(pred_idx == true_idx)\n",
    "                \n",
    "                confidences.append(max_prob)\n",
    "                accuracies.append(is_correct)\n",
    "    \n",
    "    # Train calibrator\n",
    "    model.train_calibrator(confidences, accuracies)\n",
    "    \n",
    "    # Save the trained model\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    with open(\"models/bayesian_model.pkl\", \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(\"Saved trained Bayesian model to models/bayesian_model.pkl\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"\\nEvaluating model on test set...\")\n",
    "    eval_results = {}\n",
    "    completion_points = [0.25, 0.5, 0.75, 1.0]\n",
    "    for cp in completion_points:\n",
    "        eval_results[cp] = []\n",
    "    \n",
    "    n_test = min(100, len(dataset))\n",
    "    for i in range(n_test):\n",
    "        traj = dataset[i]\n",
    "        goals = traj[\"goals\"]\n",
    "        true_idx = traj[\"true_goal_idx\"]\n",
    "        \n",
    "        # Initialize model with trajectory goals\n",
    "        model.initialize_goals(goals)\n",
    "        \n",
    "        # Process trajectory\n",
    "        positions = traj[\"positions\"]\n",
    "        inputs = traj[\"inputs\"]\n",
    "        n_steps = min(len(inputs), len(positions) - 1)\n",
    "        \n",
    "        for cp in completion_points:\n",
    "            step_idx = int(cp * n_steps) - 1\n",
    "            if step_idx < 0:\n",
    "                continue\n",
    "            \n",
    "            # Reset model\n",
    "            model.reset()\n",
    "            \n",
    "            # Process up to completion point\n",
    "            for t in range(step_idx + 1):\n",
    "                pos = positions[t]\n",
    "                h_input = inputs[t]\n",
    "                goal_probs = model.update(pos, h_input)\n",
    "            \n",
    "            # Check prediction\n",
    "            if len(goal_probs) > 0:\n",
    "                pred_idx = np.argmax(goal_probs)\n",
    "                is_correct = int(pred_idx == true_idx)\n",
    "                eval_results[cp].append(is_correct)\n",
    "    \n",
    "    # Print evaluation results\n",
    "    print(\"\\nModel performance at different path completion percentages:\")\n",
    "    for cp in completion_points:\n",
    "        if eval_results[cp]:\n",
    "            accuracy = np.mean(eval_results[cp])\n",
    "            print(f\"  {int(cp*100)}% completion: {accuracy:.3f} accuracy\")\n",
    "    \n",
    "    if visualize:\n",
    "        # Visualize model performance on a random trajectory\n",
    "        print(\"\\nVisualizing model performance...\")\n",
    "        \n",
    "        # Create a visualization environment\n",
    "        vis_env = SharedAutonomyEnv(visualize=True)\n",
    "        \n",
    "        for _ in range(3):  # Show 3 trajectories\n",
    "            # Reset environment\n",
    "            obs, _ = vis_env.reset()\n",
    "            \n",
    "            done = False\n",
    "            while not done:\n",
    "                # Get noisy human input\n",
    "                to_goal = vis_env.goal_pos - vis_env.dot_pos\n",
    "                dist = np.linalg.norm(to_goal)\n",
    "                if dist > 0:\n",
    "                    direction = to_goal / dist\n",
    "                else:\n",
    "                    direction = np.zeros(2)\n",
    "                \n",
    "                # Add noise\n",
    "                noise_scale = 0.2\n",
    "                noise = np.random.normal(0, noise_scale, size=2)\n",
    "                h_input = direction + noise\n",
    "                \n",
    "                # Normalize\n",
    "                norm = np.linalg.norm(h_input)\n",
    "                if norm > 0:\n",
    "                    h_input = h_input / norm\n",
    "                \n",
    "                # Step environment with full human control\n",
    "                obs, _, done, _, _ = vis_env.step([-1.0])  # -1.0 maps to gamma=0\n",
    "                \n",
    "                # Visualize\n",
    "                vis_env.render()\n",
    "                time.sleep(0.1)\n",
    "        \n",
    "        vis_env.close()\n",
    "    \n",
    "    return model\n",
    "\n",
    "###############################################################################\n",
    "# TRAINING FUNCTION\n",
    "###############################################################################\n",
    "def train_model(total_timesteps=500_000, visualize=False, use_joint_optimization=True,\n",
    "               bayesian_model_path=None, load_pretrained=False):\n",
    "    \"\"\"\n",
    "    Train the shared autonomy model.\n",
    "    \n",
    "    Args:\n",
    "        total_timesteps (int): Total number of timesteps for training\n",
    "        visualize (bool): Whether to visualize training\n",
    "        use_joint_optimization (bool): Whether to use joint optimization\n",
    "        bayesian_model_path (str): Path to pre-trained Bayesian model\n",
    "        load_pretrained (bool): Whether to load a pre-trained model\n",
    "        \n",
    "    Returns:\n",
    "        stable_baselines3.PPO: Trained model\n",
    "    \"\"\"\n",
    "    print(\"Initializing shared autonomy environment...\")\n",
    "    env = SharedAutonomyEnv(visualize=visualize, use_joint_optimization=use_joint_optimization)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    \n",
    "    # Set up callbacks\n",
    "    metrics_callback = EnhancedMetricsCallback()\n",
    "    curriculum_callback = CurriculumCallback()\n",
    "    callbacks = [metrics_callback, curriculum_callback]\n",
    "    \n",
    "    if visualize:\n",
    "        render_callback = RenderCallback(render_freq=1)\n",
    "        callbacks.append(render_callback)\n",
    "    \n",
    "    # Load pre-trained Bayesian model if provided\n",
    "    if bayesian_model_path:\n",
    "        try:\n",
    "            with open(bayesian_model_path, \"rb\") as f:\n",
    "                bayesian_model = pickle.load(f)\n",
    "            print(f\"Loaded Bayesian model from {bayesian_model_path}\")\n",
    "            \n",
    "            # Set the model in the environment\n",
    "            env.envs[0].bayesian_model = bayesian_model\n",
    "        except:\n",
    "            print(f\"Could not load Bayesian model from {bayesian_model_path}\")\n",
    "    \n",
    "    print(\"Initializing PPO model with dual-head architecture...\")\n",
    "    \n",
    "    if load_pretrained:\n",
    "        # Load pre-trained model\n",
    "        try:\n",
    "            model = PPO.load(\"models/shared_autonomy_model\")\n",
    "            model.set_env(env)\n",
    "            print(\"Loaded pre-trained model from models/shared_autonomy_model\")\n",
    "        except:\n",
    "            print(\"Could not load pre-trained model. Training from scratch.\")\n",
    "            model = create_new_model(env)\n",
    "    else:\n",
    "        model = create_new_model(env)\n",
    "    \n",
    "    print(f\"Starting training for {total_timesteps} timesteps...\")\n",
    "    model.learn(total_timesteps=total_timesteps, callback=CallbackList(callbacks), log_interval=1)\n",
    "    \n",
    "    # Save the trained model\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    model_path = os.path.join(\"models\", \"shared_autonomy_model\")\n",
    "    model.save(model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "    \n",
    "    # Save metrics\n",
    "    metrics_callback.save_metrics(save_dir=\"training_metrics\")\n",
    "    print(\"Metrics saved to 'training_metrics/'\")\n",
    "    \n",
    "    env.close()\n",
    "    return model\n",
    "\n",
    "def create_new_model(env):\n",
    "    \"\"\"Create a new PPO model with the appropriate architecture.\"\"\"\n",
    "    # Create PPO model with custom network architecture\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\",  # We'll use the default policy structure\n",
    "        env,\n",
    "        learning_rate=3e-4,\n",
    "        n_steps=1024,\n",
    "        batch_size=256,\n",
    "        n_epochs=5,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        clip_range=0.2,\n",
    "        clip_range_vf=0.2,\n",
    "        normalize_advantage=True,\n",
    "        ent_coef=0.01,\n",
    "        verbose=1,\n",
    "        tensorboard_log=\"./tensorboard_logs/\",\n",
    "        policy_kwargs={\n",
    "            \"net_arch\": [{\"pi\": [256, 256], \"vf\": [256, 256]}],\n",
    "            \"activation_fn\": nn.ReLU,\n",
    "            \"ortho_init\": True\n",
    "        }\n",
    "    )\n",
    "    return model\n",
    "\n",
    "###############################################################################\n",
    "# EVALUATION FUNCTIONS\n",
    "###############################################################################\n",
    "def evaluate_model(model_path=\"models/shared_autonomy_model\", n_episodes=50, visualize=True):\n",
    "    \"\"\"\n",
    "    Evaluate a trained model.\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path to the trained model\n",
    "        n_episodes (int): Number of episodes for evaluation\n",
    "        visualize (bool): Whether to visualize evaluation\n",
    "        \n",
    "    Returns:\n",
    "        dict: Evaluation metrics\n",
    "    \"\"\"\n",
    "    print(f\"Evaluating model {model_path}...\")\n",
    "    \n",
    "    # Create evaluation environment\n",
    "    env = SharedAutonomyEnv(visualize=visualize)\n",
    "    \n",
    "    try:\n",
    "        # Load the model\n",
    "        model = PPO.load(model_path)\n",
    "        print(f\"Loaded model from {model_path}\")\n",
    "    except:\n",
    "        print(f\"Could not load model from {model_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Initialize metrics\n",
    "    metrics = {\n",
    "        \"success_rate\": 0.0,\n",
    "        \"collision_rate\": 0.0,\n",
    "        \"timeout_rate\": 0.0,\n",
    "        \"avg_reward\": 0.0,\n",
    "        \"avg_steps\": 0.0,\n",
    "        \"avg_gamma\": 0.0,\n",
    "        \"goal_inference_accuracy\": 0.0,\n",
    "        \"path_efficiency\": 0.0,\n",
    "        \"completion_accuracy\": {0.25: 0.0, 0.5: 0.0, 0.75: 0.0}\n",
    "    }\n",
    "    \n",
    "    # Run evaluation episodes\n",
    "    for episode in range(n_episodes):\n",
    "        print(f\"Episode {episode+1}/{n_episodes}\", end=\"\\r\")\n",
    "        \n",
    "        # Reset environment\n",
    "        obs, _ = env.reset()\n",
    "        \n",
    "        # Episode tracking\n",
    "        done = False\n",
    "        truncated = False\n",
    "        episode_reward = 0.0\n",
    "        episode_steps = 0\n",
    "        episode_gammas = []\n",
    "        trajectory = []\n",
    "        goal_predictions = []\n",
    "        \n",
    "        # Track initial position and goal for path efficiency\n",
    "        start_pos = env.dot_pos.copy()\n",
    "        goal_pos = env.goal_pos.copy()\n",
    "        \n",
    "        # Track path completion\n",
    "        completion_recorded = {0.25: False, 0.5: False, 0.75: False}\n",
    "        completion_correct = {0.25: False, 0.5: False, 0.75: False}\n",
    "        \n",
    "        while not (done or truncated):\n",
    "            # Predict action\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            \n",
    "            # Map action to gamma\n",
    "            gamma = 0.5 * (action[0] + 1.0)\n",
    "            episode_gammas.append(gamma)\n",
    "            \n",
    "            # Save for rendering\n",
    "            env.last_gamma = gamma\n",
    "            \n",
    "            # Step environment\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "            \n",
    "            # Update tracking\n",
    "            episode_reward += reward\n",
    "            episode_steps += 1\n",
    "            trajectory.append(env.dot_pos.copy())\n",
    "            \n",
    "            # Track goal inference\n",
    "            if hasattr(env.bayesian_model, \"get_goal_probs\"):\n",
    "                probs = env.bayesian_model.get_goal_probs()\n",
    "                if len(probs) > 0:\n",
    "                    pred_goal = np.argmax(probs)\n",
    "                    goal_predictions.append(pred_goal)\n",
    "                    \n",
    "                    # Check path completion accuracy\n",
    "                    if len(trajectory) > 1:\n",
    "                        # Calculate path completion\n",
    "                        total_dist = distance(start_pos, goal_pos)\n",
    "                        current_dist = distance(env.dot_pos, goal_pos)\n",
    "                        completion = 1.0 - (current_dist / total_dist) if total_dist > 0 else 0.0\n",
    "                        \n",
    "                        # Check completion thresholds\n",
    "                        for threshold in [0.25, 0.5, 0.75]:\n",
    "                            if completion >= threshold and not completion_recorded[threshold]:\n",
    "                                completion_recorded[threshold] = True\n",
    "                                is_correct = (pred_goal == env.true_goal_idx)\n",
    "                                completion_correct[threshold] = is_correct\n",
    "            \n",
    "            # Visualize\n",
    "            if visualize:\n",
    "                env.render()\n",
    "                time.sleep(0.01)\n",
    "        \n",
    "        # Update metrics\n",
    "        terminal_reason = info.get(\"terminal_reason\", None)\n",
    "        \n",
    "        if terminal_reason == \"goal_reached\":\n",
    "            metrics[\"success_rate\"] += 1.0\n",
    "        elif terminal_reason == \"collision\":\n",
    "            metrics[\"collision_rate\"] += 1.0\n",
    "        elif terminal_reason == \"timeout\":\n",
    "            metrics[\"timeout_rate\"] += 1.0\n",
    "        \n",
    "        metrics[\"avg_reward\"] += episode_reward\n",
    "        metrics[\"avg_steps\"] += episode_steps\n",
    "        \n",
    "        if episode_gammas:\n",
    "            metrics[\"avg_gamma\"] += np.mean(episode_gammas)\n",
    "        \n",
    "        # Calculate path efficiency (ratio of direct distance to actual path length)\n",
    "        direct_distance = distance(start_pos, goal_pos)\n",
    "        path_length = 0.0\n",
    "        for i in range(1, len(trajectory)):\n",
    "            path_length += distance(trajectory[i-1], trajectory[i])\n",
    "        \n",
    "        if path_length > 0:\n",
    "            path_efficiency = direct_distance / path_length\n",
    "            metrics[\"path_efficiency\"] += path_efficiency\n",
    "        \n",
    "        # Goal inference accuracy\n",
    "        if goal_predictions and hasattr(env, \"true_goal_idx\"):\n",
    "            accuracy = np.mean([1.0 if p == env.true_goal_idx else 0.0 for p in goal_predictions])\n",
    "            metrics[\"goal_inference_accuracy\"] += accuracy\n",
    "        \n",
    "        # Path completion accuracy\n",
    "        for threshold in [0.25, 0.5, 0.75]:\n",
    "            if completion_recorded[threshold]:\n",
    "                metrics[\"completion_accuracy\"][threshold] += float(completion_correct[threshold])\n",
    "    \n",
    "    # Normalize metrics\n",
    "    metrics[\"success_rate\"] /= n_episodes\n",
    "    metrics[\"collision_rate\"] /= n_episodes\n",
    "    metrics[\"timeout_rate\"] /= n_episodes\n",
    "    metrics[\"avg_reward\"] /= n_episodes\n",
    "    metrics[\"avg_steps\"] /= n_episodes\n",
    "    metrics[\"avg_gamma\"] /= n_episodes\n",
    "    metrics[\"goal_inference_accuracy\"] /= n_episodes\n",
    "    metrics[\"path_efficiency\"] /= n_episodes\n",
    "    \n",
    "    for threshold in [0.25, 0.5, 0.75]:\n",
    "        recorded_count = sum(1 for e in range(n_episodes) if completion_recorded[threshold])\n",
    "        if recorded_count > 0:\n",
    "            metrics[\"completion_accuracy\"][threshold] /= recorded_count\n",
    "    \n",
    "    # Print evaluation results\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(f\"  Success Rate: {metrics['success_rate']:.3f}\")\n",
    "    print(f\"  Collision Rate: {metrics['collision_rate']:.3f}\")\n",
    "    print(f\"  Timeout Rate: {metrics['timeout_rate']:.3f}\")\n",
    "    print(f\"  Average Reward: {metrics['avg_reward']:.3f}\")\n",
    "    print(f\"  Average Steps: {metrics['avg_steps']:.1f}\")\n",
    "    print(f\"  Average Gamma: {metrics['avg_gamma']:.3f}\")\n",
    "    print(f\"  Goal Inference Accuracy: {metrics['goal_inference_accuracy']:.3f}\")\n",
    "    print(f\"  Path Efficiency: {metrics['path_efficiency']:.3f}\")\n",
    "    print(\"  Goal Accuracy at Path Completion:\")\n",
    "    for threshold in [0.25, 0.5, 0.75]:\n",
    "        print(f\"    {int(threshold*100)}%: {metrics['completion_accuracy'][threshold]:.3f}\")\n",
    "    \n",
    "    # Save metrics to file\n",
    "    os.makedirs(\"evaluation_results\", exist_ok=True)\n",
    "    with open(\"evaluation_results/metrics.json\", \"w\") as f:\n",
    "        import json\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    print(\"Saved evaluation metrics to evaluation_results/metrics.json\")\n",
    "    \n",
    "    env.close()\n",
    "    return metrics\n",
    "\n",
    "def compare_models(baseline_path=\"models/baseline_model\", \n",
    "                  adaptive_path=\"models/shared_autonomy_model\",\n",
    "                  n_episodes=30, visualize=True):\n",
    "    \"\"\"\n",
    "    Compare baseline and adaptive models.\n",
    "    \n",
    "    Args:\n",
    "        baseline_path (str): Path to the baseline model\n",
    "        adaptive_path (str): Path to the adaptive model\n",
    "        n_episodes (int): Number of episodes for evaluation\n",
    "        visualize (bool): Whether to visualize comparison\n",
    "    \"\"\"\n",
    "    print(\"Comparing models...\")\n",
    "    \n",
    "    # Evaluate baseline model\n",
    "    print(\"\\nEvaluating baseline model:\")\n",
    "    baseline_metrics = evaluate_model(baseline_path, n_episodes, visualize)\n",
    "    \n",
    "    # Evaluate adaptive model\n",
    "    print(\"\\nEvaluating adaptive model:\")\n",
    "    adaptive_metrics = evaluate_model(adaptive_path, n_episodes, visualize)\n",
    "    \n",
    "    if baseline_metrics is None or adaptive_metrics is None:\n",
    "        print(\"Could not compare models.\")\n",
    "        return\n",
    "    \n",
    "    # Calculate improvements\n",
    "    improvements = {}\n",
    "    for key in baseline_metrics:\n",
    "        if key == \"completion_accuracy\":\n",
    "            improvements[key] = {}\n",
    "            for threshold in baseline_metrics[key]:\n",
    "                baseline_val = baseline_metrics[key][threshold]\n",
    "                adaptive_val = adaptive_metrics[key][threshold]\n",
    "                if baseline_val > 0:\n",
    "                    rel_improvement = (adaptive_val - baseline_val) / baseline_val\n",
    "                    improvements[key][threshold] = rel_improvement\n",
    "        else:\n",
    "            baseline_val = baseline_metrics[key]\n",
    "            adaptive_val = adaptive_metrics[key]\n",
    "            if baseline_val > 0:\n",
    "                rel_improvement = (adaptive_val - baseline_val) / baseline_val\n",
    "                improvements[key] = rel_improvement\n",
    "    \n",
    "    # Print comparison\n",
    "    print(\"\\nModel Comparison (Relative Improvement):\")\n",
    "    for key in improvements:\n",
    "        if key == \"completion_accuracy\":\n",
    "            print(\"  Goal Accuracy at Path Completion:\")\n",
    "            for threshold in improvements[key]:\n",
    "                print(f\"    {int(threshold*100)}%: {improvements[key][threshold]:.2%}\")\n",
    "        else:\n",
    "            # For metrics where higher is better\n",
    "            if key in [\"success_rate\", \"avg_reward\", \"goal_inference_accuracy\", \"path_efficiency\"]:\n",
    "                print(f\"  {key}: {improvements[key]:.2%}\")\n",
    "            # For metrics where lower is better (show negative improvement as positive)\n",
    "            elif key in [\"collision_rate\", \"timeout_rate\"]:\n",
    "                print(f\"  {key}: {-improvements[key]:.2%}\")\n",
    "            else:\n",
    "                print(f\"  {key}: {improvements[key]:.2%}\")\n",
    "    \n",
    "    # Plot comparison\n",
    "    if visualize:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Metrics to compare (excluding completion_accuracy)\n",
    "        metrics = [\"success_rate\", \"collision_rate\", \"goal_inference_accuracy\", \"path_efficiency\"]\n",
    "        \n",
    "        x = np.arange(len(metrics))\n",
    "        width = 0.35\n",
    "        \n",
    "        baseline_vals = [baseline_metrics[m] for m in metrics]\n",
    "        adaptive_vals = [adaptive_metrics[m] for m in metrics]\n",
    "        \n",
    "        plt.bar(x - width/2, baseline_vals, width, label='Baseline')\n",
    "        plt.bar(x + width/2, adaptive_vals, width, label='Adaptive')\n",
    "        \n",
    "        plt.xlabel('Metrics')\n",
    "        plt.ylabel('Values')\n",
    "        plt.title('Comparison of Baseline vs. Adaptive Models')\n",
    "        plt.xticks(x, metrics)\n",
    "        plt.legend()\n",
    "        plt.grid(True, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        os.makedirs(\"evaluation_results\", exist_ok=True)\n",
    "        plt.savefig(\"evaluation_results/model_comparison.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Plot completion accuracy\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        \n",
    "        thresholds = [0.25, 0.5, 0.75]\n",
    "        x = np.arange(len(thresholds))\n",
    "        width = 0.35\n",
    "        \n",
    "        baseline_vals = [baseline_metrics[\"completion_accuracy\"][t] for t in thresholds]\n",
    "        adaptive_vals = [adaptive_metrics[\"completion_accuracy\"][t] for t in thresholds]\n",
    "        \n",
    "        plt.bar(x - width/2, baseline_vals, width, label='Baseline')\n",
    "        plt.bar(x + width/2, adaptive_vals, width, label='Adaptive')\n",
    "        \n",
    "        plt.xlabel('Path Completion')\n",
    "        plt.ylabel('Goal Inference Accuracy')\n",
    "        plt.title('Goal Inference Accuracy at Different Path Completions')\n",
    "        plt.xticks(x, [f\"{int(t*100)}%\" for t in thresholds])\n",
    "        plt.ylim(0, 1.1)\n",
    "        plt.legend()\n",
    "        plt.grid(True, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"evaluation_results/completion_accuracy_comparison.png\")\n",
    "        plt.close()\n",
    "    \n",
    "    print(\"Saved comparison plots to evaluation_results/\")\n",
    "\n",
    "###############################################################################\n",
    "# SEQUENTIAL BASELINES EVALUATION\n",
    "###############################################################################\n",
    "def evaluate_sequential_approach():\n",
    "    \"\"\"\n",
    "    Evaluate a sequential approach (separate goal inference and control).\n",
    "    This implements the sequential baseline described in the manuscript\n",
    "    for validating the joint optimization theorem.\n",
    "    \"\"\"\n",
    "    print(\"Evaluating sequential approach...\")\n",
    "    \n",
    "    # Create environment\n",
    "    env = SharedAutonomyEnv(visualize=False, use_joint_optimization=False)\n",
    "    \n",
    "    # Train sequential approach\n",
    "    # Step 1: Use pre-trained Bayesian model\n",
    "    bayesian_model = None\n",
    "    if os.path.exists(\"models/bayesian_model.pkl\"):\n",
    "        try:\n",
    "            with open(\"models/bayesian_model.pkl\", \"rb\") as f:\n",
    "                bayesian_model = pickle.load(f)\n",
    "            print(\"Loaded pre-trained Bayesian model\")\n",
    "        except:\n",
    "            print(\"Could not load pre-trained Bayesian model\")\n",
    "    \n",
    "    if bayesian_model is None:\n",
    "        # Train Bayesian model\n",
    "        bayesian_model = train_bayesian_model(epochs=3)\n",
    "    \n",
    "    # Step 2: Train PPO with fixed Bayesian model\n",
    "    env.bayesian_model = bayesian_model\n",
    "    \n",
    "    # Create sequential PPO model\n",
    "    sequential_model = PPO(\n",
    "        \"MlpPolicy\",\n",
    "        DummyVecEnv([lambda: env]),\n",
    "        learning_rate=3e-4,\n",
    "        n_steps=1024,\n",
    "        batch_size=256,\n",
    "        n_epochs=5,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        clip_range=0.2,\n",
    "        verbose=1,\n",
    "        tensorboard_log=\"./tensorboard_logs/sequential/\",\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    print(\"Training sequential PPO model...\")\n",
    "    sequential_model.learn(total_timesteps=300000)\n",
    "    \n",
    "    # Save model\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    sequential_model.save(\"models/sequential_model\")\n",
    "    print(\"Saved sequential model to models/sequential_model\")\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics = evaluate_model(\"models/sequential_model\", n_episodes=30, visualize=True)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "###############################################################################\n",
    "# ABLATION STUDIES\n",
    "###############################################################################\n",
    "def run_ablation_studies(n_episodes=20):\n",
    "    \"\"\"\n",
    "    Run ablation studies to evaluate each component's contribution.\n",
    "    \n",
    "    This evaluates:\n",
    "    1. MAP Selection: Using only the most likely goal\n",
    "    2. Fixed Threshold: Using fixed blending parameter thresholds\n",
    "    3. Independent Networks: Separate networks for goal inference and control\n",
    "    4. Partial Info: Limited goal information for the policy\n",
    "    5. Joint Optimization (Full model)\n",
    "    \"\"\"\n",
    "    print(\"Running ablation studies...\")\n",
    "    \n",
    "    # Define ablation conditions\n",
    "    ablations = [\n",
    "        (\"MAP Selection\", \"models/map_selection_model\"),\n",
    "        (\"Fixed Threshold\", \"models/fixed_threshold_model\"),\n",
    "        (\"Independent Networks\", \"models/independent_networks_model\"),\n",
    "        (\"Partial Info\", \"models/partial_info_model\"),\n",
    "        (\"Joint Optimization\", \"models/shared_autonomy_model\")\n",
    "    ]\n",
    "    \n",
    "    # Track metrics\n",
    "    ablation_metrics = {}\n",
    "    \n",
    "    for name, model_path in ablations:\n",
    "        print(f\"\\nEvaluating {name}:\")\n",
    "        \n",
    "        # Train model if it doesn't exist (simplified here)\n",
    "        if not os.path.exists(f\"{model_path}.zip\") and name != \"Joint Optimization\":\n",
    "            print(f\"Training {name} model...\")\n",
    "            \n",
    "            # Create appropriate environment based on ablation\n",
    "            if name == \"MAP Selection\":\n",
    "                # Modified environment that only uses the most likely goal\n",
    "                env = SharedAutonomyEnv(visualize=False)\n",
    "                env.use_joint_optimization = False\n",
    "                \n",
    "                # Override compute_expert_recommendation to use only MAP\n",
    "                original_recommend = env.bayesian_model.compute_expert_recommendation\n",
    "                def map_recommendation(self, agent_pos, obstacles):\n",
    "                    most_likely_goal, _ = self.get_most_likely_goal()\n",
    "                    if most_likely_goal is None:\n",
    "                        return np.zeros(2, dtype=np.float32)\n",
    "                    return potential_field_dir(agent_pos, most_likely_goal, obstacles)\n",
    "                \n",
    "                env.bayesian_model.compute_expert_recommendation = map_recommendation.__get__(\n",
    "                    env.bayesian_model, type(env.bayesian_model))\n",
    "                \n",
    "            elif name == \"Fixed Threshold\":\n",
    "                # Modified environment that uses fixed gamma thresholds\n",
    "                env = SharedAutonomyEnv(visualize=False)\n",
    "                \n",
    "                # Override step method to use fixed thresholds\n",
    "                original_step = env.step\n",
    "                def fixed_threshold_step(self, action):\n",
    "                    # Get goal probability\n",
    "                    goal_probs = self.bayesian_model.get_goal_probs()\n",
    "                    max_prob = np.max(goal_probs) if len(goal_probs) > 0 else 0.0\n",
    "                    \n",
    "                    # Simple fixed threshold policy:\n",
    "                    # High certainty (>0.8): gamma = 0.8\n",
    "                    # Medium certainty (0.5-0.8): gamma = 0.5\n",
    "                    # Low certainty (<0.5): gamma = 0.2\n",
    "                    if max_prob > 0.8:\n",
    "                        gamma = 0.8\n",
    "                    elif max_prob > 0.5:\n",
    "                        gamma = 0.5\n",
    "                    else:\n",
    "                        gamma = 0.2\n",
    "                    \n",
    "                    # Convert to action space\n",
    "                    action_fixed = np.array([2.0 * gamma - 1.0])\n",
    "                    \n",
    "                    # Call original step with fixed action\n",
    "                    return original_step(self, action_fixed)\n",
    "                \n",
    "                env.step = fixed_threshold_step.__get__(env, type(env))\n",
    "                \n",
    "            elif name == \"Independent Networks\":\n",
    "                # Use standard environment but with separate networks\n",
    "                env = SharedAutonomyEnv(visualize=False)\n",
    "                env.use_joint_optimization = False\n",
    "                \n",
    "            elif name == \"Partial Info\":\n",
    "                # Modified environment with limited goal information\n",
    "                env = SharedAutonomyEnv(visualize=False)\n",
    "                \n",
    "                # Override _get_obs to provide only maximum goal probability\n",
    "                original_get_obs = env._get_obs\n",
    "                def limited_obs(self):\n",
    "                    # Get full observation\n",
    "                    full_obs = original_get_obs(self)\n",
    "                    \n",
    "                    # Extract basic features (first 11 elements)\n",
    "                    basic_features = full_obs[:11]\n",
    "                    \n",
    "                    # Replace goal probabilities with just the maximum probability\n",
    "                    goal_probs = self.bayesian_model.get_goal_probs()\n",
    "                    max_prob = np.max(goal_probs) if len(goal_probs) > 0 else 0.0\n",
    "                    \n",
    "                    # Return limited observation\n",
    "                    return np.append(basic_features, [max_prob])\n",
    "                \n",
    "                env._get_obs = limited_obs.__get__(env, type(env))\n",
    "                \n",
    "                # Update observation space\n",
    "                env.observation_space = spaces.Box(\n",
    "                    low=np.array([0, 0, -1, -1, 0, 0, -1, -1, 0, 0, 0, 0], dtype=np.float32),\n",
    "                    high=np.array([FULL_VIEW_SIZE[0], FULL_VIEW_SIZE[1], 1, 1, \n",
    "                                  FULL_VIEW_SIZE[0], FULL_VIEW_SIZE[1], 1, 1, \n",
    "                                  1, 1, np.log(8), 1], dtype=np.float32),\n",
    "                    shape=(12,), dtype=np.float32\n",
    "                )\n",
    "            \n",
    "            # Create model\n",
    "            model = PPO(\n",
    "                \"MlpPolicy\",\n",
    "                DummyVecEnv([lambda: env]),\n",
    "                learning_rate=3e-4,\n",
    "                n_steps=1024,\n",
    "                batch_size=256,\n",
    "                n_epochs=5,\n",
    "                gamma=0.99,\n",
    "                gae_lambda=0.95,\n",
    "                clip_range=0.2,\n",
    "                verbose=1,\n",
    "                tensorboard_log=f\"./tensorboard_logs/{name.lower().replace(' ', '_')}/\",\n",
    "            )\n",
    "            \n",
    "            # Train model (with simplified settings for ablation)\n",
    "            model.learn(total_timesteps=200000)\n",
    "            \n",
    "            # Save model\n",
    "            model.save(model_path)\n",
    "            print(f\"Saved {name} model to {model_path}\")\n",
    "        \n",
    "        # Evaluate model\n",
    "        metrics = evaluate_model(model_path, n_episodes=n_episodes, visualize=True)\n",
    "        \n",
    "        if metrics is not None:\n",
    "            ablation_metrics[name] = metrics\n",
    "    \n",
    "    # Compare ablation results\n",
    "    print(\"\\nAblation Study Results:\")\n",
    "    \n",
    "    # Success Rate\n",
    "    print(\"\\nSuccess Rate:\")\n",
    "    for name in ablation_metrics:\n",
    "        print(f\"  {name}: {ablation_metrics[name]['success_rate']:.3f}\")\n",
    "    \n",
    "    # Goal Inference Accuracy\n",
    "    print(\"\\nGoal Inference Accuracy:\")\n",
    "    for name in ablation_metrics:\n",
    "        print(f\"  {name}: {ablation_metrics[name]['goal_inference_accuracy']:.3f}\")\n",
    "    \n",
    "    # Path Efficiency\n",
    "    print(\"\\nPath Efficiency:\")\n",
    "    for name in ablation_metrics:\n",
    "        print(f\"  {name}: {ablation_metrics[name]['path_efficiency']:.3f}\")\n",
    "    \n",
    "    # Create comparison plot\n",
    "    if len(ablation_metrics) > 1:\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        \n",
    "        metrics_to_plot = [\"success_rate\", \"collision_rate\", \"goal_inference_accuracy\", \"path_efficiency\"]\n",
    "        x = np.arange(len(metrics_to_plot))\n",
    "        width = 0.8 / len(ablation_metrics)\n",
    "        \n",
    "        for i, name in enumerate(ablation_metrics):\n",
    "            values = [ablation_metrics[name][m] for m in metrics_to_plot]\n",
    "            plt.bar(x + i*width - 0.4 + width/2, values, width, label=name)\n",
    "        \n",
    "        plt.xlabel('Metrics')\n",
    "        plt.ylabel('Values')\n",
    "        plt.title('Ablation Study Results')\n",
    "        plt.xticks(x, metrics_to_plot)\n",
    "        plt.legend()\n",
    "        plt.grid(True, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        os.makedirs(\"evaluation_results\", exist_ok=True)\n",
    "        plt.savefig(\"evaluation_results/ablation_study.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        print(\"Saved ablation study plot to evaluation_results/ablation_study.png\")\n",
    "    \n",
    "    return ablation_metrics\n",
    "\n",
    "###############################################################################\n",
    "# GENERATE GAMMA HEATMAP\n",
    "###############################################################################\n",
    "def generate_gamma_heatmap(model_path=\"models/shared_autonomy_model\", resolution=20):\n",
    "    \"\"\"\n",
    "    Generate a heatmap showing gamma values across the environment space.\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path to the trained model\n",
    "        resolution (int): Resolution of the heatmap grid\n",
    "    \"\"\"\n",
    "    print(\"Generating gamma heatmap...\")\n",
    "    \n",
    "    # Create environment\n",
    "    env = SharedAutonomyEnv(visualize=False)\n",
    "    \n",
    "    try:\n",
    "        # Load model\n",
    "        model = PPO.load(model_path)\n",
    "        print(f\"Loaded model from {model_path}\")\n",
    "    except:\n",
    "        print(f\"Could not load model from {model_path}\")\n",
    "        return\n",
    "    \n",
    "    # Create grid\n",
    "    x_grid = np.linspace(0, FULL_VIEW_SIZE[0], resolution)\n",
    "    y_grid = np.linspace(0, FULL_VIEW_SIZE[1], resolution)\n",
    "    gamma_values = np.zeros((resolution, resolution))\n",
    "    \n",
    "    # Reset environment to get obstacles and goals\n",
    "    obs, _ = env.reset()\n",
    "    \n",
    "    # Cache some data\n",
    "    goal_pos = env.goal_pos\n",
    "    obstacles = env.obstacles\n",
    "    original_pos = env.dot_pos.copy()\n",
    "    \n",
    "    # Compute gamma values at each grid point\n",
    "    for i, x in enumerate(x_grid):\n",
    "        for j, y in enumerate(y_grid):\n",
    "            # Temporarily set agent position\n",
    "            env.dot_pos = np.array([x, y], dtype=np.float32)\n",
    "            \n",
    "            # Get observation at this position\n",
    "            obs = env._get_obs()\n",
    "            \n",
    "            # Predict action\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            \n",
    "            # Map action to gamma\n",
    "            gamma = 0.5 * (action[0] + 1.0)\n",
    "            \n",
    "            # Store gamma value\n",
    "            gamma_values[j, i] = gamma  # Note: j, i for correct orientation\n",
    "    \n",
    "    # Restore original position\n",
    "    env.dot_pos = original_pos\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(gamma_values, origin='lower', extent=[0, FULL_VIEW_SIZE[0], 0, FULL_VIEW_SIZE[1]], \n",
    "              cmap='viridis', vmin=0, vmax=1)\n",
    "    plt.colorbar(label='Gamma')\n",
    "    \n",
    "    # Plot obstacles and goals\n",
    "    for obs_pos in obstacles:\n",
    "        circle = plt.Circle((obs_pos[0], obs_pos[1]), OBSTACLE_RADIUS, color='gray')\n",
    "        plt.gcf().gca().add_artist(circle)\n",
    "    \n",
    "    for i, g_pos in enumerate(env.goals):\n",
    "        circle = plt.Circle((g_pos[0], g_pos[1]), TARGET_RADIUS, color='yellow')\n",
    "        plt.gcf().gca().add_artist(circle)\n",
    "    \n",
    "    # Mark true goal\n",
    "    circle = plt.Circle((goal_pos[0], goal_pos[1]), TARGET_RADIUS+2, color='black', fill=False)\n",
    "    plt.gcf().gca().add_artist(circle)\n",
    "    \n",
    "    plt.title('Gamma Values Across Environment')\n",
    "    plt.xlabel('X Position')\n",
    "    plt.ylabel('Y Position')\n",
    "    plt.grid(False)\n",
    "    \n",
    "    # Save heatmap\n",
    "    os.makedirs(\"evaluation_results\", exist_ok=True)\n",
    "    plt.savefig(\"evaluation_results/gamma_heatmap.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"Saved gamma heatmap to evaluation_results/gamma_heatmap.png\")\n",
    "\n",
    "###############################################################################\n",
    "# APPLY TO ROBOTIC ARM DOMAIN\n",
    "###############################################################################\n",
    "def apply_to_robotic_arm():\n",
    "    \"\"\"\n",
    "    Demonstrate domain transfer to a simulated robotic arm environment.\n",
    "    This is a simplified demonstration of the transfer described in the paper.\n",
    "    \"\"\"\n",
    "    print(\"This function would implement the robotic arm domain transfer.\")\n",
    "    print(\"Currently a placeholder for the concept described in the manuscript.\")\n",
    "    \n",
    "    # This would require additional robotics libraries and a more complex environment\n",
    "    # The key elements would be:\n",
    "    # 1. Create a robotic arm environment (e.g., using PyBullet, MuJoCo, or Isaac Gym)\n",
    "    # 2. Adapt the observation space to include joint angles, end-effector positions, etc.\n",
    "    # 3. Modify the reward function for robotic manipulation tasks\n",
    "    # 4. Adapt the Bayesian goal inference for 3D space\n",
    "    # 5. Re-train or fine-tune the model for the new domain\n",
    "    \n",
    "    print(\"Refer to the manuscript for details on how this transfer would be implemented.\")\n",
    "\n",
    "###############################################################################\n",
    "# SEQUENTIAL EXPERIMENT RUNNER - USE THIS IN JUPYTER NOTEBOOKS\n",
    "###############################################################################\n",
    "def run_full_experiment(visualize=False, timesteps=300000):\n",
    "    \"\"\"\n",
    "    Run the complete experiment sequence:\n",
    "    1. Train Bayesian model\n",
    "    2. Train full adaptive shared autonomy model \n",
    "    3. Train baseline (sequential) model\n",
    "    4. Run ablation studies\n",
    "    5. Generate evaluation visualizations\n",
    "    \n",
    "    Args:\n",
    "        visualize (bool): Whether to visualize during training/evaluation\n",
    "        timesteps (int): Number of timesteps for training\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"CONTEXT-ADAPTIVE SHARED AUTONOMY EXPERIMENT\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Step 1: Train Bayesian model\n",
    "    print(\"\\n[1/5] Training Bayesian goal inference model...\")\n",
    "    bayesian_model = train_bayesian_model(epochs=3, visualize=visualize)\n",
    "    \n",
    "    # Step 2: Train adaptive model with joint optimization\n",
    "    print(\"\\n[2/5] Training adaptive shared autonomy model...\")\n",
    "    adaptive_model = train_model(\n",
    "        total_timesteps=timesteps, \n",
    "        visualize=visualize,\n",
    "        use_joint_optimization=True,\n",
    "        bayesian_model_path=\"models/bayesian_model.pkl\"\n",
    "    )\n",
    "    \n",
    "    # Step 3: Train baseline model (sequential approach)\n",
    "    print(\"\\n[3/5] Training baseline sequential model...\")\n",
    "    baseline_model = evaluate_sequential_approach()\n",
    "    \n",
    "    # Step 4: Run ablation studies\n",
    "    print(\"\\n[4/5] Running ablation studies...\")\n",
    "    ablation_results = run_ablation_studies(n_episodes=20)\n",
    "    \n",
    "    # Step 5: Generate visualizations\n",
    "    print(\"\\n[5/5] Generating comparative visualizations...\")\n",
    "    # Compare models\n",
    "    compare_models(\n",
    "        baseline_path=\"models/sequential_model\",\n",
    "        adaptive_path=\"models/shared_autonomy_model\",\n",
    "        n_episodes=30,\n",
    "        visualize=True\n",
    "    )\n",
    "    \n",
    "    # Generate gamma heatmap\n",
    "    generate_gamma_heatmap(\"models/shared_autonomy_model\", resolution=20)\n",
    "    \n",
    "    print(\"\\nExperiment complete! Results are saved in:\")\n",
    "    print(\"- training_metrics/ (training curves)\")\n",
    "    print(\"- models/ (trained models)\")\n",
    "    print(\"- evaluation_results/ (comparisons and heatmaps)\")\n",
    "    \n",
    "    return {\n",
    "        \"bayesian_model\": bayesian_model,\n",
    "        \"adaptive_model\": adaptive_model,\n",
    "        \"baseline_model\": baseline_model,\n",
    "        \"ablation_results\": ablation_results\n",
    "    }\n",
    "\n",
    "###############################################################################\n",
    "# MAIN - DON'T USE THIS IN JUPYTER NOTEBOOKS (use run_full_experiment instead)\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "# MAIN - WORKS IN BOTH SCRIPT AND JUPYTER NOTEBOOK ENVIRONMENTS\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    # Check if running in Jupyter\n",
    "    try:\n",
    "        # This will only exist in IPython/Jupyter environments\n",
    "        get_ipython\n",
    "        in_jupyter = True\n",
    "    except NameError:\n",
    "        in_jupyter = False\n",
    "    \n",
    "    if in_jupyter:\n",
    "        print(\"Running in Jupyter environment\")\n",
    "        print(\"To run the experiment, use the following in a new cell:\")\n",
    "        print(\"results = run_full_experiment(visualize=False, timesteps=50000)\")\n",
    "    else:\n",
    "        # Only use argument parser in script mode\n",
    "        import argparse\n",
    "        \n",
    "        parser = argparse.ArgumentParser(description=\"Context-Adaptive Shared Autonomy\")\n",
    "        parser.add_argument(\"--train\", action=\"store_true\", help=\"Train the model\")\n",
    "        parser.add_argument(\"--evaluate\", action=\"store_true\", help=\"Evaluate the model\")\n",
    "        parser.add_argument(\"--compare\", action=\"store_true\", help=\"Compare models\")\n",
    "        parser.add_argument(\"--ablation\", action=\"store_true\", help=\"Run ablation studies\")\n",
    "        parser.add_argument(\"--heatmap\", action=\"store_true\", help=\"Generate gamma heatmap\")\n",
    "        parser.add_argument(\"--timesteps\", type=int, default=500000, help=\"Training timesteps\")\n",
    "        parser.add_argument(\"--visualize\", action=\"store_true\", help=\"Visualize training/evaluation\")\n",
    "        parser.add_argument(\"--ui\", action=\"store_true\", help=\"Launch experiment UI\")\n",
    "        \n",
    "        args = parser.parse_args()\n",
    "        \n",
    "        # Run the full experiment\n",
    "        run_full_experiment(visualize=args.visualize, timesteps=args.timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CONTEXT-ADAPTIVE SHARED AUTONOMY EXPERIMENT\n",
      "======================================================================\n",
      "\n",
      "[1/5] Training Bayesian goal inference model...\n",
      "Training Bayesian goal inference model...\n",
      "No dataset provided. Creating synthetic data...\n",
      "Generating trajectory 100/100\n",
      "Generated synthetic dataset with 100 trajectories\n",
      "Saved synthetic dataset to datasets/synthetic_trajectories.pkl\n",
      "\n",
      "Performing hyperparameter grid search...\n",
      "  beta=5.0, w_theta=0.5, decay=0.8: accuracy=0.781\n",
      "  beta=5.0, w_theta=0.5, decay=0.85: accuracy=0.786\n",
      "  beta=5.0, w_theta=0.5, decay=0.9: accuracy=0.793\n",
      "  beta=5.0, w_theta=0.7, decay=0.8: accuracy=0.895\n",
      "  beta=5.0, w_theta=0.7, decay=0.85: accuracy=0.896\n",
      "  beta=5.0, w_theta=0.7, decay=0.9: accuracy=0.896\n",
      "  beta=5.0, w_theta=0.9, decay=0.8: accuracy=0.972\n",
      "  beta=5.0, w_theta=0.9, decay=0.85: accuracy=0.973\n",
      "  beta=5.0, w_theta=0.9, decay=0.9: accuracy=0.979\n",
      "  beta=10.0, w_theta=0.5, decay=0.8: accuracy=0.739\n",
      "  beta=10.0, w_theta=0.5, decay=0.85: accuracy=0.743\n",
      "  beta=10.0, w_theta=0.5, decay=0.9: accuracy=0.753\n",
      "  beta=10.0, w_theta=0.7, decay=0.8: accuracy=0.885\n",
      "  beta=10.0, w_theta=0.7, decay=0.85: accuracy=0.887\n",
      "  beta=10.0, w_theta=0.7, decay=0.9: accuracy=0.890\n",
      "  beta=10.0, w_theta=0.9, decay=0.8: accuracy=0.962\n",
      "  beta=10.0, w_theta=0.9, decay=0.85: accuracy=0.969\n",
      "  beta=10.0, w_theta=0.9, decay=0.9: accuracy=0.978\n",
      "  beta=15.0, w_theta=0.5, decay=0.8: accuracy=0.717\n",
      "  beta=15.0, w_theta=0.5, decay=0.85: accuracy=0.720\n",
      "  beta=15.0, w_theta=0.5, decay=0.9: accuracy=0.733\n",
      "  beta=15.0, w_theta=0.7, decay=0.8: accuracy=0.869\n",
      "  beta=15.0, w_theta=0.7, decay=0.85: accuracy=0.879\n",
      "  beta=15.0, w_theta=0.7, decay=0.9: accuracy=0.887\n",
      "  beta=15.0, w_theta=0.9, decay=0.8: accuracy=0.955\n",
      "  beta=15.0, w_theta=0.9, decay=0.85: accuracy=0.963\n",
      "  beta=15.0, w_theta=0.9, decay=0.9: accuracy=0.979\n",
      "\n",
      "Best parameters: beta=5.0, w_theta=0.9, w_d=0.09999999999999998, decay=0.9\n",
      "Best accuracy: 0.979\n",
      "\n",
      "Training confidence calibration...\n",
      "Calibrator trained and saved\n",
      "Saved trained Bayesian model to models/bayesian_model.pkl\n",
      "\n",
      "Evaluating model on test set...\n",
      "\n",
      "Model performance at different path completion percentages:\n",
      "  25% completion: 0.800 accuracy\n",
      "  50% completion: 0.800 accuracy\n",
      "  75% completion: 0.800 accuracy\n",
      "  100% completion: 0.600 accuracy\n",
      "\n",
      "[2/5] Training adaptive shared autonomy model...\n",
      "Initializing shared autonomy environment...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SharedAutonomyEnv' object has no attribute 'observation_space'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Run the complete experiment with your desired settings\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_full_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 2631\u001b[0m, in \u001b[0;36mrun_full_experiment\u001b[1;34m(visualize, timesteps)\u001b[0m\n\u001b[0;32m   2629\u001b[0m \u001b[38;5;66;03m# Step 2: Train adaptive model with joint optimization\u001b[39;00m\n\u001b[0;32m   2630\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[2/5] Training adaptive shared autonomy model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2631\u001b[0m adaptive_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2632\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m   2633\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2634\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_joint_optimization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2635\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbayesian_model_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/bayesian_model.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m   2636\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2638\u001b[0m \u001b[38;5;66;03m# Step 3: Train baseline model (sequential approach)\u001b[39;00m\n\u001b[0;32m   2639\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[3/5] Training baseline sequential model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 1862\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(total_timesteps, visualize, use_joint_optimization, bayesian_model_path, load_pretrained)\u001b[0m\n\u001b[0;32m   1860\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitializing shared autonomy environment...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1861\u001b[0m env \u001b[38;5;241m=\u001b[39m SharedAutonomyEnv(visualize\u001b[38;5;241m=\u001b[39mvisualize, use_joint_optimization\u001b[38;5;241m=\u001b[39muse_joint_optimization)\n\u001b[1;32m-> 1862\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mDummyVecEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1864\u001b[0m \u001b[38;5;66;03m# Set up callbacks\u001b[39;00m\n\u001b[0;32m   1865\u001b[0m metrics_callback \u001b[38;5;241m=\u001b[39m EnhancedMetricsCallback()\n",
      "File \u001b[1;32mc:\\Users\\mhfar\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:42\u001b[0m, in \u001b[0;36mDummyVecEnv.__init__\u001b[1;34m(self, env_fns)\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou tried to create multiple environments, but the function to create them returned the same instance \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead of creating different objects. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease read https://github.com/DLR-RM/stable-baselines3/issues/1151 for more information.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     40\u001b[0m     )\n\u001b[0;32m     41\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mlen\u001b[39m(env_fns), \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m, env\u001b[38;5;241m.\u001b[39maction_space)\n\u001b[0;32m     43\u001b[0m obs_space \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mobservation_space\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys, shapes, dtypes \u001b[38;5;241m=\u001b[39m obs_space_info(obs_space)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SharedAutonomyEnv' object has no attribute 'observation_space'"
     ]
    }
   ],
   "source": [
    "# Run the complete experiment with your desired settings\n",
    "results = run_full_experiment(visualize=False, timesteps=50000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
