{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.3)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tnlab\\AppData\\Roaming\\Python\\Python312\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Training PPO for 100,000 steps (this may take a while)...\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 818  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 1024 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 628          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 2048         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046207234 |\n",
      "|    clip_fraction        | 0.0559       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.84        |\n",
      "|    explained_variance   | 0.0284       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.07         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00599     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.38         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 573          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 3072         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030248011 |\n",
      "|    clip_fraction        | 0.0102       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.85        |\n",
      "|    explained_variance   | -0.18        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.443        |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00107     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 1.32         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 566         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007766219 |\n",
      "|    clip_fraction        | 0.0589      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.85       |\n",
      "|    explained_variance   | -0.194      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.42        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00516    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 1.59        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 562          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 9            |\n",
      "|    total_timesteps      | 5120         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019156798 |\n",
      "|    clip_fraction        | 0.00234      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.86        |\n",
      "|    explained_variance   | 0.132        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.8         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00115     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 32.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 558         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008747791 |\n",
      "|    clip_fraction        | 0.0617      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.87       |\n",
      "|    explained_variance   | 0.19        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.5         |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00626    |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 7.24        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 555         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 7168        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008092173 |\n",
      "|    clip_fraction        | 0.0462      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.87       |\n",
      "|    explained_variance   | 0.118       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.17        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00554    |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 30.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 551         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005245587 |\n",
      "|    clip_fraction        | 0.0433      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.87       |\n",
      "|    explained_variance   | -0.133      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.23        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00472    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 6           |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 550         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 9216        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006014066 |\n",
      "|    clip_fraction        | 0.0468      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.87       |\n",
      "|    explained_variance   | 0.393       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.66        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00481    |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 6.27        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 545          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 18           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031700563 |\n",
      "|    clip_fraction        | 0.0354       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.87        |\n",
      "|    explained_variance   | 0.241        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.89         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00508     |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 17.5         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 544          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 20           |\n",
      "|    total_timesteps      | 11264        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027677878 |\n",
      "|    clip_fraction        | 0.0174       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.88        |\n",
      "|    explained_variance   | 0.559        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.15         |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00337     |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 5.67         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 542         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 22          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005794619 |\n",
      "|    clip_fraction        | 0.0254      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.88       |\n",
      "|    explained_variance   | 0.263       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.49        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00313    |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 9.43        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 543         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 24          |\n",
      "|    total_timesteps      | 13312       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004520609 |\n",
      "|    clip_fraction        | 0.00762     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.88       |\n",
      "|    explained_variance   | 0.167       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.04        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00166    |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 11.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 543         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004128048 |\n",
      "|    clip_fraction        | 0.0453      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.88       |\n",
      "|    explained_variance   | 0.318       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.7         |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00395    |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 7.93        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 543         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 28          |\n",
      "|    total_timesteps      | 15360       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008567097 |\n",
      "|    clip_fraction        | 0.0528      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.88       |\n",
      "|    explained_variance   | 0.545       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.51        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00469    |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 6.07        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 538          |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022300454 |\n",
      "|    clip_fraction        | 0.0138       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.87        |\n",
      "|    explained_variance   | -0.16        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.77         |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00197     |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 17.3         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 539           |\n",
      "|    iterations           | 17            |\n",
      "|    time_elapsed         | 32            |\n",
      "|    total_timesteps      | 17408         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00097615935 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.88         |\n",
      "|    explained_variance   | -0.0999       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 11.3          |\n",
      "|    n_updates            | 160           |\n",
      "|    policy_gradient_loss | -0.00102      |\n",
      "|    std                  | 1.02          |\n",
      "|    value_loss           | 21            |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 540          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 34           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045222025 |\n",
      "|    clip_fraction        | 0.00811      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.88        |\n",
      "|    explained_variance   | 0.647        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.2         |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00232     |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 25.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 540         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 35          |\n",
      "|    total_timesteps      | 19456       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004372366 |\n",
      "|    clip_fraction        | 0.0453      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.88       |\n",
      "|    explained_variance   | 0.618       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.82        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.00728    |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 10.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 541          |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 37           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017036299 |\n",
      "|    clip_fraction        | 0.0486       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.87        |\n",
      "|    explained_variance   | 0.232        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.67         |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.000928    |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 12.3         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 541         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 39          |\n",
      "|    total_timesteps      | 21504       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004783192 |\n",
      "|    clip_fraction        | 0.0626      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.88       |\n",
      "|    explained_variance   | -0.626      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.99        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00314    |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 10.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 542         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 41          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008753872 |\n",
      "|    clip_fraction        | 0.0387      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.89       |\n",
      "|    explained_variance   | -0.012      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.33        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00181    |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 16.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 542          |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 43           |\n",
      "|    total_timesteps      | 23552        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031388993 |\n",
      "|    clip_fraction        | 0.0227       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.89        |\n",
      "|    explained_variance   | 0.372        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.82         |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.00526     |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 9.41         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 542         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 45          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005135928 |\n",
      "|    clip_fraction        | 0.0353      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.89       |\n",
      "|    explained_variance   | 0.249       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.91        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00499    |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 13.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 543          |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 47           |\n",
      "|    total_timesteps      | 25600        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059855357 |\n",
      "|    clip_fraction        | 0.042        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.89        |\n",
      "|    explained_variance   | 0.388        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.71         |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00862     |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 15.5         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 543          |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 48           |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049153697 |\n",
      "|    clip_fraction        | 0.0409       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.89        |\n",
      "|    explained_variance   | 0.0158       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.36         |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.00408     |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 16.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 543         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 50          |\n",
      "|    total_timesteps      | 27648       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009012425 |\n",
      "|    clip_fraction        | 0.0509      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.88       |\n",
      "|    explained_variance   | -0.397      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.9         |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.00199    |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 12.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 543         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 52          |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004565115 |\n",
      "|    clip_fraction        | 0.0283      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.87       |\n",
      "|    explained_variance   | 0.00606     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.6         |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.00397    |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 19.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 543          |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 54           |\n",
      "|    total_timesteps      | 29696        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018544513 |\n",
      "|    clip_fraction        | 0.00381      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.87        |\n",
      "|    explained_variance   | 0.128        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.14         |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.00191     |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 17.5         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 543          |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 56           |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040038805 |\n",
      "|    clip_fraction        | 0.00371      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.87        |\n",
      "|    explained_variance   | -0.859       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.8         |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.00249     |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 32           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 544          |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 58           |\n",
      "|    total_timesteps      | 31744        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049312036 |\n",
      "|    clip_fraction        | 0.0181       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.88        |\n",
      "|    explained_variance   | -0.178       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.72         |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.0023      |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 16.6         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 544          |\n",
      "|    iterations           | 32           |\n",
      "|    time_elapsed         | 60           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037285942 |\n",
      "|    clip_fraction        | 0.0563       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.87        |\n",
      "|    explained_variance   | 0.497        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.5         |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.000117    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 13.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 543          |\n",
      "|    iterations           | 33           |\n",
      "|    time_elapsed         | 62           |\n",
      "|    total_timesteps      | 33792        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016822179 |\n",
      "|    clip_fraction        | 0.00137      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.86        |\n",
      "|    explained_variance   | 0.00506      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.79         |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.00144     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 24.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 543          |\n",
      "|    iterations           | 34           |\n",
      "|    time_elapsed         | 64           |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027333298 |\n",
      "|    clip_fraction        | 0.00225      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.85        |\n",
      "|    explained_variance   | 0.116        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.7         |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.00163     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 22.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 543          |\n",
      "|    iterations           | 35           |\n",
      "|    time_elapsed         | 65           |\n",
      "|    total_timesteps      | 35840        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028820296 |\n",
      "|    clip_fraction        | 0.013        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.85        |\n",
      "|    explained_variance   | 0.0182       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.9         |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | -0.00491     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 21.6         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 543          |\n",
      "|    iterations           | 36           |\n",
      "|    time_elapsed         | 67           |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024273642 |\n",
      "|    clip_fraction        | 0.00918      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.85        |\n",
      "|    explained_variance   | 0.327        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.63         |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | -0.00168     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 17.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 543          |\n",
      "|    iterations           | 37           |\n",
      "|    time_elapsed         | 69           |\n",
      "|    total_timesteps      | 37888        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051077697 |\n",
      "|    clip_fraction        | 0.0373       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.86        |\n",
      "|    explained_variance   | -0.134       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.88         |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.00449     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 15.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 543         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 71          |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004419688 |\n",
      "|    clip_fraction        | 0.0129      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.86       |\n",
      "|    explained_variance   | 0.268       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.879       |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.00267    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 8.93        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 542         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 73          |\n",
      "|    total_timesteps      | 39936       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004579373 |\n",
      "|    clip_fraction        | 0.0706      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.85       |\n",
      "|    explained_variance   | 0.107       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.74        |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.00474    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 9.99        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 542         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 75          |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004862548 |\n",
      "|    clip_fraction        | 0.0273      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.85       |\n",
      "|    explained_variance   | 0.102       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.7         |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.00378    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 11.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 542         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 77          |\n",
      "|    total_timesteps      | 41984       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007017377 |\n",
      "|    clip_fraction        | 0.0562      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.85       |\n",
      "|    explained_variance   | -0.403      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.17        |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.00203    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 9.35        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 543          |\n",
      "|    iterations           | 42           |\n",
      "|    time_elapsed         | 79           |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051967977 |\n",
      "|    clip_fraction        | 0.0753       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.87        |\n",
      "|    explained_variance   | 0.14         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.1          |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.00208     |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 11.5         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 543        |\n",
      "|    iterations           | 43         |\n",
      "|    time_elapsed         | 81         |\n",
      "|    total_timesteps      | 44032      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00787909 |\n",
      "|    clip_fraction        | 0.0706     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.88      |\n",
      "|    explained_variance   | 0.0187     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 11.2       |\n",
      "|    n_updates            | 420        |\n",
      "|    policy_gradient_loss | -0.00552   |\n",
      "|    std                  | 1.03       |\n",
      "|    value_loss           | 27.2       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 543         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 82          |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008079513 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.89       |\n",
      "|    explained_variance   | -0.0878     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.56        |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.00794    |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 11.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 543          |\n",
      "|    iterations           | 45           |\n",
      "|    time_elapsed         | 84           |\n",
      "|    total_timesteps      | 46080        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043752547 |\n",
      "|    clip_fraction        | 0.0393       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.91        |\n",
      "|    explained_variance   | 0.277        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.23         |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.00233     |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 5.48         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 543          |\n",
      "|    iterations           | 46           |\n",
      "|    time_elapsed         | 86           |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056503424 |\n",
      "|    clip_fraction        | 0.04         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.92        |\n",
      "|    explained_variance   | -0.578       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.5          |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | -0.00158     |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 6.39         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 543         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 88          |\n",
      "|    total_timesteps      | 48128       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005804359 |\n",
      "|    clip_fraction        | 0.0524      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.93       |\n",
      "|    explained_variance   | -0.245      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.39        |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.00382    |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 15.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 541          |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 90           |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066091847 |\n",
      "|    clip_fraction        | 0.0573       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.92        |\n",
      "|    explained_variance   | 0.0287       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.81         |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.00503     |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 6.61         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 541          |\n",
      "|    iterations           | 49           |\n",
      "|    time_elapsed         | 92           |\n",
      "|    total_timesteps      | 50176        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013900872 |\n",
      "|    clip_fraction        | 0.00205      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.92        |\n",
      "|    explained_variance   | -0.0548      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.51         |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.00154     |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 13.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 541          |\n",
      "|    iterations           | 50           |\n",
      "|    time_elapsed         | 94           |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024094745 |\n",
      "|    clip_fraction        | 0.0258       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.93        |\n",
      "|    explained_variance   | 0.0571       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.49         |\n",
      "|    n_updates            | 490          |\n",
      "|    policy_gradient_loss | -0.00391     |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 34.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 540          |\n",
      "|    iterations           | 51           |\n",
      "|    time_elapsed         | 96           |\n",
      "|    total_timesteps      | 52224        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050229584 |\n",
      "|    clip_fraction        | 0.0213       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.93        |\n",
      "|    explained_variance   | 0.138        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.23         |\n",
      "|    n_updates            | 500          |\n",
      "|    policy_gradient_loss | -0.00163     |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 9.33         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 540        |\n",
      "|    iterations           | 52         |\n",
      "|    time_elapsed         | 98         |\n",
      "|    total_timesteps      | 53248      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00318786 |\n",
      "|    clip_fraction        | 0.00684    |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.92      |\n",
      "|    explained_variance   | 0.0443     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 15.3       |\n",
      "|    n_updates            | 510        |\n",
      "|    policy_gradient_loss | -0.00358   |\n",
      "|    std                  | 1.05       |\n",
      "|    value_loss           | 29.4       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 541         |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 100         |\n",
      "|    total_timesteps      | 54272       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008286045 |\n",
      "|    clip_fraction        | 0.0678      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.91       |\n",
      "|    explained_variance   | 0.0656      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.2        |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.00396    |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 15.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 541          |\n",
      "|    iterations           | 54           |\n",
      "|    time_elapsed         | 102          |\n",
      "|    total_timesteps      | 55296        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053401897 |\n",
      "|    clip_fraction        | 0.0221       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.9         |\n",
      "|    explained_variance   | -0.0303      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.8         |\n",
      "|    n_updates            | 530          |\n",
      "|    policy_gradient_loss | -0.00338     |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 28.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 541          |\n",
      "|    iterations           | 55           |\n",
      "|    time_elapsed         | 104          |\n",
      "|    total_timesteps      | 56320        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020610965 |\n",
      "|    clip_fraction        | 0.00156      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.9         |\n",
      "|    explained_variance   | 0.428        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12.6         |\n",
      "|    n_updates            | 540          |\n",
      "|    policy_gradient_loss | -0.00285     |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 20.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 541         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 105         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009193427 |\n",
      "|    clip_fraction        | 0.0767      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.88       |\n",
      "|    explained_variance   | -0.202      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.99        |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.00329    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 10.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 540          |\n",
      "|    iterations           | 57           |\n",
      "|    time_elapsed         | 107          |\n",
      "|    total_timesteps      | 58368        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043034935 |\n",
      "|    clip_fraction        | 0.0198       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.86        |\n",
      "|    explained_variance   | 0.127        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 24.2         |\n",
      "|    n_updates            | 560          |\n",
      "|    policy_gradient_loss | -0.00401     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 56.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 540          |\n",
      "|    iterations           | 58           |\n",
      "|    time_elapsed         | 109          |\n",
      "|    total_timesteps      | 59392        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048104427 |\n",
      "|    clip_fraction        | 0.0356       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.86        |\n",
      "|    explained_variance   | 0.0979       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.31         |\n",
      "|    n_updates            | 570          |\n",
      "|    policy_gradient_loss | -0.00517     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 20.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 540          |\n",
      "|    iterations           | 59           |\n",
      "|    time_elapsed         | 111          |\n",
      "|    total_timesteps      | 60416        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017991131 |\n",
      "|    clip_fraction        | 0.00244      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.86        |\n",
      "|    explained_variance   | 0.0648       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.87         |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | -0.00228     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 15.3         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 541         |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 113         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005931585 |\n",
      "|    clip_fraction        | 0.0371      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.86       |\n",
      "|    explained_variance   | -0.0447     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.74        |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.00277    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 16.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 541          |\n",
      "|    iterations           | 61           |\n",
      "|    time_elapsed         | 115          |\n",
      "|    total_timesteps      | 62464        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034764851 |\n",
      "|    clip_fraction        | 0.0386       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.87        |\n",
      "|    explained_variance   | 0.0395       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.67         |\n",
      "|    n_updates            | 600          |\n",
      "|    policy_gradient_loss | -0.00486     |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 16.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 541          |\n",
      "|    iterations           | 62           |\n",
      "|    time_elapsed         | 117          |\n",
      "|    total_timesteps      | 63488        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058821035 |\n",
      "|    clip_fraction        | 0.0521       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.87        |\n",
      "|    explained_variance   | -0.00265     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.78         |\n",
      "|    n_updates            | 610          |\n",
      "|    policy_gradient_loss | -0.0047      |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 20.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 541          |\n",
      "|    iterations           | 63           |\n",
      "|    time_elapsed         | 119          |\n",
      "|    total_timesteps      | 64512        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047808215 |\n",
      "|    clip_fraction        | 0.0378       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.87        |\n",
      "|    explained_variance   | -0.199       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.8         |\n",
      "|    n_updates            | 620          |\n",
      "|    policy_gradient_loss | -0.00339     |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 19.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 541          |\n",
      "|    iterations           | 64           |\n",
      "|    time_elapsed         | 121          |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057123993 |\n",
      "|    clip_fraction        | 0.0612       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.88        |\n",
      "|    explained_variance   | -0.0765      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.93         |\n",
      "|    n_updates            | 630          |\n",
      "|    policy_gradient_loss | -0.00266     |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 10.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 541          |\n",
      "|    iterations           | 65           |\n",
      "|    time_elapsed         | 122          |\n",
      "|    total_timesteps      | 66560        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034697847 |\n",
      "|    clip_fraction        | 0.0181       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.88        |\n",
      "|    explained_variance   | 0.531        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.5          |\n",
      "|    n_updates            | 640          |\n",
      "|    policy_gradient_loss | -0.00594     |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 12.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 541         |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 124         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002218849 |\n",
      "|    clip_fraction        | 0.00273     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.87       |\n",
      "|    explained_variance   | 0.145       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.06        |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.00181    |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 15.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 541         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 126         |\n",
      "|    total_timesteps      | 68608       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005749015 |\n",
      "|    clip_fraction        | 0.0201      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.88       |\n",
      "|    explained_variance   | 0.221       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.95        |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.00732    |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 20.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 541          |\n",
      "|    iterations           | 68           |\n",
      "|    time_elapsed         | 128          |\n",
      "|    total_timesteps      | 69632        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048087053 |\n",
      "|    clip_fraction        | 0.0401       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.88        |\n",
      "|    explained_variance   | 0.649        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.7         |\n",
      "|    n_updates            | 670          |\n",
      "|    policy_gradient_loss | -0.00327     |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 19.6         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 542          |\n",
      "|    iterations           | 69           |\n",
      "|    time_elapsed         | 130          |\n",
      "|    total_timesteps      | 70656        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046303025 |\n",
      "|    clip_fraction        | 0.04         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.88        |\n",
      "|    explained_variance   | -0.0101      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.1         |\n",
      "|    n_updates            | 680          |\n",
      "|    policy_gradient_loss | -0.00443     |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 21.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 542          |\n",
      "|    iterations           | 70           |\n",
      "|    time_elapsed         | 132          |\n",
      "|    total_timesteps      | 71680        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076358444 |\n",
      "|    clip_fraction        | 0.0384       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.89        |\n",
      "|    explained_variance   | 0.218        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.5         |\n",
      "|    n_updates            | 690          |\n",
      "|    policy_gradient_loss | -0.00381     |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 40.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 542          |\n",
      "|    iterations           | 71           |\n",
      "|    time_elapsed         | 134          |\n",
      "|    total_timesteps      | 72704        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055641215 |\n",
      "|    clip_fraction        | 0.0565       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.89        |\n",
      "|    explained_variance   | 0.548        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.3         |\n",
      "|    n_updates            | 700          |\n",
      "|    policy_gradient_loss | -0.00122     |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 20.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 541          |\n",
      "|    iterations           | 72           |\n",
      "|    time_elapsed         | 136          |\n",
      "|    total_timesteps      | 73728        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021599974 |\n",
      "|    clip_fraction        | 0.004        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.89        |\n",
      "|    explained_variance   | 0.745        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.43         |\n",
      "|    n_updates            | 710          |\n",
      "|    policy_gradient_loss | -0.00303     |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 11.5         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 541         |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 138         |\n",
      "|    total_timesteps      | 74752       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006283352 |\n",
      "|    clip_fraction        | 0.0307      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.9        |\n",
      "|    explained_variance   | 0.457       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.7        |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.00523    |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 35          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 541          |\n",
      "|    iterations           | 74           |\n",
      "|    time_elapsed         | 139          |\n",
      "|    total_timesteps      | 75776        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038756584 |\n",
      "|    clip_fraction        | 0.0245       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.91        |\n",
      "|    explained_variance   | 0.185        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.7         |\n",
      "|    n_updates            | 730          |\n",
      "|    policy_gradient_loss | -0.00168     |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 17           |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 541         |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 141         |\n",
      "|    total_timesteps      | 76800       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004177493 |\n",
      "|    clip_fraction        | 0.0447      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.91       |\n",
      "|    explained_variance   | 0.319       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.16        |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.00768    |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 21.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 542         |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 143         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006390554 |\n",
      "|    clip_fraction        | 0.0123      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.91       |\n",
      "|    explained_variance   | 0.481       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.99        |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.000612   |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 21.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 542         |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 145         |\n",
      "|    total_timesteps      | 78848       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004827663 |\n",
      "|    clip_fraction        | 0.0469      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.9        |\n",
      "|    explained_variance   | 0.403       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.91        |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.00528    |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 13.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 542          |\n",
      "|    iterations           | 78           |\n",
      "|    time_elapsed         | 147          |\n",
      "|    total_timesteps      | 79872        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012480409 |\n",
      "|    clip_fraction        | 0.000293     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.9         |\n",
      "|    explained_variance   | 0.0909       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.08         |\n",
      "|    n_updates            | 770          |\n",
      "|    policy_gradient_loss | -0.000619    |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 23.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 542          |\n",
      "|    iterations           | 79           |\n",
      "|    time_elapsed         | 149          |\n",
      "|    total_timesteps      | 80896        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018227838 |\n",
      "|    clip_fraction        | 0.00156      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.9         |\n",
      "|    explained_variance   | 0.59         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.09         |\n",
      "|    n_updates            | 780          |\n",
      "|    policy_gradient_loss | -0.00163     |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 14.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 542          |\n",
      "|    iterations           | 80           |\n",
      "|    time_elapsed         | 151          |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045633414 |\n",
      "|    clip_fraction        | 0.00791      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.9         |\n",
      "|    explained_variance   | 0.688        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.6          |\n",
      "|    n_updates            | 790          |\n",
      "|    policy_gradient_loss | -0.00224     |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 20.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 542         |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 152         |\n",
      "|    total_timesteps      | 82944       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010467507 |\n",
      "|    clip_fraction        | 0.0896      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.9        |\n",
      "|    explained_variance   | 0.813       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.06        |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.00541    |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 15.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 542         |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 154         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001994553 |\n",
      "|    clip_fraction        | 0.00127     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.9        |\n",
      "|    explained_variance   | 0.0788      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 23.5        |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.00102    |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 19.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 542          |\n",
      "|    iterations           | 83           |\n",
      "|    time_elapsed         | 156          |\n",
      "|    total_timesteps      | 84992        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037295036 |\n",
      "|    clip_fraction        | 0.0292       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.9         |\n",
      "|    explained_variance   | 0.281        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 18.9         |\n",
      "|    n_updates            | 820          |\n",
      "|    policy_gradient_loss | -0.00357     |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 62.5         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 542         |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 158         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005822875 |\n",
      "|    clip_fraction        | 0.034       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.9        |\n",
      "|    explained_variance   | 0.395       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.87        |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.00523    |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 14.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 542          |\n",
      "|    iterations           | 85           |\n",
      "|    time_elapsed         | 160          |\n",
      "|    total_timesteps      | 87040        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035201325 |\n",
      "|    clip_fraction        | 0.0202       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.9         |\n",
      "|    explained_variance   | 0.189        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.11         |\n",
      "|    n_updates            | 840          |\n",
      "|    policy_gradient_loss | -0.0012      |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 13           |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 542         |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 162         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005153681 |\n",
      "|    clip_fraction        | 0.0459      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.9        |\n",
      "|    explained_variance   | 0.306       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.97        |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.00323    |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 9.9         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 541         |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 164         |\n",
      "|    total_timesteps      | 89088       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011039337 |\n",
      "|    clip_fraction        | 0.0774      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.9        |\n",
      "|    explained_variance   | 0.788       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.9        |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.00182    |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 9.2         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 541        |\n",
      "|    iterations           | 88         |\n",
      "|    time_elapsed         | 166        |\n",
      "|    total_timesteps      | 90112      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00500884 |\n",
      "|    clip_fraction        | 0.0526     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.91      |\n",
      "|    explained_variance   | 0.247      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 4.89       |\n",
      "|    n_updates            | 870        |\n",
      "|    policy_gradient_loss | -0.00481   |\n",
      "|    std                  | 1.04       |\n",
      "|    value_loss           | 15.2       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 541          |\n",
      "|    iterations           | 89           |\n",
      "|    time_elapsed         | 168          |\n",
      "|    total_timesteps      | 91136        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060094357 |\n",
      "|    clip_fraction        | 0.0138       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.91        |\n",
      "|    explained_variance   | 0.485        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.81         |\n",
      "|    n_updates            | 880          |\n",
      "|    policy_gradient_loss | -0.00256     |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 9.87         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 542          |\n",
      "|    iterations           | 90           |\n",
      "|    time_elapsed         | 170          |\n",
      "|    total_timesteps      | 92160        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031889337 |\n",
      "|    clip_fraction        | 0.0156       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.91        |\n",
      "|    explained_variance   | 0.419        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.08         |\n",
      "|    n_updates            | 890          |\n",
      "|    policy_gradient_loss | -0.00168     |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 12.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 542         |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 171         |\n",
      "|    total_timesteps      | 93184       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003899116 |\n",
      "|    clip_fraction        | 0.0338      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.92       |\n",
      "|    explained_variance   | 0.758       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.05        |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0023     |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 9.93        |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 542           |\n",
      "|    iterations           | 92            |\n",
      "|    time_elapsed         | 173           |\n",
      "|    total_timesteps      | 94208         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00067050377 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -2.93         |\n",
      "|    explained_variance   | 0.267         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 21.3          |\n",
      "|    n_updates            | 910           |\n",
      "|    policy_gradient_loss | -0.000913     |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 22            |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 542         |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 175         |\n",
      "|    total_timesteps      | 95232       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007556334 |\n",
      "|    clip_fraction        | 0.0612      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.93       |\n",
      "|    explained_variance   | 0.604       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.63        |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.00474    |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 11.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 542          |\n",
      "|    iterations           | 94           |\n",
      "|    time_elapsed         | 177          |\n",
      "|    total_timesteps      | 96256        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055232407 |\n",
      "|    clip_fraction        | 0.0263       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.93        |\n",
      "|    explained_variance   | 0.451        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.4         |\n",
      "|    n_updates            | 930          |\n",
      "|    policy_gradient_loss | -0.00344     |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 25.6         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 542          |\n",
      "|    iterations           | 95           |\n",
      "|    time_elapsed         | 179          |\n",
      "|    total_timesteps      | 97280        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033726788 |\n",
      "|    clip_fraction        | 0.0106       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.93        |\n",
      "|    explained_variance   | 0.263        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.7         |\n",
      "|    n_updates            | 940          |\n",
      "|    policy_gradient_loss | -0.00255     |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 44.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 542          |\n",
      "|    iterations           | 96           |\n",
      "|    time_elapsed         | 181          |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027862366 |\n",
      "|    clip_fraction        | 0.0118       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.93        |\n",
      "|    explained_variance   | 0.565        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.1         |\n",
      "|    n_updates            | 950          |\n",
      "|    policy_gradient_loss | -0.00285     |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 36.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 542          |\n",
      "|    iterations           | 97           |\n",
      "|    time_elapsed         | 183          |\n",
      "|    total_timesteps      | 99328        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046829423 |\n",
      "|    clip_fraction        | 0.0141       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -2.93        |\n",
      "|    explained_variance   | 0.441        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 23.7         |\n",
      "|    n_updates            | 960          |\n",
      "|    policy_gradient_loss | -0.00236     |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 64.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 542         |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 185         |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000726369 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.94       |\n",
      "|    explained_variance   | 0.39        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.3        |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -2.58e-05   |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 62.4        |\n",
      "-----------------------------------------\n",
      "Training complete!\n",
      "\n",
      "Manual control + PPO running!\n",
      "Use arrow keys to move. Window will remain open until you close it.\n",
      "Episode finished. Total reward: -447.07\n",
      "Episode finished. Total reward: -23.49\n",
      "Episode finished. Total reward: -129.29\n",
      "Episode finished. Total reward: -25.99\n",
      "Episode finished. Total reward: -142.22\n",
      "Episode finished. Total reward: -196.33\n",
      "Episode finished. Total reward: -122.73\n",
      "Episode finished. Total reward: -38.23\n",
      "Episode finished. Total reward: -489.97\n",
      "Episode finished. Total reward: -561.81\n",
      "Episode finished. Total reward: -354.45\n",
      "Episode finished. Total reward: -262.99\n",
      "Episode finished. Total reward: -213.79\n",
      "Episode finished. Total reward: -235.53\n",
      "Episode finished. Total reward: -148.53\n",
      "Episode finished. Total reward: -104.01\n",
      "Episode finished. Total reward: -87.81\n",
      "Episode finished. Total reward: -39.23\n",
      "Episode finished. Total reward: -126.50\n",
      "Episode finished. Total reward: -83.57\n",
      "Episode finished. Total reward: -40.60\n",
      "Episode finished. Total reward: -139.41\n",
      "Episode finished. Total reward: -80.92\n",
      "Episode finished. Total reward: -501.33\n",
      "Episode finished. Total reward: -60.96\n",
      "Episode finished. Total reward: -339.91\n",
      "Episode finished. Total reward: -357.42\n",
      "Episode finished. Total reward: -71.82\n",
      "Episode finished. Total reward: -38.26\n",
      "Episode finished. Total reward: -451.56\n",
      "Episode finished. Total reward: -469.29\n",
      "Episode finished. Total reward: -500.63\n",
      "Episode finished. Total reward: -135.30\n",
      "Episode finished. Total reward: -102.89\n",
      "Episode finished. Total reward: -145.19\n",
      "Episode finished. Total reward: -509.80\n",
      "Episode finished. Total reward: -469.60\n",
      "Episode finished. Total reward: -83.03\n",
      "Episode finished. Total reward: -481.06\n",
      "Episode finished. Total reward: -579.17\n",
      "Episode finished. Total reward: -57.01\n",
      "Episode finished. Total reward: -34.11\n",
      "Episode finished. Total reward: -1.14\n",
      "Episode finished. Total reward: -62.85\n",
      "Episode finished. Total reward: -163.50\n",
      "Episode finished. Total reward: -550.25\n",
      "Episode finished. Total reward: -30.45\n",
      "Episode finished. Total reward: -442.61\n",
      "Episode finished. Total reward: -515.82\n",
      "Episode finished. Total reward: -497.27\n",
      "Episode finished. Total reward: -438.65\n",
      "Episode finished. Total reward: -297.81\n",
      "Episode finished. Total reward: -38.16\n",
      "Episode finished. Total reward: -193.32\n",
      "Episode finished. Total reward: -388.57\n",
      "Episode finished. Total reward: -10.36\n",
      "Episode finished. Total reward: -353.51\n",
      "Episode finished. Total reward: -553.74\n",
      "Episode finished. Total reward: -140.41\n",
      "Episode finished. Total reward: -142.36\n",
      "Episode finished. Total reward: -236.66\n",
      "Episode finished. Total reward: -364.93\n",
      "Episode finished. Total reward: -495.44\n",
      "Episode finished. Total reward: -545.60\n",
      "Episode finished. Total reward: -362.74\n",
      "Episode finished. Total reward: -470.41\n",
      "Episode finished. Total reward: -493.46\n",
      "Episode finished. Total reward: -192.49\n",
      "Episode finished. Total reward: -120.79\n",
      "Episode finished. Total reward: -361.04\n",
      "Episode finished. Total reward: -29.01\n",
      "Episode finished. Total reward: -80.69\n",
      "Episode finished. Total reward: -91.87\n",
      "Episode finished. Total reward: -152.85\n",
      "Episode finished. Total reward: -9.38\n",
      "Episode finished. Total reward: -556.40\n",
      "Episode finished. Total reward: -542.27\n",
      "Episode finished. Total reward: -88.62\n",
      "Episode finished. Total reward: -4.04\n",
      "Episode finished. Total reward: -454.92\n",
      "Episode finished. Total reward: -5.81\n",
      "Episode finished. Total reward: -363.80\n",
      "Episode finished. Total reward: -398.71\n",
      "Episode finished. Total reward: 9.98\n",
      "Episode finished. Total reward: -38.12\n",
      "Episode finished. Total reward: -498.48\n",
      "Episode finished. Total reward: -73.63\n",
      "Episode finished. Total reward: -442.69\n",
      "Episode finished. Total reward: -470.85\n",
      "Episode finished. Total reward: -487.12\n",
      "Episode finished. Total reward: -189.84\n",
      "Episode finished. Total reward: -511.32\n",
      "Episode finished. Total reward: -396.75\n",
      "Episode finished. Total reward: -413.29\n",
      "Episode finished. Total reward: -186.57\n",
      "Episode finished. Total reward: -289.38\n",
      "Episode finished. Total reward: -429.42\n",
      "Episode finished. Total reward: -383.11\n",
      "Episode finished. Total reward: -333.76\n",
      "Episode finished. Total reward: -469.26\n",
      "Episode finished. Total reward: -68.77\n",
      "Episode finished. Total reward: -336.72\n",
      "Episode finished. Total reward: -438.29\n",
      "Episode finished. Total reward: -129.91\n",
      "Episode finished. Total reward: -450.56\n",
      "Episode finished. Total reward: -105.78\n",
      "Episode finished. Total reward: -249.13\n",
      "Episode finished. Total reward: -505.89\n",
      "Episode finished. Total reward: -8.27\n",
      "Episode finished. Total reward: -25.46\n",
      "Episode finished. Total reward: -10.60\n",
      "Episode finished. Total reward: -432.47\n",
      "Episode finished. Total reward: -410.08\n",
      "Episode finished. Total reward: -337.27\n",
      "Episode finished. Total reward: -38.23\n",
      "Episode finished. Total reward: -351.14\n",
      "Episode finished. Total reward: -259.60\n",
      "Episode finished. Total reward: -38.29\n",
      "Episode finished. Total reward: -67.53\n",
      "Episode finished. Total reward: -185.76\n",
      "Episode finished. Total reward: -480.06\n",
      "Episode finished. Total reward: -241.24\n",
      "Episode finished. Total reward: -580.22\n",
      "Episode finished. Total reward: -438.02\n",
      "Episode finished. Total reward: -59.76\n",
      "Episode finished. Total reward: -356.74\n",
      "Episode finished. Total reward: -415.92\n",
      "Episode finished. Total reward: -38.33\n",
      "Episode finished. Total reward: -151.97\n",
      "Episode finished. Total reward: -504.06\n",
      "Episode finished. Total reward: -72.67\n",
      "Episode finished. Total reward: -435.73\n",
      "Episode finished. Total reward: -13.13\n",
      "Episode finished. Total reward: -61.92\n",
      "Episode finished. Total reward: -447.65\n",
      "Episode finished. Total reward: -501.38\n",
      "Episode finished. Total reward: -358.24\n",
      "Episode finished. Total reward: -348.00\n",
      "Episode finished. Total reward: -516.86\n",
      "Episode finished. Total reward: -476.10\n",
      "Episode finished. Total reward: -424.92\n",
      "Episode finished. Total reward: -444.93\n",
      "Episode finished. Total reward: -12.78\n",
      "Episode finished. Total reward: -80.08\n",
      "Episode finished. Total reward: -108.08\n",
      "Episode finished. Total reward: -123.43\n",
      "Episode finished. Total reward: -471.57\n",
      "Episode finished. Total reward: -583.96\n",
      "Episode finished. Total reward: -38.36\n",
      "Episode finished. Total reward: -245.47\n",
      "Episode finished. Total reward: -258.34\n",
      "Episode finished. Total reward: -464.89\n",
      "Episode finished. Total reward: -411.94\n",
      "Episode finished. Total reward: -379.12\n",
      "Episode finished. Total reward: -13.19\n",
      "Episode finished. Total reward: -408.14\n",
      "Episode finished. Total reward: -119.32\n",
      "Episode finished. Total reward: -353.35\n",
      "Episode finished. Total reward: -112.33\n",
      "Episode finished. Total reward: -521.96\n",
      "Episode finished. Total reward: -218.43\n",
      "Episode finished. Total reward: -528.71\n",
      "Episode finished. Total reward: -502.60\n",
      "Episode finished. Total reward: -466.34\n",
      "Episode finished. Total reward: -497.91\n",
      "Episode finished. Total reward: -474.67\n",
      "Episode finished. Total reward: -453.98\n",
      "Episode finished. Total reward: -9.93\n",
      "Episode finished. Total reward: -38.23\n",
      "Episode finished. Total reward: -493.06\n",
      "Episode finished. Total reward: -188.78\n",
      "Episode finished. Total reward: -255.86\n",
      "Episode finished. Total reward: -209.18\n",
      "Episode finished. Total reward: -151.99\n",
      "Episode finished. Total reward: -467.08\n",
      "Episode finished. Total reward: -426.45\n",
      "Episode finished. Total reward: -115.45\n",
      "Episode finished. Total reward: -433.69\n",
      "Episode finished. Total reward: -400.54\n",
      "Episode finished. Total reward: -115.03\n",
      "Episode finished. Total reward: -500.71\n",
      "Episode finished. Total reward: -109.04\n",
      "Episode finished. Total reward: -18.88\n",
      "Episode finished. Total reward: -138.75\n",
      "Episode finished. Total reward: -392.82\n",
      "Episode finished. Total reward: -357.22\n",
      "Episode finished. Total reward: -152.34\n",
      "Episode finished. Total reward: -454.26\n",
      "Episode finished. Total reward: -376.80\n",
      "Episode finished. Total reward: -421.26\n",
      "Episode finished. Total reward: -220.83\n",
      "Episode finished. Total reward: -138.47\n",
      "Episode finished. Total reward: -409.05\n",
      "Episode finished. Total reward: -385.73\n",
      "Episode finished. Total reward: -12.63\n",
      "Episode finished. Total reward: -43.15\n",
      "Episode finished. Total reward: -441.07\n",
      "Episode finished. Total reward: -512.71\n",
      "Episode finished. Total reward: -549.27\n",
      "Episode finished. Total reward: -390.43\n",
      "Episode finished. Total reward: -75.96\n",
      "Episode finished. Total reward: -374.17\n",
      "Episode finished. Total reward: -8.47\n",
      "Episode finished. Total reward: -21.07\n",
      "Episode finished. Total reward: -245.95\n",
      "Episode finished. Total reward: -134.14\n",
      "Episode finished. Total reward: -347.40\n",
      "Episode finished. Total reward: -479.54\n",
      "Episode finished. Total reward: -76.10\n",
      "Episode finished. Total reward: -295.52\n",
      "Episode finished. Total reward: -430.59\n",
      "Episode finished. Total reward: -467.31\n",
      "Episode finished. Total reward: -444.66\n",
      "Episode finished. Total reward: -97.96\n",
      "Episode finished. Total reward: -64.69\n",
      "Episode finished. Total reward: -114.80\n",
      "Episode finished. Total reward: -102.65\n",
      "Episode finished. Total reward: -426.68\n",
      "Episode finished. Total reward: -242.02\n",
      "Episode finished. Total reward: -474.99\n",
      "Episode finished. Total reward: -35.50\n",
      "Episode finished. Total reward: -398.19\n",
      "Episode finished. Total reward: -41.12\n",
      "Episode finished. Total reward: -373.33\n",
      "Episode finished. Total reward: -97.07\n",
      "Episode finished. Total reward: -472.30\n",
      "Episode finished. Total reward: -119.96\n",
      "Episode finished. Total reward: -102.35\n",
      "Episode finished. Total reward: -304.34\n",
      "Episode finished. Total reward: -521.20\n",
      "Episode finished. Total reward: -369.02\n",
      "Episode finished. Total reward: -61.02\n",
      "Episode finished. Total reward: -508.91\n",
      "Episode finished. Total reward: -451.30\n",
      "Episode finished. Total reward: -306.91\n",
      "Episode finished. Total reward: -468.24\n",
      "Episode finished. Total reward: -384.93\n",
      "Episode finished. Total reward: -394.42\n",
      "Episode finished. Total reward: -262.26\n",
      "Episode finished. Total reward: -536.38\n",
      "Episode finished. Total reward: -86.94\n",
      "Episode finished. Total reward: -103.83\n",
      "Episode finished. Total reward: -542.82\n",
      "Episode finished. Total reward: -464.31\n",
      "Episode finished. Total reward: -444.33\n",
      "Episode finished. Total reward: -571.54\n",
      "Episode finished. Total reward: -428.90\n",
      "Episode finished. Total reward: -448.01\n",
      "Episode finished. Total reward: -496.28\n",
      "Episode finished. Total reward: -247.59\n",
      "Episode finished. Total reward: -463.54\n",
      "Episode finished. Total reward: -427.02\n",
      "Episode finished. Total reward: -38.23\n",
      "Episode finished. Total reward: -454.28\n",
      "Episode finished. Total reward: -323.93\n",
      "Episode finished. Total reward: -464.54\n",
      "Episode finished. Total reward: -172.86\n",
      "Episode finished. Total reward: -155.36\n",
      "Episode finished. Total reward: -50.85\n",
      "Episode finished. Total reward: -460.86\n",
      "Episode finished. Total reward: -574.72\n",
      "Episode finished. Total reward: -432.78\n",
      "Episode finished. Total reward: 9.96\n",
      "Episode finished. Total reward: -45.29\n",
      "Episode finished. Total reward: -106.14\n",
      "Episode finished. Total reward: -37.87\n",
      "Episode finished. Total reward: -451.27\n",
      "Episode finished. Total reward: -400.71\n",
      "Episode finished. Total reward: -7.69\n",
      "Episode finished. Total reward: -456.80\n",
      "Episode finished. Total reward: -275.96\n",
      "Episode finished. Total reward: -195.82\n",
      "Episode finished. Total reward: -224.19\n",
      "Episode finished. Total reward: -439.72\n",
      "Episode finished. Total reward: -445.60\n",
      "Episode finished. Total reward: -38.23\n",
      "Episode finished. Total reward: -415.19\n",
      "Episode finished. Total reward: -264.81\n",
      "Episode finished. Total reward: -368.50\n",
      "Episode finished. Total reward: -462.03\n",
      "Episode finished. Total reward: -57.27\n",
      "Episode finished. Total reward: -264.81\n",
      "Episode finished. Total reward: -116.68\n",
      "Episode finished. Total reward: -3.29\n",
      "Episode finished. Total reward: -298.90\n",
      "Episode finished. Total reward: -465.01\n",
      "Episode finished. Total reward: -25.72\n",
      "Episode finished. Total reward: -154.04\n",
      "Episode finished. Total reward: -340.39\n",
      "Episode finished. Total reward: -405.34\n",
      "Episode finished. Total reward: -177.80\n",
      "Episode finished. Total reward: -478.80\n",
      "Episode finished. Total reward: -326.39\n",
      "Episode finished. Total reward: -468.20\n",
      "Episode finished. Total reward: -478.40\n",
      "Episode finished. Total reward: -108.67\n",
      "Episode finished. Total reward: -93.81\n",
      "Episode finished. Total reward: -475.46\n",
      "Episode finished. Total reward: -499.48\n",
      "Episode finished. Total reward: -503.56\n",
      "Episode finished. Total reward: -338.08\n",
      "Episode finished. Total reward: -257.45\n",
      "Episode finished. Total reward: -113.00\n",
      "Episode finished. Total reward: -392.69\n",
      "Episode finished. Total reward: -38.23\n",
      "Episode finished. Total reward: 4.12\n",
      "Episode finished. Total reward: -130.35\n",
      "Episode finished. Total reward: -493.80\n",
      "Episode finished. Total reward: -380.18\n",
      "Episode finished. Total reward: -377.95\n",
      "Episode finished. Total reward: -92.15\n",
      "Episode finished. Total reward: -362.15\n",
      "Episode finished. Total reward: -540.20\n",
      "Episode finished. Total reward: -504.59\n",
      "Episode finished. Total reward: -368.22\n",
      "Episode finished. Total reward: -407.50\n",
      "Episode finished. Total reward: -396.83\n",
      "Episode finished. Total reward: -95.97\n",
      "Episode finished. Total reward: -295.36\n",
      "Episode finished. Total reward: -467.14\n",
      "Episode finished. Total reward: -533.93\n",
      "Episode finished. Total reward: -452.96\n",
      "Episode finished. Total reward: -514.73\n",
      "Episode finished. Total reward: -200.65\n",
      "Episode finished. Total reward: -462.80\n",
      "Episode finished. Total reward: -342.12\n",
      "Episode finished. Total reward: -66.01\n",
      "Episode finished. Total reward: -491.09\n",
      "Episode finished. Total reward: -466.63\n",
      "Episode finished. Total reward: -410.64\n",
      "Episode finished. Total reward: -157.57\n",
      "Episode finished. Total reward: -395.72\n",
      "Episode finished. Total reward: -507.36\n",
      "Episode finished. Total reward: -389.45\n",
      "Episode finished. Total reward: -91.02\n",
      "Episode finished. Total reward: -447.00\n",
      "Episode finished. Total reward: -384.86\n",
      "Episode finished. Total reward: -483.50\n",
      "Episode finished. Total reward: -47.99\n",
      "Episode finished. Total reward: -32.91\n",
      "Episode finished. Total reward: -510.84\n",
      "Episode finished. Total reward: -358.12\n",
      "Episode finished. Total reward: -173.22\n",
      "Episode finished. Total reward: -75.92\n",
      "Episode finished. Total reward: -438.87\n",
      "Episode finished. Total reward: -500.89\n",
      "Episode finished. Total reward: -524.88\n",
      "Episode finished. Total reward: -132.80\n",
      "Episode finished. Total reward: 10.00\n",
      "Episode finished. Total reward: -357.43\n",
      "Episode finished. Total reward: -302.44\n",
      "Episode finished. Total reward: -132.41\n",
      "Episode finished. Total reward: -15.74\n",
      "Episode finished. Total reward: -337.80\n",
      "Episode finished. Total reward: -507.55\n",
      "Episode finished. Total reward: -365.61\n",
      "Episode finished. Total reward: -111.70\n",
      "Episode finished. Total reward: -184.25\n",
      "Episode finished. Total reward: -17.31\n",
      "Episode finished. Total reward: -443.59\n",
      "Episode finished. Total reward: -420.37\n",
      "Episode finished. Total reward: -535.28\n",
      "Episode finished. Total reward: -38.23\n",
      "Episode finished. Total reward: -0.60\n",
      "Episode finished. Total reward: -18.04\n",
      "Episode finished. Total reward: -89.34\n",
      "Episode finished. Total reward: 10.00\n",
      "Episode finished. Total reward: -409.96\n",
      "Episode finished. Total reward: -113.57\n",
      "Episode finished. Total reward: -355.04\n",
      "Episode finished. Total reward: -132.87\n",
      "Episode finished. Total reward: -475.47\n",
      "Episode finished. Total reward: -532.68\n",
      "Episode finished. Total reward: -434.55\n",
      "Episode finished. Total reward: -517.17\n",
      "Episode finished. Total reward: -510.54\n",
      "Episode finished. Total reward: -540.84\n",
      "Episode finished. Total reward: -89.19\n",
      "Episode finished. Total reward: -16.73\n",
      "Episode finished. Total reward: -391.19\n",
      "Episode finished. Total reward: -450.29\n",
      "Episode finished. Total reward: -475.64\n",
      "Episode finished. Total reward: -26.99\n",
      "Episode finished. Total reward: -479.74\n",
      "Episode finished. Total reward: -79.07\n",
      "Episode finished. Total reward: -15.77\n",
      "Episode finished. Total reward: -390.82\n",
      "Episode finished. Total reward: -261.36\n",
      "Episode finished. Total reward: 2.35\n",
      "Episode finished. Total reward: -458.39\n",
      "Episode finished. Total reward: -396.74\n",
      "Episode finished. Total reward: -378.06\n",
      "Episode finished. Total reward: -17.53\n",
      "Episode finished. Total reward: -174.83\n",
      "Episode finished. Total reward: -501.58\n",
      "Episode finished. Total reward: -100.22\n",
      "Episode finished. Total reward: -70.00\n",
      "Episode finished. Total reward: -389.23\n",
      "Episode finished. Total reward: -540.26\n",
      "Episode finished. Total reward: -74.69\n",
      "Episode finished. Total reward: -8.60\n",
      "Episode finished. Total reward: -404.48\n",
      "Episode finished. Total reward: -116.82\n",
      "Episode finished. Total reward: -457.34\n",
      "Episode finished. Total reward: -457.36\n",
      "Episode finished. Total reward: -457.58\n",
      "Episode finished. Total reward: -99.59\n",
      "Episode finished. Total reward: -488.81\n",
      "Episode finished. Total reward: -83.95\n",
      "Episode finished. Total reward: -73.51\n",
      "Episode finished. Total reward: 10.00\n",
      "Episode finished. Total reward: -182.69\n",
      "Episode finished. Total reward: -490.36\n",
      "Episode finished. Total reward: -34.81\n",
      "Episode finished. Total reward: -387.26\n",
      "Episode finished. Total reward: -46.18\n",
      "Episode finished. Total reward: -399.93\n",
      "Episode finished. Total reward: -238.31\n",
      "Episode finished. Total reward: -292.44\n",
      "Episode finished. Total reward: -514.68\n",
      "Episode finished. Total reward: -463.78\n",
      "Episode finished. Total reward: -486.93\n",
      "Episode finished. Total reward: -58.61\n",
      "Episode finished. Total reward: -22.67\n",
      "Episode finished. Total reward: -458.57\n",
      "Episode finished. Total reward: -247.61\n",
      "Episode finished. Total reward: -105.20\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "import pygame\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "#\n",
    "# 1) HumanLikeAgent replaced by \"manual\" input from keyboard in the enjoy phase.\n",
    "#    If you still want to simulate a \"human\" automatically, re-introduce that code.\n",
    "#\n",
    "\n",
    "\n",
    "class CursorControlEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    A 2D environment with:\n",
    "      - Exactly ONE randomly chosen goal each episode\n",
    "      - Weighted sum of agent_action + manual/human_action\n",
    "      - Reward for staying close to the perfect path from start->goal (straight line).\n",
    "    No obstacles.\n",
    "\n",
    "    In \"training mode\", we simulate a random 'human' so the agent can learn a policy.\n",
    "    In \"manual mode\", we read keyboard input so you can see how the policy behaves with your input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        width=800,\n",
    "        height=600,\n",
    "        gamma=0.5,              # weight for PPO agent's action in the sum\n",
    "        max_steps=500,\n",
    "        goal_distance_threshold=20.0,\n",
    "        render_mode=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.gamma = gamma\n",
    "        self.max_steps = max_steps\n",
    "        self.goal_distance_threshold = goal_distance_threshold\n",
    "\n",
    "        # Spaces\n",
    "        # PPO agent outputs a 2D action in [-1,1].\n",
    "        # We'll combine it with a \"human/manual\" action (also in [-1,1]) in step().\n",
    "        self.action_space = spaces.Box(\n",
    "            low=np.array([-1.0, -1.0], dtype=np.float32),\n",
    "            high=np.array([1.0,  1.0], dtype=np.float32),\n",
    "            shape=(2,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        # Observations: [dot_x, dot_y, goal_x, goal_y]\n",
    "        high = np.array([width, height, width, height], dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.zeros_like(high, dtype=np.float32),\n",
    "            high=high,\n",
    "            shape=(4,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # State\n",
    "        self.dot_pos = None\n",
    "        self.start_pos = None\n",
    "        self.goal_pos = None\n",
    "        self.current_step = 0\n",
    "        self.done_flag = False\n",
    "\n",
    "        self.max_speed = 3.0  # scale for final movement\n",
    "        self.dot_radius = 10.0\n",
    "\n",
    "        # Rendering\n",
    "        self.render_mode = render_mode\n",
    "        if self.render_mode:\n",
    "            pygame.init()\n",
    "            self.screen = pygame.display.set_mode((self.width, self.height))\n",
    "            pygame.display.set_caption(\"Cursor Control: Manual + Agent\")\n",
    "            self.clock = pygame.time.Clock()\n",
    "            self.font = pygame.font.SysFont(None, 24)\n",
    "\n",
    "        # For training mode, we simulate a random human action\n",
    "        # (If you want a better \"human\" model, adapt these.)\n",
    "        self.simulate_human_noise = 0.3\n",
    "\n",
    "    def reset(self):\n",
    "        self.done_flag = False\n",
    "        self.current_step = 0\n",
    "        self.dot_pos = np.array([self.width/2, self.height/2], dtype=np.float32)\n",
    "        self.start_pos = self.dot_pos.copy()\n",
    "\n",
    "        gx = random.uniform(0, self.width)\n",
    "        gy = random.uniform(0, self.height)\n",
    "        self.goal_pos = np.array([gx, gy], dtype=np.float32)\n",
    "\n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, agent_action, manual_action=None):\n",
    "        \"\"\"\n",
    "        Step the environment:\n",
    "          agent_action: (dx, dy) from PPO in [-1,1].\n",
    "          manual_action: (dx, dy) from keyboard/human in [-1,1].\n",
    "                         If None, we simulate a random 'human' for training.\n",
    "\n",
    "        Weighted sum:\n",
    "          combined = gamma*(agent_action) + (1-gamma)*(manual_action)\n",
    "        Then scaled by self.max_speed, applied to self.dot_pos.\n",
    "\n",
    "        Reward = negative distance to the perfect line (start->goal),\n",
    "                 +10 if within threshold of the goal.\n",
    "        \"\"\"\n",
    "        if self.done_flag:\n",
    "            # If called after done, just return the current state\n",
    "            return self._get_obs(), 0.0, True, {}\n",
    "\n",
    "        self.current_step += 1\n",
    "        if self.current_step >= self.max_steps:\n",
    "            self.done_flag = True\n",
    "            return self._get_obs(), 0.0, True, {}\n",
    "\n",
    "        # If no manual_action is provided, we assume \"training mode\"\n",
    "        if manual_action is None:\n",
    "            manual_action = self._simulate_random_human()\n",
    "\n",
    "        # Weighted sum\n",
    "        combined_x = self.gamma * agent_action[0] + (1 - self.gamma) * manual_action[0]\n",
    "        combined_y = self.gamma * agent_action[1] + (1 - self.gamma) * manual_action[1]\n",
    "\n",
    "        # Scale by max_speed\n",
    "        dx = float(combined_x) * self.max_speed\n",
    "        dy = float(combined_y) * self.max_speed\n",
    "\n",
    "        new_x = np.clip(self.dot_pos[0] + dx, 0, self.width)\n",
    "        new_y = np.clip(self.dot_pos[1] + dy, 0, self.height)\n",
    "        self.dot_pos = np.array([new_x, new_y], dtype=np.float32)\n",
    "\n",
    "        # Reward shaping\n",
    "        dist_line = self._distance_to_line(self.start_pos, self.goal_pos, self.dot_pos)\n",
    "        reward = -0.01 * dist_line\n",
    "\n",
    "        dist_to_goal = np.linalg.norm(self.dot_pos - self.goal_pos)\n",
    "        if dist_to_goal < self.goal_distance_threshold:\n",
    "            reward += 10.0\n",
    "            self.done_flag = True\n",
    "\n",
    "        return self._get_obs(), reward, self.done_flag, {}\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        if not self.render_mode:\n",
    "            return\n",
    "        # Standard Pygame event check so the window doesn't freeze\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                sys.exit()\n",
    "\n",
    "        self.screen.fill((255, 255, 255))\n",
    "\n",
    "        # Perfect path line\n",
    "        pygame.draw.line(\n",
    "            self.screen, (200, 200, 200),\n",
    "            (int(self.start_pos[0]), int(self.start_pos[1])),\n",
    "            (int(self.goal_pos[0]), int(self.goal_pos[1])),\n",
    "            2\n",
    "        )\n",
    "        # Goal\n",
    "        pygame.draw.circle(\n",
    "            self.screen,\n",
    "            (255, 0, 0),\n",
    "            (int(self.goal_pos[0]), int(self.goal_pos[1])),\n",
    "            8\n",
    "        )\n",
    "        # Dot\n",
    "        pygame.draw.circle(\n",
    "            self.screen,\n",
    "            (0, 0, 255),\n",
    "            (int(self.dot_pos[0]), int(self.dot_pos[1])),\n",
    "            int(self.dot_radius)\n",
    "        )\n",
    "\n",
    "        text = self.font.render(f\"Step: {self.current_step}\", True, (0,0,0))\n",
    "        self.screen.blit(text, (10, 10))\n",
    "\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(60)\n",
    "\n",
    "    def close(self):\n",
    "        if self.render_mode:\n",
    "            pygame.quit()\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return np.array([\n",
    "            self.dot_pos[0],\n",
    "            self.dot_pos[1],\n",
    "            self.goal_pos[0],\n",
    "            self.goal_pos[1]\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "    def _simulate_random_human(self):\n",
    "        \"\"\"\n",
    "        For training: a simple random direction to emulate a 'confused' or random user.\n",
    "        Could be replaced by a more realistic noisy action toward the goal.\n",
    "        \"\"\"\n",
    "        dx = np.random.uniform(-1, 1)\n",
    "        dy = np.random.uniform(-1, 1)\n",
    "        # Optional mild normalization\n",
    "        mag = math.hypot(dx, dy)\n",
    "        if mag > 1e-6:\n",
    "            dx /= mag\n",
    "            dy /= mag\n",
    "        # Add small noise\n",
    "        dx += np.random.normal(0, self.simulate_human_noise)\n",
    "        dy += np.random.normal(0, self.simulate_human_noise)\n",
    "        # Re-normalize if we want to keep it in [-1,1]\n",
    "        mag = math.hypot(dx, dy)\n",
    "        if mag > 1e-6:\n",
    "            dx /= mag\n",
    "            dy /= mag\n",
    "        return np.array([dx, dy], dtype=np.float32)\n",
    "\n",
    "    def _distance_to_line(self, start, end, point):\n",
    "        line_len = np.linalg.norm(end - start)\n",
    "        if line_len < 1e-9:\n",
    "            return np.linalg.norm(point - start)\n",
    "        t = np.dot(point - start, end - start) / (line_len**2)\n",
    "        t = max(0.0, min(1.0, t))\n",
    "        proj = start + t * (end - start)\n",
    "        return np.linalg.norm(point - proj)\n",
    "\n",
    "\n",
    "def run_manual_control(model):\n",
    "    \"\"\"\n",
    "    Let you manually control the environment (via arrow keys).\n",
    "    The PPO model also outputs an action each step.\n",
    "    We do a weighted sum of (model_action, your_action).\n",
    "    This loop continues until you close the window.\n",
    "    \"\"\"\n",
    "    env = CursorControlEnv(render_mode=True)  # GUI\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0.0\n",
    "\n",
    "    print(\"\\nManual control + PPO running!\")\n",
    "    print(\"Use arrow keys to move. Window will remain open until you close it.\")\n",
    "\n",
    "    while True:  # We'll let you run multiple episodes in a row\n",
    "        # If episode is done, reset\n",
    "        if done:\n",
    "            print(f\"Episode finished. Total reward: {total_reward:.2f}\")\n",
    "            obs = env.reset()\n",
    "            done = False\n",
    "            total_reward = 0.0\n",
    "\n",
    "        # 1) Check for user input from pygame\n",
    "        #    We'll convert arrow keys to a [-1,1] action\n",
    "        #    e.g. left arrow => (-1, 0), right arrow => (+1, 0)\n",
    "        pygame.event.pump()  # to get current key states\n",
    "        keys = pygame.key.get_pressed()\n",
    "        user_dx = 0.0\n",
    "        user_dy = 0.0\n",
    "        if keys[pygame.K_LEFT]:\n",
    "            user_dx -= 1.0\n",
    "        if keys[pygame.K_RIGHT]:\n",
    "            user_dx += 1.0\n",
    "        if keys[pygame.K_UP]:\n",
    "            user_dy -= 1.0\n",
    "        if keys[pygame.K_DOWN]:\n",
    "            user_dy += 1.0\n",
    "\n",
    "        # If you want to exit, press ESC or close the window\n",
    "        if keys[pygame.K_ESCAPE]:\n",
    "            break\n",
    "\n",
    "        # Normalize if needed\n",
    "        mag = math.hypot(user_dx, user_dy)\n",
    "        if mag > 1e-6:\n",
    "            user_dx /= mag\n",
    "            user_dy /= mag\n",
    "\n",
    "        manual_action = np.array([user_dx, user_dy], dtype=np.float32)\n",
    "\n",
    "        # 2) Get the PPO model's action\n",
    "        model_action, _states = model.predict(obs, deterministic=True)\n",
    "\n",
    "        # 3) Step the environment with both\n",
    "        obs, reward, done, info = env.step(\n",
    "            agent_action=model_action,\n",
    "            manual_action=manual_action\n",
    "        )\n",
    "        total_reward += reward\n",
    "\n",
    "        # 4) Render\n",
    "        env.render()\n",
    "\n",
    "    env.close()\n",
    "    print(\"Closed manual control window. Exiting...\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    1) Train a PPO model in a headless environment (where 'human' is random).\n",
    "    2) Afterwards, let the user manually control the dot (arrow keys) combined\n",
    "       with the model’s action in real time.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) TRAINING (headless)\n",
    "    train_env = CursorControlEnv(render_mode=False)\n",
    "    vec_env = DummyVecEnv([lambda: train_env])\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\",\n",
    "        vec_env,\n",
    "        verbose=1,\n",
    "        n_steps=1024,\n",
    "        batch_size=64,\n",
    "        learning_rate=3e-4,\n",
    "        gamma=0.99,\n",
    "        ent_coef=0.01\n",
    "    )\n",
    "\n",
    "    print(\"Training PPO for 100,000 steps (this may take a while)...\")\n",
    "    model.learn(total_timesteps=100_000)\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "    # 2) MANUAL CONTROL DEMO\n",
    "    run_manual_control(model)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
