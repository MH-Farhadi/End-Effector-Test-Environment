{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da66c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "train_dqn_gamma.py\n",
    "──────────────────\n",
    "Pre‑trains a *discrete* γ‑controller with DQN for the cursor‑control\n",
    "environment.  The controller maps a 10‑D state  →  γ ∈ {0.0, 0.05, …, 1.0}.\n",
    "Reward shaping is identical to the PPO setup in BRACE (§A.2).\n",
    "\n",
    "Output\n",
    "------\n",
    "  dqn_gamma.pt      # torch state‑dict (q_net)\n",
    "  dqn_gamma.zip     # full SB3 model            (for inspection)\n",
    "\n",
    "Requirements\n",
    "------------\n",
    "pip install stable-baselines3==2.2.1 torch numpy gymnasium\n",
    "\"\"\"\n",
    "\n",
    "import os, math, argparse, pathlib\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import torch\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 1)  Import the same cursor environment you used before\n",
    "# ----------------------------------------------------------------------\n",
    "from demo_arbitration_env import DemoArbitrationEnv     # 10‑D obs, γ∈[-1,1]\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 2)  Wrap env:  discrete index 0..K‑1  →  gamma∈[-1,1]\n",
    "# ----------------------------------------------------------------------\n",
    "class GammaDiscretizer(gym.ActionWrapper):\n",
    "    def __init__(self, env: gym.Env, n_bins: int = 21):\n",
    "        super().__init__(env)\n",
    "        self.n_bins = n_bins\n",
    "        self.action_space = spaces.Discrete(n_bins)\n",
    "        # pre‑compute mapping\n",
    "        self.disc2cont = np.linspace(-1.0, 1.0, n_bins, dtype=np.float32)\n",
    "\n",
    "    def action(self, act_idx):\n",
    "        cont = np.array([self.disc2cont[int(act_idx)]], dtype=np.float32)\n",
    "        return cont                                 # original env expects np.array([γ_raw])\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 3)  Reward shaping (identical to PPO training in BRACE appendix)\n",
    "# ----------------------------------------------------------------------\n",
    "# Already implemented inside DemoArbitrationEnv.step()\n",
    "# -> we can reuse it directly.\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 4)  Training function\n",
    "# ----------------------------------------------------------------------\n",
    "def train_dqn(\n",
    "    total_timesteps: int = 1_000_000,\n",
    "    n_bins: int = 21,\n",
    "    save_dir: str = \"dqn_gamma_out\",\n",
    "):\n",
    "    save_path = pathlib.Path(save_dir)\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    base_env = DemoArbitrationEnv(visualize=False)\n",
    "    env = DummyVecEnv([lambda: GammaDiscretizer(base_env, n_bins)])\n",
    "\n",
    "    policy_kwargs = dict(net_arch=[256, 256], activation_fn=torch.nn.ReLU)\n",
    "    model = DQN(\n",
    "        policy=\"MlpPolicy\",\n",
    "        env=env,\n",
    "        learning_rate=2.5e-4,\n",
    "        buffer_size=500_000,\n",
    "        learning_starts=50_000,\n",
    "        batch_size=1024,\n",
    "        tau=0.005,\n",
    "        gamma=0.99,\n",
    "        target_update_interval=5_000,\n",
    "        exploration_fraction=0.3,\n",
    "        exploration_final_eps=0.05,\n",
    "        train_freq=(4, \"step\"),\n",
    "        policy_kwargs=policy_kwargs,\n",
    "        verbose=1,\n",
    "        tensorboard_log=str(save_path / \"tb\"),\n",
    "        seed=0,\n",
    "        device=\"auto\",\n",
    "    )\n",
    "    model.learn(total_timesteps=total_timesteps, progress_bar=True)\n",
    "    model.save(save_path / \"dqn_gamma\")           # full SB3 model\n",
    "\n",
    "    # Export only the Q‑network state‑dict (smaller, easy to load)\n",
    "    torch.save(model.q_net.state_dict(), save_path / \"dqn_gamma.pt\")\n",
    "    print(\"✅  saved to\", save_path.absolute())\n",
    "    env.close()\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 5)  CLI\n",
    "# ----------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--steps\", type=int, default=1_000_000, help=\"training frames\")\n",
    "    ap.add_argument(\"--bins\", type=int, default=21,          help=\"#discrete γ bins\")\n",
    "    ap.add_argument(\"--out\",  type=str, default=\"dqn_gamma_out\", help=\"output folder\")\n",
    "    args = ap.parse_args()\n",
    "    train_dqn(args.steps, args.bins, args.out)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
