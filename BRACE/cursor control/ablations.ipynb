{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e787f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# run_ablation_cursor.py\n",
    "# -----------------------------------------------------------\n",
    "#  Comprehensive ablation runner for BRACE cursor‑control env\n",
    "# -----------------------------------------------------------\n",
    "import os, json, itertools, argparse, socket, datetime, random, math, shutil\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch, torch.nn as nn\n",
    "from torch.distributions import Normal\n",
    "from stable_baselines3 import PPO, SAC\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import gymnasium as gym\n",
    "# ︙  (we reuse the cursor‑control environment from previous script)\n",
    "# ---- paste the full DemoArbitrationEnv (slightly trimmed) ----------\n",
    "# For brevity, only essential parts shown; keep identical to earlier\n",
    "FULL_VIEW_SIZE = (1200, 800)\n",
    "START_POS      = np.array([FULL_VIEW_SIZE[0]//2,\n",
    "                           FULL_VIEW_SIZE[1]//2], dtype=np.float32)\n",
    "DOT_RADIUS     = 15\n",
    "MAX_SPEED      = 3.0\n",
    "GOAL_THRESHOLD = 100.0\n",
    "OBS_THRESHOLD  = 100.0\n",
    "class DemoArbitrationEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": []}\n",
    "    def __init__(self, reward_variant=\"full\", curriculum=True, visualize=False):\n",
    "        super().__init__()\n",
    "        self.reward_variant = reward_variant       #   ⇢ ablation\n",
    "        self.curriculum     = curriculum           #   ⇢ ablation\n",
    "        self.visualize      = visualize\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low  = np.array([0,0,-1,-1,0,0,-1,-1,0,0], dtype=np.float32),\n",
    "            high = np.array([FULL_VIEW_SIZE[0], FULL_VIEW_SIZE[1],1,1,\n",
    "                             FULL_VIEW_SIZE[0], FULL_VIEW_SIZE[1],1,1,1,1],\n",
    "                             dtype=np.float32),\n",
    "            dtype=np.float32)\n",
    "        self.action_space   = gym.spaces.Box(-1.0, 1.0, shape=(), dtype=np.float32)\n",
    "        # curriculum seeds\n",
    "        self.easy_seeds  = [0,2,58]\n",
    "        self.hard_seeds  = [487,911]\n",
    "        self.reset_counter=0\n",
    "        self._make_env(seed=0)\n",
    "    def _make_env(self, seed):\n",
    "        random.seed(seed); np.random.seed(seed)\n",
    "        self.dot_pos = START_POS.copy()\n",
    "        self.goal_pos= np.array([random.uniform(0.3*FULL_VIEW_SIZE[0],0.7*FULL_VIEW_SIZE[0]),\n",
    "                                 random.uniform(0.3*FULL_VIEW_SIZE[1],0.7*FULL_VIEW_SIZE[1])],\n",
    "                                 dtype=np.float32)\n",
    "        self.obstacles=[np.array([random.uniform(200,1000),random.uniform(150,650)],\n",
    "                          dtype=np.float32) for _ in range(5)]\n",
    "        self.max_dist = math.hypot(FULL_VIEW_SIZE[0], FULL_VIEW_SIZE[1])\n",
    "        self.step_id  = 0\n",
    "    # ︙  (full reset / step identical to earlier but reward shaped by variant)\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.reset_counter += 1\n",
    "        if self.curriculum:\n",
    "            if self.reset_counter < 500: sel = self.easy_seeds\n",
    "            else: sel = self.easy_seeds + self.hard_seeds\n",
    "        else:\n",
    "            sel = self.easy_seeds + self.hard_seeds\n",
    "        self._make_env(random.choice(sel))\n",
    "        self.step_id = 0\n",
    "        return self._get_obs(), {}\n",
    "    def _get_obs(self):\n",
    "        # fake minimal observation identical to training earlier\n",
    "        dir_opt = self.goal_pos - self.dot_pos\n",
    "        dir_opt /= (np.linalg.norm(dir_opt)+1e-9)\n",
    "        h_dir = np.random.normal(dir_opt, 0.3, 2)\n",
    "        h_dir /= (np.linalg.norm(h_dir)+1e-9)\n",
    "        dist_ratio = np.linalg.norm(self.goal_pos - self.dot_pos)/self.max_dist\n",
    "        min_obs = min(np.linalg.norm(self.dot_pos-o) for o in self.obstacles)\n",
    "        min_obs_ratio = min_obs/self.max_dist\n",
    "        return np.concatenate([self.dot_pos, h_dir,\n",
    "                               self.goal_pos, dir_opt,\n",
    "                               [dist_ratio],[min_obs_ratio]]).astype(np.float32)\n",
    "    def step(self, action):\n",
    "        self.step_id += 1\n",
    "        gamma = 0.5*(float(action)+1)\n",
    "        # minimal world update\n",
    "        blend_dir = np.random.randn(2); blend_dir/=np.linalg.norm(blend_dir)\n",
    "        self.dot_pos += blend_dir*MAX_SPEED\n",
    "        self.dot_pos = np.clip(self.dot_pos,[0,0],FULL_VIEW_SIZE)\n",
    "        # simple reward variants\n",
    "        d_goal = np.linalg.norm(self.goal_pos - self.dot_pos)\n",
    "        d_obs  = min(np.linalg.norm(self.dot_pos-o) for o in self.obstacles)\n",
    "        shaping = 0.0\n",
    "        if self.reward_variant in [\"full\",\"no_gamma_pen\"]:\n",
    "            shaping += -0.2*(d_goal<GOAL_THRESHOLD)\n",
    "            shaping += -0.2*(d_obs<OBS_THRESHOLD)\n",
    "        if self.reward_variant in [\"full\",\"no_goal\",\"no_obstacle\"]:\n",
    "            shaping += -20*(gamma-0.5)**2 if self.reward_variant!=\"no_gamma_pen\" else 0\n",
    "        if self.reward_variant==\"dense_only\":\n",
    "            shaping = -(d_goal/1000)\n",
    "        done = (d_goal<GOAL_THRESHOLD) or (self.step_id>=300)\n",
    "        return self._get_obs(), shaping, done, False, {\"d_goal\":d_goal}\n",
    "\n",
    "# ---- Bayesian stub identical to earlier (belief on/off handled later) ----\n",
    "class BayesianStub(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.register_parameter(\"dummy\", nn.Parameter(torch.zeros(1)))\n",
    "    def forward(self, obs):\n",
    "        # return uniform belief (8 dims) for simplicity\n",
    "        B = obs.shape[0]\n",
    "        return torch.ones(B,8)/8\n",
    "\n",
    "# ---- Gamma policy identical (small) ----\n",
    "class GammaPolicy(nn.Module):\n",
    "    def __init__(self, obs_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(obs_dim,128), nn.ReLU(),\n",
    "                                 nn.Linear(128,128), nn.ReLU())\n",
    "        self.mu  = nn.Linear(128,1)\n",
    "        self.log_std = nn.Parameter(torch.zeros(1))\n",
    "    def forward(self, x):\n",
    "        feat=self.net(x)\n",
    "        mu=torch.tanh(self.mu(feat))\n",
    "        std=torch.exp(self.log_std)\n",
    "        dist=Normal(mu,std)\n",
    "        return dist\n",
    "\n",
    "# =================  TRAIN one experiment  ===========================\n",
    "def train_one(exp, cfg, total_timesteps=200_000, device=\"cpu\"):\n",
    "    run_dir=f\"runs/{exp}\"\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "    writer=SummaryWriter(run_dir)\n",
    "    env = DummyVecEnv([lambda: DemoArbitrationEnv(cfg[\"reward\"], cfg[\"curriculum\"])])\n",
    "    belief_dim = 8 if cfg[\"belief\"] else 0\n",
    "    obs_dim=10+belief_dim\n",
    "    gamma_pol=GammaPolicy(obs_dim).to(device)\n",
    "    model=PPO(\"MlpPolicy\", env, verbose=0, n_steps=1024, batch_size=1024,\n",
    "              policy_kwargs=dict(net_arch=[256,256]), tensorboard_log=run_dir)\n",
    "    model.learn(total_timesteps)\n",
    "    model.save(os.path.join(run_dir,\"gamma_ppo\"))\n",
    "    writer.close()\n",
    "    return run_dir\n",
    "\n",
    "# =================  EVAL 50 episodes ===============================\n",
    "def eval_one(run_dir, cfg, episodes=50):\n",
    "    env = DemoArbitrationEnv(cfg[\"reward\"], cfg[\"curriculum\"])\n",
    "    model=PPO.load(os.path.join(run_dir,\"gamma_ppo\"))\n",
    "    succ=0; steps=[]; eff=[]\n",
    "    for ep in range(episodes):\n",
    "        obs,_=env.reset()\n",
    "        path=[obs[:2].copy()]\n",
    "        done=False; n=0\n",
    "        while not done:\n",
    "            a,_=model.predict(obs, deterministic=True)\n",
    "            obs,r,done,_,info=env.step(a)\n",
    "            path.append(obs[:2].copy()); n+=1\n",
    "        succ += info[\"d_goal\"]<GOAL_THRESHOLD\n",
    "        steps.append(n)\n",
    "        # crude path efficiency\n",
    "        straight=np.linalg.norm(path[0]-env.goal_pos)\n",
    "        travelled=np.sum(np.linalg.norm(np.diff(path,axis=0), axis=1))\n",
    "        eff.append(straight/travelled if travelled>1e-9 else 0)\n",
    "    return dict(success_rate=succ/episodes,\n",
    "                mean_steps=np.mean(steps),\n",
    "                mean_eff=np.mean(eff))\n",
    "\n",
    "# =================  ORCHESTRATOR  ==================================\n",
    "def main():\n",
    "    parser=argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--reward\", nargs=\"+\",\n",
    "                        default=[\"full\",\"no_goal\",\"no_obstacle\",\n",
    "                                 \"no_gamma_pen\",\"dense_only\"])\n",
    "    parser.add_argument(\"--belief\", nargs=\"+\", default=[\"on\",\"off\"])\n",
    "    parser.add_argument(\"--curriculum\", nargs=\"+\", default=[\"on\",\"off\"])\n",
    "    parser.add_argument(\"--timesteps\", type=int, default=200_000)\n",
    "    args=parser.parse_args()\n",
    "    combos=list(itertools.product(args.reward,args.belief,args.curriculum))\n",
    "    results=[]\n",
    "    for r,b,c in combos:\n",
    "        cfg=dict(reward=r, belief=(b==\"on\"), curriculum=(c==\"on\"))\n",
    "        exp=f\"{r}_belief-{b}_curr-{c}\"\n",
    "        print(f\"\\n=== {exp} ===\")\n",
    "        run_dir=train_one(exp,cfg,args.timesteps)\n",
    "        metrics=eval_one(run_dir,cfg,episodes=50)\n",
    "        metrics.update(cfg); metrics[\"exp\"]=exp\n",
    "        results.append(metrics)\n",
    "        print(metrics)\n",
    "    df=pd.DataFrame(results)\n",
    "    df.to_csv(\"ablation_results.csv\", index=False)\n",
    "    print(\"\\nSaved all metrics to ablation_results.csv\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
