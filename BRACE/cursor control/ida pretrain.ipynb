{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a0d9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "train_ida.py  –  Pre‑trains the IDA‑controlled ‘gamma mode’ for the cursor‑control environment\n",
    "=============================================================================================\n",
    "Steps\n",
    "-----\n",
    "1.  Train a soft‑actor‑critic (SAC) expert that **fully observes** the goal (Algorithm 1, line 3 of the\n",
    "    NeurIPS‑24 IDA paper ‹Shared Autonomy with IDA› :contentReference[oaicite:0]{index=0}).\n",
    "\n",
    "2.  Roll out the expert for N transitions, **mask the goal**, and write a replay buffer\n",
    "    D = {(s̃, aᴱ)} – these are goal‑agnostic demonstrations (§ 3.3 in the paper).\n",
    "\n",
    "3.  Train a *denoising‑diffusion* copilot π_c(a_c | s̃, a_p, t) on D to imitate expert actions\n",
    "    from *noised* inputs (equation 2).\n",
    "\n",
    "4.  Export the expert’s *Q*‑critic as a standalone Torch module; at run‑time we compute\n",
    "    the copilot‑advantage   \n",
    "        A(s̃, a_c, a_p) = sign(Q(s̃, a_c) − Q(s̃, a_p))                (eqs. 5–8)  \n",
    "    and set  \n",
    "        γ_IDA = 1  if A == +1   else 0              # binary intervention\n",
    "\n",
    "No hyper‑parameter γ needs tuning – the intervention function\n",
    "guarantees J(π_IDA) ≥ max(J(π_pilot), J(π_copilot)) (Theorem 1).\n",
    "\n",
    "Dependencies\n",
    "------------\n",
    "- gymnasium               >= 0.29\n",
    "- stable‑baselines3       >= 2.2    (SAC implementation)\n",
    "- torch                   >= 2.1\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import argparse, os, json, time, math, random, pathlib, copy, pickle\n",
    "from typing import Tuple\n",
    "\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1)  CURSOR‑CONTROL ENV  (imported from your previous code)\n",
    "# -----------------------------------------------------------------------------\n",
    "from demo_arbitration_env import DemoArbitrationEnv          # <- make sure this is importable\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2)  EXPERT (SAC) TRAINING\n",
    "# -----------------------------------------------------------------------------\n",
    "def train_expert(\n",
    "        total_steps: int = 3_000_000,\n",
    "        seed: int = 0,\n",
    "        save_dir: str = \"ida_outputs/expert\",\n",
    ") -> SAC:\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    env = DummyVecEnv([lambda: DemoArbitrationEnv(visualize=False)])\n",
    "    model = SAC(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        learning_rate=3e-4,\n",
    "        tau=0.005,\n",
    "        gamma=0.99,\n",
    "        buffer_size=1_000_000,\n",
    "        batch_size=512,\n",
    "        policy_kwargs=dict(net_arch=[256, 256, 256]),\n",
    "        verbose=1,\n",
    "        seed=seed,\n",
    "        device=\"auto\",\n",
    "    )\n",
    "    model.learn(total_timesteps=total_steps, progress_bar=True)\n",
    "    model.save(os.path.join(save_dir, \"sac_expert\"))\n",
    "    # also save just the Q‑functions for inference‑time advantage\n",
    "    torch.save(\n",
    "        {\n",
    "            \"critic1\": model.critic.qf1.state_dict(),\n",
    "            \"critic2\": model.critic.qf2.state_dict(),\n",
    "            \"obs_norm\": copy.deepcopy(model.policy.actor).state_dict(),  # reuse normaliser\n",
    "        },\n",
    "        os.path.join(save_dir, \"q_heads.pt\"),\n",
    "    )\n",
    "    env.close()\n",
    "    return model\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3)  COLLECT GOAL‑MASKED DEMONSTRATIONS\n",
    "# -----------------------------------------------------------------------------\n",
    "def collect_demonstrations(\n",
    "        model: SAC,\n",
    "        n_transitions: int = 1_000_000,\n",
    "        save_dir: str = \"ida_outputs/demos\",\n",
    ") -> str:\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    env = DemoArbitrationEnv(visualize=False)\n",
    "    # we only store goal‑agnostic obs; the env’s _get_obs already outputs s̃\n",
    "    states, acts = [], []\n",
    "    obs, _ = env.reset()\n",
    "    for _ in range(n_transitions):\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        states.append(obs.copy())\n",
    "        acts.append(action.copy())\n",
    "        obs, _, term, trunc, _ = env.step(action)\n",
    "        if term or trunc:\n",
    "            obs, _ = env.reset()\n",
    "    arr_s = np.stack(states).astype(np.float32)\n",
    "    arr_a = np.stack(acts).astype(np.float32)\n",
    "    np.savez_compressed(os.path.join(save_dir, \"expert_demos.npz\"), s=arr_s, a=arr_a)\n",
    "    env.close()\n",
    "    return os.path.join(save_dir, \"expert_demos.npz\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4)  SIMPLE DDPM COPILOT\n",
    "#     (lightweight: cosine schedule, linear β_t, MLP ε‑predictor)\n",
    "# -----------------------------------------------------------------------------\n",
    "class EpsPredNet(nn.Module):\n",
    "    def __init__(self, obs_dim: int, act_dim: int, hidden: int = 256):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim + act_dim + 1, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, act_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # t ∈ [0,1] as float\n",
    "        return self.net(torch.cat([x, t.unsqueeze(-1)], -1))\n",
    "\n",
    "class DiffusionCopilot:\n",
    "    def __init__(self, obs_dim: int, act_dim: int,\n",
    "                 timesteps: int = 1000,\n",
    "                 beta_start: float = 1e-4,\n",
    "                 beta_end: float = 0.02,\n",
    "                 lr: float = 3e-4,\n",
    "                 device: str = \"auto\"):\n",
    "        self.timesteps = timesteps\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() and device == \"auto\" else \"cpu\")\n",
    "        self.betas = torch.linspace(beta_start, beta_end, timesteps, device=self.device)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        self.net = EpsPredNet(obs_dim, act_dim).to(self.device)\n",
    "        self.optim = optim.Adam(self.net.parameters(), lr=lr)\n",
    "\n",
    "    # q sample: add noise at arbitrary timestep t\n",
    "    def q_sample(self, x0, t, noise):\n",
    "        acp = self.alphas_cumprod[t].unsqueeze(1)\n",
    "        return torch.sqrt(acp) * x0 + torch.sqrt(1 - acp) * noise\n",
    "\n",
    "    def train_loop(self, loader: DataLoader, epochs: int = 10):\n",
    "        self.net.train()\n",
    "        for epoch in range(epochs):\n",
    "            for obs, act in loader:\n",
    "                obs, act = obs.to(self.device), act.to(self.device)\n",
    "                noise = torch.randn_like(act)\n",
    "                t = torch.randint(0, self.timesteps, (act.size(0),), device=self.device)\n",
    "                noisy = self.q_sample(act, t, noise)\n",
    "                pred_eps = self.net(torch.cat([obs, noisy], dim=1), t.float() / self.timesteps)\n",
    "                loss = nn.functional.mse_loss(pred_eps, noise)\n",
    "                self.optim.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optim.step()\n",
    "            if epoch % 5 == 0:\n",
    "                print(f\"[DDPM] epoch {epoch:03d} | loss={loss.item():.4f}\")\n",
    "\n",
    "    # single reverse step (DDIM‑like ancestral)\n",
    "    def predict(self, obs, a_p, steps: int = 50):\n",
    "        self.net.eval()\n",
    "        obs = torch.tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "        x = torch.tensor(a_p, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            for i in reversed(range(steps)):\n",
    "                t = torch.full((1,), i, device=self.device)\n",
    "                eps = self.net(torch.cat([obs, x], dim=1), t.float() / self.timesteps)\n",
    "                alpha = self.alphas[i]\n",
    "                alpha_bar = self.alphas_cumprod[i]\n",
    "                x = (1/torch.sqrt(alpha)) * (x - (1-alpha)/torch.sqrt(1-alpha_bar) * eps)\n",
    "                if i > 0:\n",
    "                    noise = torch.randn_like(x)\n",
    "                    beta = self.betas[i]\n",
    "                    x += torch.sqrt(beta) * noise\n",
    "        return x.squeeze(0).cpu().numpy()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# UTIL – tiny wrapper to expose critic‑Q for arbitrary (s,a)\n",
    "# -----------------------------------------------------------------------------\n",
    "class ExpertQ(nn.Module):\n",
    "    def __init__(self, saved: dict, obs_dim: int, act_dim: int):\n",
    "        super().__init__()\n",
    "        self.q1 = SAC.load(\"dummy\").critic.qf1  # placeholder to get architecture\n",
    "        # overwrite weights afterwards\n",
    "    # (for brevity we omit full code – see README)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# MAIN\n",
    "# -----------------------------------------------------------------------------\n",
    "def main(args):\n",
    "    out_root = pathlib.Path(args.out_dir)\n",
    "    out_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ---------- 1) Expert ----------------------------------------------------\n",
    "    if not (out_root / \"expert\" / \"sac_expert.zip\").exists():\n",
    "        expert = train_expert(args.expert_steps, save_dir=out_root / \"expert\")\n",
    "    else:\n",
    "        expert = SAC.load(out_root / \"expert\" / \"sac_expert\")\n",
    "\n",
    "    # ---------- 2) Demonstrations -------------------------------------------\n",
    "    demo_file = out_root / \"demos\" / \"expert_demos.npz\"\n",
    "    if not demo_file.exists():\n",
    "        demo_file = collect_demonstrations(expert,\n",
    "                                           n_transitions=args.demo_transitions,\n",
    "                                           save_dir=out_root / \"demos\")\n",
    "    demos = np.load(demo_file)\n",
    "    obs, acts = demos[\"s\"], demos[\"a\"]\n",
    "    dataset = TensorDataset(torch.from_numpy(obs), torch.from_numpy(acts))\n",
    "    loader = DataLoader(dataset, batch_size=2048, shuffle=True, drop_last=True)\n",
    "\n",
    "    # --------- 3) Train Diffusion Copilot -----------------------------------\n",
    "    obs_dim, act_dim = obs.shape[1], acts.shape[1]\n",
    "    copilot = DiffusionCopilot(obs_dim, act_dim,\n",
    "                               timesteps=1000,\n",
    "                               lr=args.ddpm_lr,\n",
    "                               device=\"auto\")\n",
    "    copilot.train_loop(loader, epochs=args.ddpm_epochs)\n",
    "    torch.save(copilot.net.state_dict(), out_root / \"copilot_ddpm.pt\")\n",
    "\n",
    "    # --------- 4) Export quick‑config ---------------------------------------\n",
    "    cfg = dict(\n",
    "        obs_dim=int(obs_dim),\n",
    "        act_dim=int(act_dim),\n",
    "        timesteps=1000,\n",
    "        q_heads=str(out_root / \"expert\" / \"q_heads.pt\"),\n",
    "        ddpm_weights=str(out_root / \"copilot_ddpm.pt\"),\n",
    "    )\n",
    "    json.dump(cfg, open(out_root / \"ida_config.json\", \"w\"), indent=2)\n",
    "    print(\"\\n✅  All artefacts written to\", out_root.resolve())\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    p = argparse.ArgumentParser()\n",
    "    p.add_argument(\"--expert_steps\", type=int, default=3_000_000,\n",
    "                   help=\"SAC training steps for the expert\")\n",
    "    p.add_argument(\"--demo_transitions\", type=int, default=1_000_000,\n",
    "                   help=\"#state‑action pairs to record from the expert\")\n",
    "    p.add_argument(\"--ddpm_epochs\", type=int, default=200,\n",
    "                   help=\"gradient steps over the demo set\")\n",
    "    p.add_argument(\"--ddpm_lr\", type=float, default=3e-4,\n",
    "                   help=\"learning rate for diffusion ε‑network\")\n",
    "    p.add_argument(\"--out_dir\", type=str, default=\"ida_outputs\")\n",
    "    args = p.parse_args()\n",
    "    main(args)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
