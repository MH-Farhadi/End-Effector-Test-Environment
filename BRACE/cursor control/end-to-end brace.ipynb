{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817f3b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# train_brace_integrated.py\n",
    "# -----------------------------------------------------------\n",
    "#  End‑to‑end BRACE training:   Bayesian ⇆ γ‑actor‑critic\n",
    "#  Implements Algorithm 1 (paper §3.1 + Appendix B.1)\n",
    "# -----------------------------------------------------------\n",
    "import os, math, random, argparse, collections, itertools, time\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torch.distributions import Normal, Categorical\n",
    "\n",
    "from stable_baselines3 import SAC            # to load expert\n",
    "from gymnasium import spaces\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "FULL   = torch.tensor([1200., 800.], device=DEVICE)\n",
    "MAX_SPEED = 5.0\n",
    "DOT_R, OBS_R, TGT_R = 14., 10., 9.\n",
    "\n",
    "# ===========================================================\n",
    "#  1.  Utility geometry / helpers\n",
    "# ===========================================================\n",
    "def distance(a, b): return torch.linalg.norm(a-b, ord=2, dim=-1)\n",
    "\n",
    "def potential_field_dir(p, g, obstacles):\n",
    "    \"\"\"differentiable potential‑field direction\"\"\"\n",
    "    att = (g-p); att = att/ (att.norm(dim=-1, keepdim=True)+1e-8)\n",
    "    rep = torch.zeros_like(att)\n",
    "    REP_RAD, REP_GAIN = 90., 3e4\n",
    "    d = g-p            # dummy to keep shape\n",
    "    for o in obstacles:           # obstacles is Tensor[K,2]\n",
    "        d = p-o\n",
    "        dn = d.norm(dim=-1, keepdim=True)+1e-9\n",
    "        mask = (dn < REP_RAD).float()\n",
    "        rep += mask * (d/dn) * (REP_GAIN/(dn**2))\n",
    "    v = att + rep\n",
    "    v = v/(v.norm(dim=-1, keepdim=True)+1e-8)\n",
    "    return v\n",
    "\n",
    "# ===========================================================\n",
    "#  2.  Bayesian goal‑inference module  (learnable)\n",
    "# ===========================================================\n",
    "class BayesianIntent(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements Eqs.(1)–(3).  Parameters to *learn*:\n",
    "        beta, wθ, wd   (all constrained positive via softplus)\n",
    "    Return:\n",
    "        belief          (batch × n_goals)\n",
    "        logp_step       scalar ‑log p(h_t|x_t;θ)  needed for REINFORCE\n",
    "    \"\"\"\n",
    "    def __init__(self, goals):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"goals\", goals)            # (n,2)\n",
    "        self.raw_beta   = nn.Parameter(torch.tensor(2.0))\n",
    "        self.raw_wth    = nn.Parameter(torch.tensor(0.8))\n",
    "        self.raw_wdist  = nn.Parameter(torch.tensor(0.2))\n",
    "        self.n = goals.shape[0]\n",
    "\n",
    "    def positive(self, p):           # softplus\n",
    "        return torch.nn.functional.softplus(p)\n",
    "\n",
    "    def forward(self, cursor, h_vec, prev_belief):\n",
    "        \"\"\"\n",
    "        cursor, h_vec : (B,2)   prev_belief : (B,n) or None\n",
    "        \"\"\"\n",
    "        B = cursor.shape[0]\n",
    "        beta  = self.positive(self.raw_beta)\n",
    "        wth   = self.positive(self.raw_wth)\n",
    "        wdist = self.positive(self.raw_wdist)\n",
    "\n",
    "        # compute angle‑ & distance‑based cost for **each goal**\n",
    "        g = self.goals.to(cursor)                           # (n,2)\n",
    "        to_g   = g[None,:,:] - cursor[:,None,:]             # B×n×2\n",
    "        opt_d  = to_g/ (to_g.norm(dim=-1, keepdim=True)+1e-8)\n",
    "        h_dir  = h_vec/ (h_vec.norm(dim=-1, keepdim=True)+1e-8)\n",
    "        ang    = torch.arccos(torch.clamp((h_dir[:,None,:]*opt_d).sum(-1), -1.,1.))   # B×n\n",
    "        # simple speed cost = |‖h‖ – ‖opt‖| / ‖opt‖ ;  here opt magnitude =1\n",
    "        dist_dev = (h_vec.norm(dim=-1, keepdim=True).expand_as(ang)-1.).abs()\n",
    "        cost   = wth*ang + wdist*dist_dev\n",
    "        log_likelihood = -beta*cost            # B×n  (unnormalised)\n",
    "        ll = torch.exp(log_likelihood - log_likelihood.max(dim=1, keepdim=True).values)\n",
    "        belief = ll if prev_belief is None else prev_belief*ll\n",
    "        belief = belief / belief.sum(dim=1, keepdim=True)\n",
    "\n",
    "        # negative log prob of current observation GIVEN each goal distribution\n",
    "        #   used later for REINFORCE   (score‑function ∇θ log p)\n",
    "        logp = torch.log((belief.detach()*ll).sum(dim=1)+1e-9)   # B\n",
    "        return belief, -logp.mean()\n",
    "\n",
    "# ===========================================================\n",
    "#  3.  γ‑Actor‑Critic  (shared trunk -> π & V)\n",
    "# ===========================================================\n",
    "class GammaActorCritic(nn.Module):\n",
    "    def __init__(self, obs_dim):\n",
    "        super().__init__()\n",
    "        hid = 256\n",
    "        self.shared = nn.Sequential(nn.Linear(obs_dim, hid),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(hid, hid),\n",
    "                                    nn.ReLU())\n",
    "        self.mu_head  = nn.Linear(hid, 1)\n",
    "        self.log_std  = nn.Parameter(torch.zeros(1))\n",
    "        self.v_head   = nn.Linear(hid, 1)\n",
    "\n",
    "    def forward(self, obs, deterministic=False):\n",
    "        feat = self.shared(obs)\n",
    "        mu   = torch.tanh(self.mu_head(feat))          # in [-1,1]\n",
    "        std  = torch.exp(self.log_std).clamp(1e-3,1.0)\n",
    "        dist = Normal(mu, std)\n",
    "        a    = mu if deterministic else torch.tanh(dist.rsample())\n",
    "        logp = dist.log_prob(a).sum(-1, keepdim=True)\n",
    "        v    = self.v_head(feat)\n",
    "        return a, logp, v\n",
    "\n",
    "# ===========================================================\n",
    "#  4.  Expert SAC  (frozen)\n",
    "# ===========================================================\n",
    "EXPERT_PATH = \"expert_sac.zip\"\n",
    "expert = SAC.load(EXPERT_PATH, device=DEVICE)\n",
    "expert.policy.eval()\n",
    "for p in expert.policy.parameters(): p.requires_grad_(False)\n",
    "\n",
    "# helper to get w‑action\n",
    "@torch.no_grad()\n",
    "def expert_w_action(obs_np):\n",
    "    a,_ = expert.predict(obs_np, deterministic=True)\n",
    "    return torch.tensor(a, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "# ===========================================================\n",
    "#  5.  Simulated environment (vectorised)\n",
    "# ===========================================================\n",
    "class SimWorld:\n",
    "    \"\"\"\n",
    "    Vectorised batch of N parallel worlds, purely in torch to keep gradients.\n",
    "    Each world has:\n",
    "        cursor (2) , one active goal, K obstacles\n",
    "    \"\"\"\n",
    "    def __init__(self, batch, n_goals=8, n_obs=4):\n",
    "        self.B = batch\n",
    "        self.n_goals = n_goals\n",
    "        self.n_obs   = n_obs\n",
    "        self.reset()\n",
    "\n",
    "    # ----------------------------------------------\n",
    "    def reset(self):\n",
    "        margin = 80.\n",
    "        self.cursor = FULL/2 + torch.zeros(self.B,2, device=DEVICE)\n",
    "        goals = []\n",
    "        for _ in range(self.B):\n",
    "            gs=[]\n",
    "            while len(gs)<self.n_goals:\n",
    "                p = torch.rand(2, device=DEVICE)* (FULL-2*margin)+margin\n",
    "                if distance(p, self.cursor[0])>260: gs.append(p)\n",
    "            goals.append(torch.stack(gs))\n",
    "        self.goals = torch.stack(goals)            # B×n×2\n",
    "        idx = torch.randint(0,self.n_goals,(self.B,), device=DEVICE)\n",
    "        self.goal_xy = self.goals[torch.arange(self.B), idx]\n",
    "        # obstacles\n",
    "        obs = []\n",
    "        for b in range(self.B):\n",
    "            oz=[]\n",
    "            for g in self.goals[b,:3]:\n",
    "                t  = torch.rand((), device=DEVICE)*0.25+0.55\n",
    "                base = self.cursor[b]+t*(g-self.cursor[b])\n",
    "                perp = torch.tensor([- (g-self.cursor[b])[1],\n",
    "                                      (g-self.cursor[b])[0]])\n",
    "                perp = perp/(torch.norm(perp)+1e-9)\n",
    "                off  = perp* (torch.rand((), device=DEVICE)*20+60)*\\\n",
    "                       (1 if torch.rand(())>0.5 else -1)\n",
    "                oz.append(base+off)\n",
    "            obs.append(torch.stack(oz))\n",
    "        self.obstacles = torch.stack(obs)          # B×n_obs×2\n",
    "        self.steps = torch.zeros(self.B, device=DEVICE, dtype=torch.int32)\n",
    "        self.prev_dist_goal = distance(self.cursor, self.goal_xy)\n",
    "        self.done = torch.zeros(self.B, dtype=torch.bool, device=DEVICE)\n",
    "        return self._state_for_expert()\n",
    "\n",
    "    # ----------------------------------------------\n",
    "    def _state_for_expert(self):\n",
    "        \"\"\"\n",
    "        expert expects 6‑D:  cursor(2) goal(2) d_goal_norm d_obs_norm\n",
    "        \"\"\"\n",
    "        d_goal = distance(self.cursor, self.goal_xy)/torch.linalg.norm(FULL)\n",
    "        d_obs = torch.min(distance(self.cursor[:,None,:], self.obstacles),dim=2).values\n",
    "        d_obs = d_obs/torch.linalg.norm(FULL)\n",
    "        return torch.cat([self.cursor, self.goal_xy,\n",
    "                          d_goal.unsqueeze(1), d_obs.unsqueeze(1)], dim=1).detach().cpu().numpy()\n",
    "\n",
    "    # ----------------------------------------------\n",
    "    def step(self, human_vec, gamma):\n",
    "        \"\"\"\n",
    "        human_vec, gamma : (B,2) , (B,1) tensors\n",
    "        returns reward, done mask\n",
    "        \"\"\"\n",
    "        w_dir = potential_field_dir(self.cursor, self.goal_xy, self.obstacles)\n",
    "        blend = gamma*w_dir + (1-gamma)*human_vec\n",
    "        blend = blend/(blend.norm(dim=-1, keepdim=True)+1e-8)\n",
    "        self.cursor = torch.clip(self.cursor + blend*MAX_SPEED,\n",
    "                                 torch.zeros_like(FULL), FULL)\n",
    "\n",
    "        d_goal = distance(self.cursor, self.goal_xy)\n",
    "        d_obs  = torch.min(distance(self.cursor[:,None,:], self.obstacles),dim=2).values\n",
    "        collide = d_obs < (DOT_R+OBS_R)\n",
    "\n",
    "        # shaped reward (same as expert env but vectorised)\n",
    "        reward = -0.01 + (self.prev_dist_goal - d_goal)*0.5\n",
    "        self.prev_dist_goal = d_goal\n",
    "        reward = reward + torch.where(d_goal < 36.,  2.0, 0.0)\n",
    "        reward = reward + torch.where(collide,        -4.0, 0.0)\n",
    "\n",
    "        done = torch.logical_or(collide, d_goal<36.)\n",
    "        self.done = torch.logical_or(self.done, done)\n",
    "        self.steps += 1\n",
    "        done = torch.logical_or(done, self.steps>=400)\n",
    "        return reward.detach(), done\n",
    "\n",
    "# ===========================================================\n",
    "#  6.  Integrated training loop (Algorithm 1)\n",
    "# ===========================================================\n",
    "def train_brace(batch=64, epochs=6000, unroll=64,\n",
    "                lr_gamma=3e-4, lr_bayes=1e-3):\n",
    "    torch.manual_seed(0)\n",
    "    # one world to get goal tensor for init\n",
    "    tmp = SimWorld(1); goals_tensor = tmp.goals[0].detach().cpu()\n",
    "    bayes = BayesianIntent(goals_tensor).to(DEVICE)\n",
    "    gamma_pi = GammaActorCritic(obs_dim=2+2+1+1+bayes.n).to(DEVICE)  # cursor(2) d_goal d_obs + belief\n",
    "    optim_pi = optim.Adam(gamma_pi.parameters(), lr=lr_gamma)\n",
    "    optim_b  = optim.Adam(bayes.parameters(), lr=lr_bayes)\n",
    "\n",
    "    BUFFER = collections.deque(maxlen=1024)\n",
    "\n",
    "    world = SimWorld(batch)\n",
    "    for epoch in range(epochs):\n",
    "        world.reset()\n",
    "        bayes_belief = torch.ones(batch, bayes.n, device=DEVICE)/bayes.n\n",
    "        logp_b_list, logp_a_list, values, rewards = [],[],[],[]\n",
    "        for t in range(unroll):\n",
    "            # ---------------- simulate human noisy vec -----------------\n",
    "            w_dir = potential_field_dir(world.cursor, world.goal_xy, world.obstacles)\n",
    "            human_vec = w_dir + torch.randn_like(w_dir)*0.3\n",
    "            human_vec = human_vec/(human_vec.norm(dim=-1,keepdim=True)+1e-8)\n",
    "\n",
    "            # Bayesian update\n",
    "            bayes_belief, neglogp_h = bayes(world.cursor, human_vec, bayes_belief)\n",
    "            logp_b_list.append(-neglogp_h)                    # store +log p\n",
    "\n",
    "            # γ‑policy input\n",
    "            d_goal = distance(world.cursor, world.goal_xy)/torch.linalg.norm(FULL)\n",
    "            d_obs  = torch.min(distance(world.cursor[:,None,:], world.obstacles),dim=2).values\n",
    "            d_obs  = d_obs/torch.linalg.norm(FULL)\n",
    "            obs_pi = torch.cat([world.cursor, d_goal.unsqueeze(1), d_obs.unsqueeze(1),\n",
    "                                bayes_belief], dim=1)\n",
    "            a, logp_a, v = gamma_pi(obs_pi)\n",
    "            gamma = 0.5*(a+1.0)           # map [-1,1]→[0,1]\n",
    "\n",
    "            r, done = world.step(human_vec, gamma)\n",
    "            rewards.append(r)\n",
    "            logp_a_list.append(logp_a.squeeze(-1))\n",
    "            values.append(v.squeeze(-1))\n",
    "\n",
    "            if done.all(): break\n",
    "\n",
    "        # stack trajectory tensors   (T,B)\n",
    "        R = torch.stack(rewards)                       # T×B\n",
    "        logp_a = torch.stack(logp_a_list)\n",
    "        logp_b = torch.stack(logp_b_list)\n",
    "        V = torch.stack(values)\n",
    "        with torch.no_grad():\n",
    "            G = torch.zeros_like(R)\n",
    "            running = torch.zeros(batch, device=DEVICE)\n",
    "            for t in reversed(range(R.size(0))):\n",
    "                running = R[t] + 0.99*running\n",
    "                G[t] = running\n",
    "            Adv = (G - V).detach()\n",
    "\n",
    "        # ----------------------------------  losses\n",
    "        policy_loss = -(Adv * logp_a).mean()\n",
    "        value_loss  = 0.5*((G - V)**2).mean()\n",
    "        entropy_reg = -0.0005 * (- (logp_a+1e-9).exp()*logp_a).mean()\n",
    "        loss_pi = policy_loss + value_loss + entropy_reg\n",
    "\n",
    "        #     REINFORCE for Bayesian parameters\n",
    "        loss_b = -(Adv.detach() * logp_b).mean()\n",
    "\n",
    "        optim_pi.zero_grad()\n",
    "        loss_pi.backward()\n",
    "        optim_pi.step()\n",
    "\n",
    "        optim_b.zero_grad()\n",
    "        loss_b.backward()\n",
    "        optim_b.step()\n",
    "\n",
    "        if epoch%100==0:\n",
    "            print(f\"ep {epoch:05d} | reward {R.sum(0).mean():6.2f} \"\n",
    "                  f\"| π_loss {loss_pi.item():6.3f} | B_loss {loss_b.item():6.3f}\")\n",
    "\n",
    "    torch.save({\n",
    "        \"gamma_pi\": gamma_pi.state_dict(),\n",
    "        \"bayes\": bayes.state_dict()\n",
    "    }, \"brace_integrated.pt\")\n",
    "    print(\"✓ integrated model saved to brace_integrated.pt\")\n",
    "\n",
    "# ===========================================================\n",
    "#  7.  Roll‑out viewer (deterministic policy)\n",
    "# ===========================================================\n",
    "def watch_one():\n",
    "    ckpt = torch.load(\"brace_integrated.pt\", map_location=DEVICE)\n",
    "    tmp = SimWorld(1); goals_tensor = tmp.goals[0].detach().cpu()\n",
    "    bayes = BayesianIntent(goals_tensor).to(DEVICE)\n",
    "    gamma_pi = GammaActorCritic(obs_dim=2+2+1+1+bayes.n).to(DEVICE)\n",
    "    gamma_pi.load_state_dict(ckpt[\"gamma_pi\"]); gamma_pi.eval()\n",
    "    bayes.load_state_dict(ckpt[\"bayes\"]); bayes.eval()\n",
    "\n",
    "    world = SimWorld(1)\n",
    "    world.reset()\n",
    "    belief = torch.ones(1, bayes.n, device=DEVICE)/bayes.n\n",
    "    total=0.\n",
    "    for t in range(600):\n",
    "        w_dir = potential_field_dir(world.cursor, world.goal_xy, world.obstacles)\n",
    "        h_vec = w_dir                                           # no extra noise for demo\n",
    "        belief,_ = bayes(world.cursor, h_vec, belief)\n",
    "        d_goal = distance(world.cursor, world.goal_xy)/torch.linalg.norm(FULL)\n",
    "        d_obs  = torch.min(distance(world.cursor[:,None,:], world.obstacles),dim=2).values\n",
    "        d_obs  = d_obs/torch.linalg.norm(FULL)\n",
    "        obs_pi = torch.cat([world.cursor, d_goal.unsqueeze(1), d_obs.unsqueeze(1), belief], dim=1)\n",
    "        with torch.no_grad():\n",
    "            a,_,_ = gamma_pi(obs_pi, deterministic=True)\n",
    "        gamma = 0.5*(a+1.0)\n",
    "        r,_ = world.step(h_vec, gamma)\n",
    "        total += r.item()\n",
    "        if world.done[0]: break\n",
    "    print(\"episode return:\", total)\n",
    "\n",
    "# ===========================================================\n",
    "if __name__ == \"__main__\":\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--watch\", action=\"store_true\")\n",
    "    args = ap.parse_args()\n",
    "    if args.watch:\n",
    "        watch_one()\n",
    "    else:\n",
    "        train_brace()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
