{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db72d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# train_brace_integrated_reacher.py\n",
    "# -----------------------------------------------------------\n",
    "#  Integrated BRACE training (Algorithm 1) for Reacher‑v4\n",
    "# -----------------------------------------------------------\n",
    "import os, math, argparse, collections\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import SAC\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "GOAL_RADIUS = 0.18                         # fixed radius of MuJoCo target\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "#  1. helper: extract fingertip position from env.observation\n",
    "#     Reacher‑v4 obs = 11‑D: [cosθ1, sinθ1, cosθ2, sinθ2, θ̇1, θ̇2,\n",
    "#                             fingertip_x, fingertip_y,\n",
    "#                             target_x, target_y]\n",
    "def split_obs(obs):                         # obs: Tensor (B,11)\n",
    "    tip = obs[:, 6:8]\n",
    "    target = obs[:, 8:10]\n",
    "    return tip, target\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "#  2.  Bayesian goal‑inference module (learnable)\n",
    "class BayesianIntent(nn.Module):\n",
    "    \"\"\"\n",
    "    Candidate goals are the 8 fixed points on a circle of radius GOAL_RADIUS.\n",
    "    The true MuJoCo target is always *one* of them, but the policy does not\n",
    "    know which one initially.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # fixed candidates in xy plane\n",
    "        th = torch.linspace(0, 2*math.pi, 9)[:-1]\n",
    "        self.register_buffer(\"goals\", torch.stack([GOAL_RADIUS*torch.cos(th),\n",
    "                                                   GOAL_RADIUS*torch.sin(th)], 1)) # (8,2)\n",
    "        self.raw_beta   = nn.Parameter(torch.tensor(2.0))\n",
    "        self.raw_wang   = nn.Parameter(torch.tensor(0.8))\n",
    "\n",
    "    def forward(self, fingertip, human_vec, belief=None):\n",
    "        \"\"\"\n",
    "        fingertip, human_vec : (B,2)\n",
    "        belief (prev)        : (B,8) or None\n",
    "        returns new belief (B,8) and −log p step for REINFORCE\n",
    "        \"\"\"\n",
    "        beta = torch.nn.functional.softplus(self.raw_beta)\n",
    "        wang = torch.nn.functional.softplus(self.raw_wang)\n",
    "\n",
    "        vec_to_g = self.goals[None,:,:] - fingertip[:,None,:]   # B×8×2\n",
    "        dir_g    = vec_to_g / (vec_to_g.norm(dim=-1, keepdim=True)+1e-8)\n",
    "        h_dir    = human_vec / (human_vec.norm(dim=-1, keepdim=True)+1e-8)\n",
    "        angle    = torch.arccos(torch.clamp((dir_g*h_dir[:,None,:]).sum(-1), -1., 1.)) # B×8\n",
    "        ll       = torch.exp(-beta*wang*angle)                  # likelihood (B×8)\n",
    "        if belief is None:\n",
    "            belief = torch.ones_like(ll)/ll.size(1)\n",
    "        belief   = belief * ll\n",
    "        belief   = belief / belief.sum(dim=1, keepdim=True)\n",
    "        neglogp  = -torch.log((belief.detach()*ll).sum(dim=1)+1e-9).mean()\n",
    "        return belief, neglogp\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "#  3.  γ‑Actor‑Critic\n",
    "class GammaAC(nn.Module):\n",
    "    def __init__(self, obs_dim):\n",
    "        super().__init__()\n",
    "        hid = 256\n",
    "        self.shared = nn.Sequential(nn.Linear(obs_dim, hid),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(hid, hid),\n",
    "                                    nn.ReLU())\n",
    "        self.mu_head  = nn.Linear(hid, 1)\n",
    "        self.log_std  = nn.Parameter(torch.zeros(1))\n",
    "        self.v_head   = nn.Linear(hid, 1)\n",
    "\n",
    "    def forward(self, obs, deterministic=False):\n",
    "        feat = self.shared(obs)\n",
    "        mu   = torch.tanh(self.mu_head(feat))\n",
    "        std  = torch.exp(self.log_std).clip(1e-3, 1.0)\n",
    "        dist = Normal(mu, std)\n",
    "        a    = mu if deterministic else torch.tanh(dist.rsample())\n",
    "        logp = dist.log_prob(a).sum(-1, keepdim=True)\n",
    "        v    = self.v_head(feat)\n",
    "        return a, logp, v\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "#  4.  Integrated simulator (vectorised, pure torch)\n",
    "class VecReacher:\n",
    "    def __init__(self, n_env:int, expert_path=\"expert_reacher.zip\"):\n",
    "        self.n = n_env\n",
    "        self.envs = [gym.make(\"Reacher-v4\") for _ in range(n_env)]\n",
    "        self.obs  = np.stack([e.reset(seed=i)[0] for i,e in enumerate(self.envs)])\n",
    "        self.done = np.zeros(n_env, dtype=np.bool_)\n",
    "        self.steps= np.zeros(n_env, dtype=np.int32)\n",
    "\n",
    "        # preload expert (frozen)\n",
    "        self.expert = SAC.load(expert_path, device=DEVICE)\n",
    "        self.expert.policy.eval()\n",
    "        for p in self.expert.policy.parameters(): p.requires_grad_(False)\n",
    "\n",
    "    def get_w_action(self, obs_np):\n",
    "        a,_ = self.expert.predict(obs_np, deterministic=True)\n",
    "        return torch.tensor(a, dtype=torch.float32, device=DEVICE)  # (n,2)\n",
    "\n",
    "    def step(self, blended_act):\n",
    "        \"\"\"\n",
    "        blended_act: torch (n,2)  in torque space, clipped automatically\n",
    "        returns: obs_torch, reward_tensor, done_mask\n",
    "        \"\"\"\n",
    "        acts = blended_act.detach().cpu().numpy()\n",
    "        new_obs, rews, dones, truncs = [],[],[],[]\n",
    "        for i,e in enumerate(self.envs):\n",
    "            if self.done[i]:\n",
    "                new_obs.append(self.obs[i])\n",
    "                rews.append(0.0)\n",
    "                dones.append(True)\n",
    "                truncs.append(False)\n",
    "                continue\n",
    "            ob, r, d, tr, _ = e.step(acts[i])\n",
    "            new_obs.append(ob)\n",
    "            rews.append(r)\n",
    "            dones.append(d)\n",
    "            truncs.append(tr)\n",
    "            if d or tr: ob,_ = e.reset()\n",
    "            self.done[i] = d or tr\n",
    "            self.steps[i]+=1\n",
    "        self.obs = np.stack(new_obs)\n",
    "        return torch.tensor(self.obs, dtype=torch.float32, device=DEVICE), \\\n",
    "               torch.tensor(rews, dtype=torch.float32, device=DEVICE), \\\n",
    "               torch.tensor(dones, dtype=torch.bool,  device=DEVICE)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "#  5.  Training loop\n",
    "def train_brace(total_epochs=8000, n_env=32, rollout_len=64):\n",
    "    world   = VecReacher(n_env)\n",
    "    bayes   = BayesianIntent().to(DEVICE)\n",
    "    gamma_pi= GammaAC(obs_dim=11 + 8).to(DEVICE)          # 11 env obs + 8‑belief\n",
    "    opt_pi  = optim.Adam(gamma_pi.parameters(), 3e-4)\n",
    "    opt_b   = optim.Adam(bayes.parameters(),   1e-3)\n",
    "\n",
    "    for ep in range(total_epochs):\n",
    "        belief = torch.ones(n_env,8,device=DEVICE)/8\n",
    "        buffers = {\"r\":[], \"logp_a\":[], \"logp_b\":[], \"v\":[], \"done\":[]}\n",
    "        for t in range(rollout_len):\n",
    "            obs = torch.tensor(world.obs, dtype=torch.float32, device=DEVICE)\n",
    "            tip, target = split_obs(obs)\n",
    "            w_act = world.get_w_action(world.obs)\n",
    "\n",
    "            # simulated noisy human torque\n",
    "            h_act = w_act + torch.randn_like(w_act)*0.1\n",
    "\n",
    "            # Bayesian update (uses fingertip + direction of h_act in joint space)\n",
    "            belief, neglogp_h = bayes(tip, h_act, belief)\n",
    "\n",
    "            # policy input\n",
    "            inp = torch.cat([obs, belief], dim=1)\n",
    "            a, logp_a, v = gamma_pi(inp)\n",
    "            gamma = 0.5*(a+1.0)\n",
    "            blend_act = gamma*w_act + (1-gamma)*h_act\n",
    "            next_obs, r, done = world.step(blend_act)\n",
    "\n",
    "            # store\n",
    "            buffers[\"r\"].append(r)\n",
    "            buffers[\"logp_a\"].append(logp_a.squeeze(-1))\n",
    "            buffers[\"logp_b\"].append(-neglogp_h)          # +log p\n",
    "            buffers[\"v\"].append(v.squeeze(-1))\n",
    "            buffers[\"done\"].append(done.float())\n",
    "\n",
    "        # cat to tensors: T×B\n",
    "        R   = torch.stack(buffers[\"r\"])\n",
    "        V   = torch.stack(buffers[\"v\"])\n",
    "        logp_a = torch.stack(buffers[\"logp_a\"])\n",
    "        logp_b = torch.stack(buffers[\"logp_b\"])\n",
    "        D   = torch.stack(buffers[\"done\"])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            G, adv = torch.zeros_like(R), torch.zeros_like(R)\n",
    "            future = torch.zeros(n_env, device=DEVICE)\n",
    "            for t in reversed(range(rollout_len)):\n",
    "                future = R[t] + 0.99*future*(1-D[t])\n",
    "                G[t]   = future\n",
    "            adv = G - V\n",
    "\n",
    "        # losses\n",
    "        policy_loss = -(adv.detach()*logp_a).mean()\n",
    "        value_loss  = 0.5*((G-V)**2).mean()\n",
    "        entropy_reg = -0.0005 * (- (logp_a.exp()+1e-9).log()).mean()\n",
    "        loss_pi = policy_loss + value_loss + entropy_reg\n",
    "\n",
    "        reinforce_b = -(adv.detach()*logp_b).mean()\n",
    "\n",
    "        opt_pi.zero_grad()\n",
    "        loss_pi.backward()\n",
    "        opt_pi.step()\n",
    "\n",
    "        opt_b.zero_grad()\n",
    "        reinforce_b.backward()\n",
    "        opt_b.step()\n",
    "\n",
    "        if ep % 100 == 0:\n",
    "            print(f\"ep {ep:04d} | R {R.sum(0).mean():6.2f} \"\n",
    "                  f\"| π {loss_pi.item():6.3f} | B {reinforce_b.item():6.3f}\")\n",
    "\n",
    "    torch.save({\"gamma\": gamma_pi.state_dict(),\n",
    "                \"bayes\": bayes.state_dict()},\n",
    "               \"brace_integrated_reacher.pt\")\n",
    "    print(\"✓ saved brace_integrated_reacher.pt\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "#  6.  Demo viewer\n",
    "def watch(model_path=\"brace_integrated_reacher.pt\"):\n",
    "    ckpt   = torch.load(model_path, map_location=DEVICE)\n",
    "    bayes  = BayesianIntent().to(DEVICE)\n",
    "    gamma  = GammaAC(obs_dim=11+8).to(DEVICE)\n",
    "    gamma.load_state_dict(ckpt[\"gamma\"]); gamma.eval()\n",
    "    bayes.load_state_dict(ckpt[\"bayes\"]); bayes.eval()\n",
    "\n",
    "    env = gym.make(\"Reacher-v4\", render_mode=\"human\")\n",
    "    obs,_ = env.reset(seed=0)\n",
    "    belief = torch.ones(1,8,device=DEVICE)/8\n",
    "    ep_ret = 0.0\n",
    "    for _ in range(600):\n",
    "        ob_t = torch.tensor(obs, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "        tip, _ = split_obs(ob_t)\n",
    "        w_act = SAC.load(\"expert_reacher.zip\", device=DEVICE).predict(obs, deterministic=True)[0]\n",
    "        w_act = torch.tensor(w_act, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "        h_act = w_act                                 # no noise for demo\n",
    "        belief,_ = bayes(tip, h_act, belief)\n",
    "        inp = torch.cat([ob_t, belief], dim=1)\n",
    "        a,_,_ = gamma(inp, deterministic=True)\n",
    "        gamma_val = 0.5*(a+1.0)\n",
    "        act = (gamma_val*w_act + (1-gamma_val)*h_act).squeeze(0).cpu().numpy()\n",
    "        obs, r, done, trunc, _ = env.step(act)\n",
    "        ep_ret += r\n",
    "        if done or trunc: break\n",
    "    print(\"return:\", ep_ret)\n",
    "    env.close()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--watch\", action=\"store_true\")\n",
    "    args = ap.parse_args()\n",
    "    if args.watch:\n",
    "        watch()\n",
    "    else:\n",
    "        train_brace()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
