{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed428b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# train_ida_reacher.py  –  IDA baseline pre‑training for Reacher‑2D\n",
    "# ---------------------------------------------------------------\n",
    "#  © 2025   MIT License\n",
    "#  Follows Algorithm 1 in “Shared Autonomy with IDA” (NeurIPS 24)\n",
    "# ---------------------------------------------------------------\n",
    "from __future__ import annotations\n",
    "import argparse, json, math, os, pathlib, random, copy, time\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1.  ENVIRONMENT  (default: MuJoCo Reacher‑v4)  --------------------\n",
    "# ------------------------------------------------------------------\n",
    "def make_env(full_obs: bool = True):\n",
    "    \"\"\"\n",
    "    Returns an env whose .observation_space is:\n",
    "      *full_obs=True*   : [ sinθ1 cosθ1 sinθ2 cosθ2 dθ1 dθ2 (x_e,y_e) (x_t,y_t) ]\n",
    "      *full_obs=False*  : same but target coords zeroed  (goal‑masked)\n",
    "    This trick matches § 3.3 of the IDA paper.\n",
    "    \"\"\"\n",
    "    base = gym.make(\"Reacher-v4\")   # requires mujoco\n",
    "    if full_obs:\n",
    "        return base\n",
    "\n",
    "    class GoalMasked(gym.ObservationWrapper):\n",
    "        def __init__(self, env):\n",
    "            super().__init__(env)\n",
    "            low = env.observation_space.low.copy()\n",
    "            high = env.observation_space.high.copy()\n",
    "            # last 2 dims are target x,y  in MuJoCo Reacher\n",
    "            low[-2:] = 0.0 ; high[-2:] = 0.0\n",
    "            self.observation_space = gym.spaces.Box(low, high, dtype=np.float32)\n",
    "\n",
    "        def observation(self, obs):\n",
    "            obs = obs.copy()\n",
    "            obs[-2:] = 0.0\n",
    "            return obs\n",
    "    return GoalMasked(base)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2.  TRAIN SAC EXPERT  -------------------------------------------\n",
    "# ------------------------------------------------------------------\n",
    "def train_expert(total_steps: int, seed: int, save_dir: pathlib.Path) -> SAC:\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    env = DummyVecEnv([lambda: make_env(full_obs=True)])\n",
    "    model = SAC(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        learning_rate=3e-4,\n",
    "        gamma=0.98,\n",
    "        tau=0.005,\n",
    "        buffer_size=1_000_000,\n",
    "        batch_size=1024,\n",
    "        train_freq=(1, \"episode\"),\n",
    "        gradient_steps=1,\n",
    "        policy_kwargs=dict(net_arch=[256, 256, 256]),\n",
    "        seed=seed,\n",
    "        device=\"auto\",\n",
    "        verbose=1,\n",
    "    )\n",
    "    model.learn(total_timesteps=total_steps, progress_bar=True)\n",
    "    model.save(save_dir / \"sac_expert\")\n",
    "    # export Q‑heads for runtime advantage computations\n",
    "    torch.save(\n",
    "        {\n",
    "            \"qf1\": model.critic.qf1.state_dict(),\n",
    "            \"qf2\": model.critic.qf2.state_dict(),\n",
    "        },\n",
    "        save_dir / \"q_heads.pt\",\n",
    "    )\n",
    "    env.close()\n",
    "    return model\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3.  COLLECT GOAL‑MASKED DEMOS  -----------------------------------\n",
    "# ------------------------------------------------------------------\n",
    "def collect_demos(model: SAC,\n",
    "                  n_steps: int,\n",
    "                  save_file: pathlib.Path):\n",
    "    env = make_env(full_obs=False)            # goal‑masked\n",
    "    obs, _ = env.reset(seed=0)\n",
    "    S, A = [], []\n",
    "    for _ in tqdm(range(n_steps), desc=\"collect demos\"):\n",
    "        act, _ = model.predict(obs, deterministic=True)\n",
    "        S.append(obs);  A.append(act)\n",
    "        obs, _, term, trunc, _ = env.step(act)\n",
    "        if term or trunc:\n",
    "            obs, _ = env.reset()\n",
    "    np.savez_compressed(save_file, s=np.asarray(S, np.float32),\n",
    "                        a=np.asarray(A, np.float32))\n",
    "    env.close()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4.  DIFFUSION COPILOT  -------------------------------------------\n",
    "# ------------------------------------------------------------------\n",
    "class EpsNet(nn.Module):\n",
    "    def __init__(self, obs_dim: int, act_dim: int, hid: int = 256):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim + act_dim + 1, hid), nn.ReLU(),\n",
    "            nn.Linear(hid, hid), nn.ReLU(),\n",
    "            nn.Linear(hid, act_dim)\n",
    "        )\n",
    "    def forward(self, x, t):                 # t ∈ [0,1]\n",
    "        return self.net(torch.cat([x, t[:, None]], -1))\n",
    "\n",
    "\n",
    "class Diffusion:\n",
    "    def __init__(self, obs_dim, act_dim,\n",
    "                 T=1000, beta1=1e-4, beta2=0.02, lr=3e-4, device=\"auto\"):\n",
    "        self.T = T\n",
    "        self.betas = torch.linspace(beta1, beta2, T)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.a_cum = torch.cumprod(self.alphas, 0)\n",
    "        self.net = EpsNet(obs_dim, act_dim).to(device)\n",
    "        self.opt = optim.Adam(self.net.parameters(), lr=lr)\n",
    "        self.dev = device\n",
    "\n",
    "    def q_sample(self, x0, t, eps):\n",
    "        a_bar = self.a_cum[t].unsqueeze(1)\n",
    "        return torch.sqrt(a_bar) * x0 + torch.sqrt(1 - a_bar) * eps\n",
    "\n",
    "    def train_loop(self, loader, epochs=200):\n",
    "        self.net.train()\n",
    "        for ep in range(epochs):\n",
    "            for obs, act in loader:\n",
    "                obs, act = obs.to(self.dev), act.to(self.dev)\n",
    "                eps = torch.randn_like(act)\n",
    "                t = torch.randint(0, self.T, (act.size(0),), device=self.dev)\n",
    "                noisy = self.q_sample(act, t, eps)\n",
    "                pred = self.net(torch.cat([obs, noisy], 1), t.float()/self.T)\n",
    "                loss = nn.functional.mse_loss(pred, eps)\n",
    "                self.opt.zero_grad(); loss.backward(); self.opt.step()\n",
    "            if ep % 20 == 0:\n",
    "                print(f\"[DDPM] epoch {ep:03d}  loss {loss.item():.4f}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# MAIN  -------------------------------------------------------------\n",
    "# ------------------------------------------------------------------\n",
    "def main(args):\n",
    "    out = pathlib.Path(args.out_dir)\n",
    "    (out / \"expert\").mkdir(parents=True, exist_ok=True)\n",
    "    # ---------- Expert -------------------------------------------------------\n",
    "    if not (out / \"expert\" / \"sac_expert.zip\").exists():\n",
    "        expert = train_expert(args.expert_steps, args.seed, out / \"expert\")\n",
    "    else:\n",
    "        expert = SAC.load(out / \"expert\" / \"sac_expert\")\n",
    "\n",
    "    # ---------- Demonstrations ----------------------------------------------\n",
    "    demo_file = out / \"demos\" / \"expert_demos.npz\"\n",
    "    if not demo_file.exists():\n",
    "        os.makedirs(out / \"demos\", exist_ok=True)\n",
    "        collect_demos(expert, args.demo_steps, demo_file)\n",
    "\n",
    "    demos = np.load(demo_file)\n",
    "    obs_dim = demos[\"s\"].shape[1]; act_dim = demos[\"a\"].shape[1]\n",
    "    ds = TensorDataset(torch.from_numpy(demos[\"s\"]),\n",
    "                       torch.from_numpy(demos[\"a\"]))\n",
    "    loader = DataLoader(ds, batch_size=2048, shuffle=True, drop_last=True)\n",
    "\n",
    "    # ---------- Copilot ------------------------------------------------------\n",
    "    diff = Diffusion(obs_dim, act_dim, lr=args.ddpm_lr, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    diff.train_loop(loader, epochs=args.ddpm_epochs)\n",
    "    torch.save(diff.net.state_dict(), out / \"copilot_ddpm.pt\")\n",
    "\n",
    "    # ---------- Quick‑load config -------------------------------------------\n",
    "    cfg = dict(\n",
    "        obs_dim=obs_dim,\n",
    "        act_dim=act_dim,\n",
    "        T=1000,\n",
    "        q_heads=str(out / \"expert\" / \"q_heads.pt\"),\n",
    "        ddpm=str(out / \"copilot_ddpm.pt\"),\n",
    "    )\n",
    "    with open(out / \"ida_config.json\", \"w\") as fp:\n",
    "        json.dump(cfg, fp, indent=2)\n",
    "    print(\"✅  all artefacts saved in\", out.resolve())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    p = argparse.ArgumentParser()\n",
    "    p.add_argument(\"--expert_steps\", type=int, default=3_000_000)\n",
    "    p.add_argument(\"--demo_steps\",   type=int, default=1_000_000)\n",
    "    p.add_argument(\"--ddpm_epochs\",  type=int, default=200)\n",
    "    p.add_argument(\"--ddpm_lr\",      type=float, default=3e-4)\n",
    "    p.add_argument(\"--seed\",         type=int, default=0)\n",
    "    p.add_argument(\"--out_dir\",      type=str, default=\"ida_reacher_out\")\n",
    "    main(p.parse_args())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
