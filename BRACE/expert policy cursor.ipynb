{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5077a1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# train_expert_brace.py\n",
    "# -----------------------------------------------------------\n",
    "#  Train an expert SAC policy that can reach goals autonomously\n",
    "# -----------------------------------------------------------\n",
    "import os, math, random, argparse\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "import torch\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# ------------------- geometry / constants ------------------\n",
    "FULL          = np.array([1200, 800], dtype=np.float32)\n",
    "MAX_SPEED     = 5.0\n",
    "DOT_R         = 14\n",
    "OBS_R         = 10\n",
    "TGT_R         = 9\n",
    "STEP_PENALTY  = -0.01        # small living‑cost\n",
    "GOAL_BONUS    =  2.0\n",
    "COLLIDE_PENAL = -4.0\n",
    "\n",
    "def distance(a, b): return np.linalg.norm(a-b, ord=2)\n",
    "\n",
    "# ------------------- Expert environment --------------------\n",
    "class BraceExpertEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    action  : Δx,Δy in [-1,1]^2   → scaled by MAX_SPEED\n",
    "    state   : [cursor_x, cursor_y, goal_x, goal_y,\n",
    "               d_goal_norm, min_obs_norm]\n",
    "    reward  : shaped negative distance + bonuses/penalties\n",
    "    \"\"\"\n",
    "    metadata = {\"render_modes\": []}\n",
    "    def __init__(self, n_goals=8, seed=None, visualize=False):\n",
    "        super().__init__()\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self.visualize = visualize\n",
    "        self.n_goals = n_goals\n",
    "        # spaces\n",
    "        low  = np.array([0,0,  0,0,  0,0], dtype=np.float32)\n",
    "        high = np.array([*FULL, *FULL, 1,1], dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low, high, dtype=np.float32)\n",
    "        self.action_space      = spaces.Box(-1.0, 1.0, shape=(2,), dtype=np.float32)\n",
    "\n",
    "        self.max_dist = np.linalg.norm(FULL)\n",
    "        self.goal_thr = 36.0\n",
    "        if visualize:\n",
    "            import pygame\n",
    "            pygame.init()\n",
    "            self.pg = pygame\n",
    "            self.screen = pygame.display.set_mode(FULL)\n",
    "            self.clock  = pygame.time.Clock()\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    def _generate_layout(self):\n",
    "        margin = 70\n",
    "        self.cursor = FULL/2\n",
    "        goals = []\n",
    "        while len(goals) < self.n_goals:\n",
    "            p = self.rng.uniform([margin,margin], FULL-[margin,margin])\n",
    "            if distance(p, self.cursor) > 260:\n",
    "                goals.append(p)\n",
    "        self.goals = np.stack(goals, axis=0)\n",
    "        self.goal_xy = self.goals[self.rng.integers(self.n_goals)]\n",
    "\n",
    "        # three obstacles on the way\n",
    "        obstacles = []\n",
    "        for g in self.goals[:3]:\n",
    "            t  = self.rng.uniform(0.55,0.8)\n",
    "            base = self.cursor + t*(g-self.cursor)\n",
    "            perp = np.array([-(g-self.cursor)[1], (g-self.cursor)[0]])\n",
    "            perp /= np.linalg.norm(perp)+1e-9\n",
    "            off = perp*self.rng.uniform(60,80)*self.rng.choice([-1,1])\n",
    "            obstacles.append(base+off)\n",
    "        self.obstacles = np.stack(obstacles, axis=0)\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self._generate_layout()\n",
    "        self.t = 0\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    def _get_obs(self):\n",
    "        d_goal_norm = distance(self.cursor, self.goal_xy)/self.max_dist\n",
    "        d_obs_norm  = np.min(np.linalg.norm(self.cursor-self.obstacles, axis=1))/self.max_dist\n",
    "        return np.concatenate([self.cursor, self.goal_xy,\n",
    "                               [d_goal_norm, d_obs_norm]]).astype(np.float32)\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    def _render(self):\n",
    "        if not self.visualize: return\n",
    "        for e in self.pg.event.get():\n",
    "            if e.type == self.pg.QUIT: self.pg.quit(); exit()\n",
    "        self.screen.fill((255,255,255))\n",
    "        for o in self.obstacles:\n",
    "            self.pg.draw.circle(self.screen, (128,128,128), o.astype(int), OBS_R)\n",
    "        for g in self.goals:\n",
    "            self.pg.draw.circle(self.screen, (255,255,0), g.astype(int), TGT_R)\n",
    "        self.pg.draw.circle(self.screen, (0,0,0), self.goal_xy.astype(int), TGT_R+2, 2)\n",
    "        self.pg.draw.circle(self.screen, (0,0,0), self.cursor.astype(int), DOT_R, 2)\n",
    "        self.pg.display.flip()\n",
    "        self.clock.tick(30)\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    def step(self, action):\n",
    "        a = np.clip(action, -1, 1)\n",
    "        self.cursor = np.clip(self.cursor + a*MAX_SPEED, [0,0], FULL)\n",
    "        self.t += 1\n",
    "\n",
    "        # distances\n",
    "        d_goal = distance(self.cursor, self.goal_xy)\n",
    "        d_obs  = np.min(np.linalg.norm(self.cursor-self.obstacles, axis=1))\n",
    "        collide = d_obs < DOT_R+OBS_R\n",
    "\n",
    "        # shaped reward\n",
    "        reward = STEP_PENALTY + (self.prev_d - d_goal)*0.5 if hasattr(self, \"prev_d\") else 0\n",
    "        self.prev_d = d_goal\n",
    "        if collide: reward += COLLIDE_PENAL\n",
    "        reached = d_goal < self.goal_thr\n",
    "        if reached: reward += GOAL_BONUS\n",
    "\n",
    "        terminated = collide or reached\n",
    "        truncated  = self.t >= 400\n",
    "        info = {}\n",
    "        if self.visualize: self._render()\n",
    "        return self._get_obs(), float(reward), terminated, truncated, info\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    def close(self):\n",
    "        if self.visualize:\n",
    "            self.pg.quit()\n",
    "        super().close()\n",
    "\n",
    "# ------------------- simple save callback ------------------\n",
    "class SaveCB(BaseCallback):\n",
    "    def __init__(self, path=\"expert_sac.zip\"): super().__init__(); self.path=path\n",
    "    def _on_training_end(self): self.model.save(self.path); print(\"✓ expert saved to\", self.path)\n",
    "\n",
    "# ===========================================================\n",
    "#  Train / watch helpers\n",
    "# ===========================================================\n",
    "def train_expert(steps:int=1_000_000):\n",
    "    env = DummyVecEnv([lambda: BraceExpertEnv()])\n",
    "    model = SAC(\n",
    "        \"MlpPolicy\", env,\n",
    "        learning_rate=3e-4,\n",
    "        buffer_size=300_000,\n",
    "        batch_size=1024,\n",
    "        tau=0.005, gamma=0.995,\n",
    "        train_freq=64, gradient_steps=64,\n",
    "        learning_starts=5000,\n",
    "        verbose=1, tensorboard_log=\"./tb_expert\")\n",
    "    model.learn(steps, callback=SaveCB())\n",
    "\n",
    "def watch_expert(path=\"expert_sac.zip\"):\n",
    "    model = SAC.load(path)\n",
    "    env   = BraceExpertEnv(visualize=True)\n",
    "    obs,_ = env.reset()\n",
    "    done=trunc=False\n",
    "    R=0\n",
    "    while not(done or trunc):\n",
    "        act,_ = model.predict(obs, deterministic=True)\n",
    "        obs,r,done,trunc,_ = env.step(act)\n",
    "        R += r\n",
    "    print(\"episode return:\", R)\n",
    "    env.close()\n",
    "\n",
    "# ===========================================================\n",
    "if __name__ == \"__main__\":\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--watch\", action=\"store_true\")\n",
    "    ap.add_argument(\"--steps\", type=int, default=1_000_000)\n",
    "    args = ap.parse_args()\n",
    "    if args.watch:\n",
    "        watch_expert()\n",
    "    else:\n",
    "        train_expert(args.steps)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
