{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c2868c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# train_actor_critic_brace.py\n",
    "# -----------------------------------------------------------\n",
    "# Train the BRACE actor‑critic (γ–policy + value) with PPO\n",
    "# -----------------------------------------------------------\n",
    "import os, math, random, argparse, json, time\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "\n",
    "# ===========================================================\n",
    "#  1.  Analytic Bayesian goal‑inference stub (replace later)\n",
    "# ===========================================================\n",
    "class BayesianGoalInferenceStub:\n",
    "    \"\"\"\n",
    "    Quick analytic approximation of Eq.(1)-(3) from BRACE.\n",
    "    Produces a |G|‑dim belief vector b_t every time step.\n",
    "    \"\"\"\n",
    "    def __init__(self, goals, beta=8.0, w_th=0.8):\n",
    "        self.goals = np.array(goals, dtype=np.float32)\n",
    "        self.n = len(goals)\n",
    "        self.prior = np.ones(self.n, dtype=np.float32) / self.n\n",
    "        self.beta, self.w_th = beta, w_th\n",
    "        self.belief = self.prior.copy()\n",
    "\n",
    "    def reset(self):\n",
    "        self.belief[:] = self.prior\n",
    "\n",
    "    def step(self, cursor_xy, human_vec):\n",
    "        \"\"\"Return updated belief vector (numpy, shape (n,))\"\"\"\n",
    "        if np.linalg.norm(human_vec) < 1e-5:\n",
    "            return self.belief        # no evidence\n",
    "        h_dir = human_vec / (np.linalg.norm(human_vec) + 1e-9)\n",
    "        log_lik = []\n",
    "        for g in self.goals:\n",
    "            to_g = g - cursor_xy\n",
    "            if np.linalg.norm(to_g) < 1e-5:\n",
    "                ang = 0.0\n",
    "            else:\n",
    "                to_dir = to_g / np.linalg.norm(to_g)\n",
    "                cos = np.clip(np.dot(h_dir, to_dir), -1., 1.)\n",
    "                ang = math.acos(cos)\n",
    "            log_lik.append(-self.beta * self.w_th * ang)\n",
    "        log_lik = np.array(log_lik, dtype=np.float32)\n",
    "        self.belief *= np.exp(log_lik - log_lik.max())  # for stability\n",
    "        self.belief /= self.belief.sum()\n",
    "        return self.belief.copy()\n",
    "\n",
    "# ===========================================================\n",
    "#  2.  Environment producing context ⊕ belief  (observation)\n",
    "# ===========================================================\n",
    "FULL = np.array([1200, 800], dtype=np.float32)\n",
    "MAX_SPEED = 3.0\n",
    "DOT_R, OBS_R, TGT_R = 14, 10, 9\n",
    "\n",
    "class BraceArbitrationEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Observation  = [x,y, d_goal_norm, d_obs_norm, belief_1..belief_N]\n",
    "    Action (Box) = scalar a ∈[‑1,1]  →  γ = (a+1)/2\n",
    "    Reward       = −λ(γ − γ*)²  − 2 if collision\n",
    "    γ* computed from BRACE’s rule-of‑thumb:\n",
    "        near goal  → low γ\n",
    "        near obstacle → high γ\n",
    "        conflict → mid\n",
    "    \"\"\"\n",
    "    metadata = {\"render_modes\": []}\n",
    "    def __init__(self, n_goals=6, seed=None):\n",
    "        super().__init__()\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self.n_goals = n_goals\n",
    "        self.goal_xy = None\n",
    "        self.cursor  = None\n",
    "        self.obstacles = None\n",
    "        low  = np.concatenate([[0,0, 0,0], np.zeros(n_goals, np.float32)])\n",
    "        high = np.concatenate([FULL, 1*np.ones(2+n_goals, np.float32)])\n",
    "        self.observation_space = spaces.Box(low, high, dtype=np.float32)\n",
    "        self.action_space      = spaces.Box(-1.0, 1.0, shape=(), dtype=np.float32)\n",
    "\n",
    "        # geometry\n",
    "        self.max_dist = np.linalg.norm(FULL)\n",
    "        self.goal_thr = 120.0\n",
    "        self.obs_thr  = 120.0\n",
    "        # Bayesian filter\n",
    "        self.belief_filter = None\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    def _geometry_reset(self):\n",
    "        margin = 80\n",
    "        self.cursor = FULL/2\n",
    "        goals = []\n",
    "        while len(goals) < self.n_goals:\n",
    "            p = self.rng.uniform([margin,margin], FULL-[margin,margin])\n",
    "            if np.linalg.norm(p-self.cursor) > 250:\n",
    "                goals.append(p)\n",
    "        self.goals = np.stack(goals, axis=0)\n",
    "        obstacles = []\n",
    "        for g in goals[:3]:\n",
    "            t = self.rng.uniform(0.55,0.8)\n",
    "            base = self.cursor + t*(g-self.cursor)\n",
    "            perp = np.array([-(g-self.cursor)[1], (g-self.cursor)[0]])\n",
    "            perp /= np.linalg.norm(perp)+1e-9\n",
    "            off  = perp*self.rng.uniform(60,90)*self.rng.choice([-1,1])\n",
    "            obstacles.append(base+off)\n",
    "        self.obstacles = np.stack(obstacles, axis=0)\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self._geometry_reset()\n",
    "        self.goal_xy = self.goals[self.rng.integers(self.n_goals)]\n",
    "        self.step_cnt = 0\n",
    "        if self.belief_filter is None or self.belief_filter.n != self.n_goals:\n",
    "            self.belief_filter = BayesianGoalInferenceStub(self.goals)\n",
    "        else:\n",
    "            self.belief_filter.reset()\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    def _get_obs(self):\n",
    "        d_goal = np.linalg.norm(self.cursor-self.goal_xy)/self.max_dist\n",
    "        d_obs  = np.min(np.linalg.norm(self.cursor-self.obstacles, axis=1))/self.max_dist\n",
    "        obs = np.concatenate([self.cursor, [d_goal, d_obs], self.belief])\n",
    "        return obs.astype(np.float32)\n",
    "\n",
    "    # -------------------------------------------------------\n",
    "    @property\n",
    "    def belief(self):\n",
    "        return self.belief_filter.belief\n",
    "\n",
    "    def _ideal_gamma(self, d_goal, d_obs):\n",
    "        if d_goal < self.goal_thr/self.max_dist and d_obs < self.obs_thr/self.max_dist:\n",
    "            return 0.5\n",
    "        if d_goal < self.goal_thr/self.max_dist:\n",
    "            return 0.2\n",
    "        if d_obs  < self.obs_thr /self.max_dist:\n",
    "            return 0.8\n",
    "        return 0.5\n",
    "\n",
    "    def step(self, action):\n",
    "        a = float(action)\n",
    "        gamma = (a+1.0)/2.0                       # map [‑1,1]→[0,1]\n",
    "\n",
    "        # synthetic human vector: noisy ideal\n",
    "        w_dir = (self.goal_xy-self.cursor)\n",
    "        w_dir /= np.linalg.norm(w_dir)+1e-9\n",
    "        h_dir = w_dir + self.rng.normal(0, 0.3, size=2)\n",
    "        h_dir /= np.linalg.norm(h_dir)+1e-9\n",
    "\n",
    "        # update belief\n",
    "        self.belief_filter.step(self.cursor, h_dir)\n",
    "\n",
    "        # blended motion\n",
    "        step_vec = (gamma*w_dir + (1-gamma)*h_dir)\n",
    "        step_vec /= np.linalg.norm(step_vec)+1e-9\n",
    "        self.cursor = np.clip(self.cursor + step_vec*MAX_SPEED, [0,0], FULL)\n",
    "\n",
    "        # collision?\n",
    "        collide = (np.min(np.linalg.norm(self.cursor-self.obstacles, axis=1)) < DOT_R+OBS_R)\n",
    "\n",
    "        self.step_cnt += 1\n",
    "        d_goal = np.linalg.norm(self.cursor-self.goal_xy)/self.max_dist\n",
    "        d_obs  = np.min(np.linalg.norm(self.cursor-self.obstacles, axis=1))/self.max_dist\n",
    "        ideal_g = self._ideal_gamma(d_goal, d_obs)\n",
    "\n",
    "        reward = -20*(gamma-ideal_g)**2\n",
    "        terminated = collide\n",
    "        if collide: reward -= 2.0\n",
    "        truncated = self.step_cnt >= 400\n",
    "\n",
    "        obs = self._get_obs()\n",
    "        info = {\"ideal_gamma\": ideal_g}\n",
    "        return obs, float(reward), terminated, truncated, info\n",
    "\n",
    "# ===========================================================\n",
    "#  3.  Custom policy (shared trunk → π‑head & V‑head)\n",
    "# ===========================================================\n",
    "class GammaBeliefPolicy(ActorCriticPolicy):\n",
    "    \"\"\"\n",
    "    Observation dims vary with |G|, so build net at runtime.\n",
    "    Action: tanh‑squashed scalar a∈[‑1,1]  (γ = 0.5(a+1))\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    def build_mlp(self, sizes):\n",
    "        layers = []\n",
    "        for i in range(len(sizes)-1):\n",
    "            layers += [nn.Linear(sizes[i], sizes[i+1]), nn.ReLU()]\n",
    "        return nn.Sequential(*layers[:-1])          # drop last ReLU\n",
    "    def _build(self, obs_dim):\n",
    "        hidden = 256\n",
    "        self.shared = self.build_mlp([obs_dim, hidden, hidden])\n",
    "        self.mu_head = nn.Linear(hidden, 1)\n",
    "        self.log_std = nn.Parameter(torch.zeros(1))\n",
    "        self.v_head  = nn.Linear(hidden, 1)\n",
    "    def extract_features(self, obs):\n",
    "        if not hasattr(self, \"shared\"):\n",
    "            self._build(obs.shape[-1])\n",
    "        return self.shared(obs)\n",
    "    def forward(self, obs, deterministic=False):\n",
    "        latent = self.extract_features(obs)\n",
    "        mean   = torch.tanh(self.mu_head(latent))\n",
    "        std    = self.log_std.exp().expand_as(mean)\n",
    "        dist   = Normal(mean, std)\n",
    "        action = mean if deterministic else torch.tanh(dist.rsample())\n",
    "        logp   = dist.log_prob(action).sum(dim=-1, keepdim=True)\n",
    "        value  = self.v_head(latent)\n",
    "        return action, value, logp\n",
    "\n",
    "# ===========================================================\n",
    "#  4.  Minimal training‑metric callback\n",
    "# ===========================================================\n",
    "class SimpleMetrics(BaseCallback):\n",
    "    def __init__(self): super().__init__()\n",
    "    def _on_training_end(self):\n",
    "        path = \"brace_gamma_ppo.zip\"\n",
    "        self.model.save(path)\n",
    "        print(\"✓ model saved to\", path)\n",
    "\n",
    "# ===========================================================\n",
    "#  5.  Train / watch\n",
    "# ===========================================================\n",
    "def train(timesteps:int=600_000):\n",
    "    env = DummyVecEnv([lambda: BraceArbitrationEnv()])\n",
    "    model = PPO(\n",
    "        GammaBeliefPolicy, env,\n",
    "        learning_rate=3e-4,\n",
    "        n_steps=1024, batch_size=1024, n_epochs=4,\n",
    "        gamma=0.995, gae_lambda=0.97,\n",
    "        clip_range=0.2, verbose=1,\n",
    "        tensorboard_log=\"./tb_brace\")\n",
    "    model.learn(timesteps, callback=SimpleMetrics())\n",
    "\n",
    "def watch(path:str=\"brace_gamma_ppo.zip\"):\n",
    "    model = PPO.load(path)\n",
    "    env = BraceArbitrationEnv()\n",
    "    obs, _ = env.reset()\n",
    "    done = trunc = False\n",
    "    ret = 0\n",
    "    while not (done or trunc):\n",
    "        a,_ = model.predict(obs, deterministic=True)\n",
    "        obs,r,done,trunc,_ = env.step(a)\n",
    "        ret += r\n",
    "    print(\"episode return:\", ret)\n",
    "\n",
    "# ===========================================================\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--watch\", action=\"store_true\")\n",
    "    parser.add_argument(\"--steps\", type=int, default=600_000)\n",
    "    args = parser.parse_args()\n",
    "    if args.watch:\n",
    "        watch()\n",
    "    else:\n",
    "        train(args.steps)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
