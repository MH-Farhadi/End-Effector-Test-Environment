{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Only needed if visualize=True\n",
    "try:\n",
    "    import pygame\n",
    "except ImportError:\n",
    "    pygame = None\n",
    "\n",
    "###############################################################################\n",
    "# ENVIRONMENT CONSTANTS\n",
    "###############################################################################\n",
    "FULL_VIEW_SIZE = (1200, 800)\n",
    "SCALING_FACTOR_X = FULL_VIEW_SIZE[0] / 600.0\n",
    "SCALING_FACTOR_Y = FULL_VIEW_SIZE[1] / 600.0\n",
    "SCALING_FACTOR   = (SCALING_FACTOR_X + SCALING_FACTOR_Y) / 2\n",
    "\n",
    "DOT_RADIUS       = int(15 * SCALING_FACTOR)\n",
    "TARGET_RADIUS    = int(10 * SCALING_FACTOR)\n",
    "OBSTACLE_RADIUS  = int(10 * SCALING_FACTOR)\n",
    "COLLISION_BUFFER = int(5  * SCALING_FACTOR)\n",
    "MAX_SPEED        = 3 * SCALING_FACTOR\n",
    "NOISE_MAGNITUDE  = 0.5\n",
    "RENDER_FPS       = 30\n",
    "\n",
    "# The start position is fixed in the middle.\n",
    "START_POS = np.array([FULL_VIEW_SIZE[0] // 2, FULL_VIEW_SIZE[1] // 2], dtype=np.float32)\n",
    "\n",
    "WHITE = (255, 255, 255)\n",
    "GRAY  = (128, 128, 128)\n",
    "YELLOW= (255, 255, 0)\n",
    "BLACK = (0, 0, 0)\n",
    "\n",
    "###############################################################################\n",
    "# UTILITY FUNCTIONS\n",
    "###############################################################################\n",
    "def distance(a, b):\n",
    "    return math.hypot(a[0] - b[0], a[1] - b[1])\n",
    "\n",
    "def check_line_collision(start, end, center, radius):\n",
    "    dx = end[0] - start[0]\n",
    "    dy = end[1] - start[1]\n",
    "    fx = center[0] - start[0]\n",
    "    fy = center[1] - start[1]\n",
    "    l2 = dx * dx + dy * dy\n",
    "    if l2 < 1e-9:\n",
    "        return distance(start, center) <= radius\n",
    "    t = max(0, min(1, (fx * dx + fy * dy) / l2))\n",
    "    px = start[0] + t * dx\n",
    "    py = start[1] + t * dy\n",
    "    return distance((px, py), center) <= radius\n",
    "\n",
    "def line_collision(pos, new_pos, obstacles):\n",
    "    for obs in obstacles:\n",
    "        if check_line_collision(pos, new_pos, obs, OBSTACLE_RADIUS + COLLISION_BUFFER):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def inside_obstacle(pos, obstacles):\n",
    "    for obs in obstacles:\n",
    "        if distance(pos, obs) <= (OBSTACLE_RADIUS + DOT_RADIUS):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def potential_field_dir(pos, goal, obstacles):\n",
    "    \"\"\"\n",
    "    Compute a normalized direction vector that is a combination of an attractive \n",
    "    force toward the goal and repulsive forces from obstacles.\n",
    "    \"\"\"\n",
    "    gx = goal[0] - pos[0]\n",
    "    gy = goal[1] - pos[1]\n",
    "    dg = math.hypot(gx, gy)\n",
    "    if dg < 1e-6:\n",
    "        return np.zeros(2, dtype=np.float32)\n",
    "    att = np.array([gx/dg, gy/dg], dtype=np.float32)\n",
    "\n",
    "    repulse_x = 0.0\n",
    "    repulse_y = 0.0\n",
    "    repulsion_radius = 150.0 * SCALING_FACTOR\n",
    "    repulsion_gain   = 15000.0\n",
    "\n",
    "    for obs in obstacles:\n",
    "        dx = pos[0] - obs[0]\n",
    "        dy = pos[1] - obs[1]\n",
    "        dobs = math.hypot(dx, dy)\n",
    "        if dobs < 1e-6:\n",
    "            continue\n",
    "        if dobs < repulsion_radius:\n",
    "            pushx    = dx / dobs\n",
    "            pushy    = dy / dobs\n",
    "            strength = repulsion_gain / (dobs**2)\n",
    "            repulse_x += pushx * strength\n",
    "            repulse_y += pushy * strength\n",
    "\n",
    "    px = att[0] + repulse_x\n",
    "    py = att[1] + repulse_y\n",
    "    mg = math.hypot(px, py)\n",
    "    if mg < 1e-6:\n",
    "        return np.zeros(2, dtype=np.float32)\n",
    "    return np.array([px/mg, py/mg], dtype=np.float32)\n",
    "\n",
    "###############################################################################\n",
    "# CALLBACK WITH MOVING AVERAGE METRICS\n",
    "###############################################################################\n",
    "class MetricsCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.episode_rewards = []\n",
    "        self.episode_gammas  = []\n",
    "        self.current_episode_gammas = []\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "        self.n_collisions = 0\n",
    "        self.n_episodes   = 0\n",
    "        self.action_low  = None\n",
    "        self.action_high = None\n",
    "\n",
    "    def _on_training_start(self):\n",
    "        self.n_collisions = 0\n",
    "        self.n_episodes   = 0\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.action_low is None:\n",
    "            self.action_low  = float(self.model.action_space.low)\n",
    "            self.action_high = float(self.model.action_space.high)\n",
    "\n",
    "        actions = self.locals['actions']\n",
    "        rewards = self.locals['rewards']\n",
    "        done    = self.locals['dones'][0]\n",
    "        infos   = self.locals['infos']\n",
    "\n",
    "        raw_action = float(actions[0])\n",
    "        raw_action = max(self.action_low, min(self.action_high, raw_action))\n",
    "        gamma = 0.5 * (raw_action + 1.0)\n",
    "\n",
    "        r = float(rewards[0])\n",
    "        self.total_reward += r\n",
    "\n",
    "        if done:\n",
    "            self.episode_rewards.append(self.total_reward)\n",
    "            avg_g = np.mean(self.current_episode_gammas) if self.current_episode_gammas else 0.0\n",
    "            self.episode_gammas.append(avg_g)\n",
    "            self.total_reward = 0.0\n",
    "            self.current_episode_gammas.clear()\n",
    "            self.n_episodes += 1\n",
    "\n",
    "            if infos[0].get(\"terminal_reason\") == \"collision\":\n",
    "                self.n_collisions += 1\n",
    "        else:\n",
    "            self.current_episode_gammas.append(gamma)\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _moving_average(self, data, window=10):\n",
    "        if len(data) < window:\n",
    "            return np.array(data)\n",
    "        return np.convolve(data, np.ones(window)/window, mode='valid')\n",
    "\n",
    "    def save_metrics(self, save_dir=\"training_metrics\"):\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        # Plot Episode Reward with Moving Average\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(self.episode_rewards, label=\"Episode Reward\", alpha=0.6)\n",
    "        ma_rewards = self._moving_average(self.episode_rewards, window=10)\n",
    "        plt.plot(range(10 - 1, len(self.episode_rewards)), ma_rewards, label=\"Moving Average (window=10)\", linewidth=2)\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Reward\")\n",
    "        plt.title(\"Episode Reward Over Time\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(save_dir, \"episode_reward.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        # Plot Average Gamma with Moving Average\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(self.episode_gammas, label=\"Average Gamma\", alpha=0.6)\n",
    "        ma_gamma = self._moving_average(self.episode_gammas, window=10)\n",
    "        plt.plot(range(10 - 1, len(self.episode_gammas)), ma_gamma, label=\"Moving Average (window=10)\", linewidth=2)\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Gamma\")\n",
    "        plt.title(\"Average Gamma per Episode\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(save_dir, \"average_gamma.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        with open(os.path.join(save_dir, \"training_summary.txt\"), 'w') as f:\n",
    "            f.write(f\"Total Episodes: {len(self.episode_rewards)}\\n\")\n",
    "            if self.episode_rewards:\n",
    "                f.write(f\"Mean Reward: {np.mean(self.episode_rewards):.3f}\\n\")\n",
    "                f.write(f\"Mean Gamma: {np.mean(self.episode_gammas):.3f}\\n\")\n",
    "            f.write(f\"Collisions Count: {self.n_collisions}\\n\")\n",
    "\n",
    "###############################################################################\n",
    "# ENVIRONMENT: DemoArbitrationEnv with Reproducible Randomization\n",
    "###############################################################################\n",
    "class DemoArbitrationEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    The dot must be extremely close to the goal (dist < 10) to get +alpha*g.\n",
    "    Otherwise it sees -beta*g => big punish for high gamma away from the goal.\n",
    "    Collisions => -2 and end the episode.\n",
    "    \n",
    "    This version supports two modes:\n",
    "      - If fixed_scenario_seed is provided, the environment will always use that seed\n",
    "        to generate the goals and obstacles.\n",
    "      - If scenario_mode is True (and fixed_scenario_seed is None), then every 10 episodes\n",
    "        the environment will cycle through a predefined list of 5 seeds.\n",
    "    \"\"\"\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": RENDER_FPS}\n",
    "\n",
    "    def __init__(self, visualize=False, scenario_mode=False, fixed_scenario_seed=None):\n",
    "        super().__init__()\n",
    "        self.visualize = visualize\n",
    "\n",
    "        low  = np.array([0, 0, -1, -1, 0, 0, -1, -1, 0], dtype=np.float32)\n",
    "        high = np.array([\n",
    "            FULL_VIEW_SIZE[0], FULL_VIEW_SIZE[1],\n",
    "            1, 1,\n",
    "            FULL_VIEW_SIZE[0], FULL_VIEW_SIZE[1],\n",
    "            1, 1,\n",
    "            1\n",
    "        ], dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=low, high=high, shape=(9,), dtype=np.float32)\n",
    "        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(), dtype=np.float32)\n",
    "\n",
    "        self.dot_pos = None\n",
    "        self.goal_pos = None\n",
    "        self.step_count = 0\n",
    "        self.max_steps = 300\n",
    "        self.episode_reward = 0.0\n",
    "        self.max_dist = math.hypot(FULL_VIEW_SIZE[0], FULL_VIEW_SIZE[1])\n",
    "        self.close_threshold = 10.0\n",
    "        self.alpha = 3.0\n",
    "        self.beta  = 3.0\n",
    "\n",
    "        # For reproducible scenarios:\n",
    "        self.scenario_mode = scenario_mode\n",
    "        self.fixed_scenario_seed = fixed_scenario_seed\n",
    "        self.scenario_seed = fixed_scenario_seed  # if fixed, use it always\n",
    "        self.last_scenario_seed = None  # store the seed used at last randomization\n",
    "        # Predefined seeds for scenarios if cycling\n",
    "        self.SCENARIO_SEEDS = [42, 123, 2021, 777, 999]\n",
    "        self.scenario_index = 0\n",
    "\n",
    "        # Initially randomize the environment.\n",
    "        self.randomize_env()\n",
    "\n",
    "        if self.visualize and pygame is not None:\n",
    "            pygame.init()\n",
    "            self.window = pygame.display.set_mode(FULL_VIEW_SIZE)\n",
    "            pygame.display.set_caption(\"Extremely Close => High Gamma\")\n",
    "            self.clock = pygame.time.Clock()\n",
    "        else:\n",
    "            self.window = None\n",
    "            self.clock = None\n",
    "\n",
    "        self.episode_counter = 0\n",
    "\n",
    "    def randomize_env(self):\n",
    "        \"\"\"\n",
    "        Randomize positions for goals and obstacles.\n",
    "        If self.scenario_seed is set, use it for reproducibility.\n",
    "        \"\"\"\n",
    "        if self.scenario_seed is not None:\n",
    "            random.seed(self.scenario_seed)\n",
    "            np.random.seed(self.scenario_seed)\n",
    "\n",
    "        N_GOALS = 8\n",
    "        N_OBSTACLES = 5\n",
    "        margin = 50 * SCALING_FACTOR\n",
    "        min_goal_distance = 300 * SCALING_FACTOR\n",
    "\n",
    "        new_goals = []\n",
    "        attempts = 0\n",
    "        while len(new_goals) < N_GOALS and attempts < 1000:\n",
    "            x = random.uniform(margin, FULL_VIEW_SIZE[0] - margin)\n",
    "            y = random.uniform(margin, FULL_VIEW_SIZE[1] - margin)\n",
    "            candidate = np.array([x, y], dtype=np.float32)\n",
    "            if distance(candidate, START_POS) >= min_goal_distance:\n",
    "                if all(distance(candidate, g) >= TARGET_RADIUS * 2 for g in new_goals):\n",
    "                    new_goals.append(candidate)\n",
    "            attempts += 1\n",
    "        self.goals = new_goals\n",
    "\n",
    "        new_obstacles = []\n",
    "        for _ in range(N_OBSTACLES):\n",
    "            goal = random.choice(self.goals)\n",
    "            t = random.uniform(0.3, 0.7)\n",
    "            base_point = START_POS + t * (goal - START_POS)\n",
    "            vec = goal - START_POS\n",
    "            if np.linalg.norm(vec) < 1e-6:\n",
    "                offset = np.array([0, 0], dtype=np.float32)\n",
    "            else:\n",
    "                perp = np.array([-vec[1], vec[0]], dtype=np.float32)\n",
    "                perp /= np.linalg.norm(perp)\n",
    "                offset_magnitude = random.uniform(-30 * SCALING_FACTOR, 30 * SCALING_FACTOR)\n",
    "                offset = perp * offset_magnitude\n",
    "            candidate = base_point + offset\n",
    "            candidate[0] = np.clip(candidate[0], margin, FULL_VIEW_SIZE[0] - margin)\n",
    "            candidate[1] = np.clip(candidate[1], margin, FULL_VIEW_SIZE[1] - margin)\n",
    "            valid = True\n",
    "            if distance(candidate, START_POS) < (DOT_RADIUS + OBSTACLE_RADIUS + 10):\n",
    "                valid = False\n",
    "            for g in self.goals:\n",
    "                if distance(candidate, g) < (TARGET_RADIUS + OBSTACLE_RADIUS + 10):\n",
    "                    valid = False\n",
    "            for obs in new_obstacles:\n",
    "                if distance(candidate, obs) < (2 * OBSTACLE_RADIUS + 10):\n",
    "                    valid = False\n",
    "            if valid:\n",
    "                new_obstacles.append(candidate)\n",
    "            else:\n",
    "                candidate = base_point.copy()\n",
    "                candidate[0] = np.clip(candidate[0], margin, FULL_VIEW_SIZE[0] - margin)\n",
    "                candidate[1] = np.clip(candidate[1], margin, FULL_VIEW_SIZE[1] - margin)\n",
    "                valid = True\n",
    "                for g in self.goals:\n",
    "                    if distance(candidate, g) < (TARGET_RADIUS + OBSTACLE_RADIUS + 10):\n",
    "                        valid = False\n",
    "                for obs in new_obstacles:\n",
    "                    if distance(candidate, obs) < (2 * OBSTACLE_RADIUS + 10):\n",
    "                        valid = False\n",
    "                if valid:\n",
    "                    new_obstacles.append(candidate)\n",
    "        self.obstacles = new_obstacles\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.episode_counter += 1\n",
    "        # If a fixed scenario seed is provided, always use that.\n",
    "        if self.fixed_scenario_seed is not None:\n",
    "            self.scenario_seed = self.fixed_scenario_seed\n",
    "            self.last_scenario_seed = self.scenario_seed\n",
    "            self.randomize_env()\n",
    "        # If scenario_mode is active, update every 10 episodes.\n",
    "        elif self.scenario_mode:\n",
    "            if self.episode_counter % 10 == 0:\n",
    "                self.scenario_seed = self.SCENARIO_SEEDS[self.scenario_index]\n",
    "                self.last_scenario_seed = self.scenario_seed\n",
    "                self.scenario_index = (self.scenario_index + 1) % len(self.SCENARIO_SEEDS)\n",
    "                self.randomize_env()\n",
    "        else:\n",
    "            # Default: randomize every 10 episodes without fixed seeds.\n",
    "            if self.episode_counter % 10 == 0:\n",
    "                self.randomize_env()\n",
    "\n",
    "        self.step_count = 0\n",
    "        self.episode_reward = 0.0\n",
    "        self.dot_pos = START_POS.copy()\n",
    "        # Choose a random goal from the current list.\n",
    "        if self.goals:\n",
    "            idx = random.randint(0, len(self.goals) - 1)\n",
    "            self.goal_pos = self.goals[idx].copy()\n",
    "        else:\n",
    "            self.goal_pos = np.array([\n",
    "                random.uniform(0.2 * FULL_VIEW_SIZE[0], 0.8 * FULL_VIEW_SIZE[0]),\n",
    "                random.uniform(0.2 * FULL_VIEW_SIZE[1], 0.8 * FULL_VIEW_SIZE[1])\n",
    "            ], dtype=np.float32)\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        raw_a = float(action)\n",
    "        raw_a = np.clip(raw_a, -1.0, 1.0)\n",
    "        gamma = 0.5 * (raw_a + 1.0)\n",
    "        self.step_count += 1\n",
    "\n",
    "        w_dir = potential_field_dir(self.dot_pos, self.goal_pos, self.obstacles)\n",
    "        noise = np.random.normal(0, NOISE_MAGNITUDE, size=2)\n",
    "        h_dir = w_dir + noise\n",
    "        hm = np.hypot(h_dir[0], h_dir[1])\n",
    "        if hm > 1e-6:\n",
    "            h_dir /= hm\n",
    "\n",
    "        c_dir = gamma * w_dir + (1 - gamma) * h_dir\n",
    "        cm = np.hypot(c_dir[0], c_dir[1])\n",
    "        if cm > 1e-6:\n",
    "            c_dir /= cm\n",
    "\n",
    "        move_vec = c_dir * MAX_SPEED\n",
    "        new_pos = self.dot_pos + move_vec\n",
    "        if not line_collision(self.dot_pos, new_pos, self.obstacles):\n",
    "            new_pos[0] = np.clip(new_pos[0], 0, FULL_VIEW_SIZE[0])\n",
    "            new_pos[1] = np.clip(new_pos[1], 0, FULL_VIEW_SIZE[1])\n",
    "            self.dot_pos = new_pos\n",
    "\n",
    "        collided = inside_obstacle(self.dot_pos, self.obstacles)\n",
    "        info = {}\n",
    "        if collided:\n",
    "            original_reward = -2.0\n",
    "            done = True\n",
    "            info[\"terminal_reason\"] = \"collision\"\n",
    "        else:\n",
    "            original_reward = 0.0\n",
    "            done = False\n",
    "            info[\"terminal_reason\"] = None\n",
    "\n",
    "        truncated = (self.step_count >= self.max_steps)\n",
    "        if truncated and not done:\n",
    "            info[\"terminal_reason\"] = \"timeout\"\n",
    "\n",
    "        dist_g = distance(self.dot_pos, self.goal_pos)\n",
    "        if dist_g < self.close_threshold:\n",
    "            shaping_reward = self.alpha * gamma\n",
    "        else:\n",
    "            shaping_reward = -self.beta * gamma\n",
    "\n",
    "        reward = original_reward + shaping_reward\n",
    "        self.episode_reward += reward\n",
    "\n",
    "        if self.visualize:\n",
    "            self._render()\n",
    "\n",
    "        obs = self._get_obs()\n",
    "        return obs, float(reward), done, truncated, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        to_g = self.goal_pos - self.dot_pos\n",
    "        d = math.hypot(to_g[0], to_g[1])\n",
    "        dist_ratio = d / self.max_dist if self.max_dist > 1e-6 else 0.0\n",
    "\n",
    "        w_dir = potential_field_dir(self.dot_pos, self.goal_pos, self.obstacles)\n",
    "        noise = np.random.normal(0, NOISE_MAGNITUDE, size=2)\n",
    "        h_dir = w_dir + noise\n",
    "        hm = np.hypot(h_dir[0], h_dir[1])\n",
    "        if hm > 1e-6:\n",
    "            h_dir /= hm\n",
    "\n",
    "        obs = np.concatenate([\n",
    "            self.dot_pos,\n",
    "            h_dir,\n",
    "            self.goal_pos,\n",
    "            w_dir,\n",
    "            [dist_ratio]\n",
    "        ]).astype(np.float32)\n",
    "        return obs\n",
    "\n",
    "    def _render(self):\n",
    "        if not self.window or not pygame:\n",
    "            return\n",
    "\n",
    "        self.window.fill(WHITE)\n",
    "        for obs in self.obstacles:\n",
    "            pygame.draw.circle(self.window, GRAY, (int(obs[0]), int(obs[1])), OBSTACLE_RADIUS)\n",
    "        for gpos in self.goals:\n",
    "            pygame.draw.circle(self.window, YELLOW, (int(gpos[0]), int(gpos[1])), TARGET_RADIUS)\n",
    "        pygame.draw.circle(self.window, BLACK, (int(self.goal_pos[0]), int(self.goal_pos[1])), TARGET_RADIUS + 2, width=2)\n",
    "        pygame.draw.circle(self.window, BLACK, (int(self.dot_pos[0]), int(self.dot_pos[1])), DOT_RADIUS, width=2)\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(RENDER_FPS)\n",
    "\n",
    "    def close(self):\n",
    "        if self.visualize and pygame is not None:\n",
    "            pygame.quit()\n",
    "        super().close()\n",
    "\n",
    "###############################################################################\n",
    "# TRAINING FUNCTION\n",
    "###############################################################################\n",
    "def train(visualize=False, total_timesteps=300_000):\n",
    "    from stable_baselines3.common.callbacks import CallbackList\n",
    "    # Enable scenario_mode so that every 10 episodes a new scenario is used\n",
    "    env = DemoArbitrationEnv(visualize=visualize, scenario_mode=True)\n",
    "    metrics_callback = MetricsCallback()\n",
    "    callback = CallbackList([metrics_callback])\n",
    "\n",
    "    model = PPO(\n",
    "        policy=\"MlpPolicy\",\n",
    "        env=env,\n",
    "        learning_rate=3e-4,\n",
    "        n_steps=1024,\n",
    "        batch_size=1024,\n",
    "        n_epochs=4,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        clip_range=0.2,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        print(f\"Starting PPO training (visualize={visualize}) ...\")\n",
    "        model.learn(total_timesteps=total_timesteps, callback=callback)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training interrupted; saving partial model...\")\n",
    "\n",
    "    os.makedirs(\"trained_models\", exist_ok=True)\n",
    "    model.save(\"extreme_close_gamma_ppo\")\n",
    "    print(\"Model saved to trained_models/extreme_close_gamma_ppo.zip\")\n",
    "    metrics_callback.save_metrics(\"training_metrics\")\n",
    "    print(\"Metrics saved in training_metrics/\")\n",
    "    env.close()\n",
    "\n",
    "###############################################################################\n",
    "# MAIN\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    vis = (len(sys.argv) > 1 and sys.argv[1].lower() == \"visualize\")\n",
    "    train(visualize=vis, total_timesteps=300_000)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
