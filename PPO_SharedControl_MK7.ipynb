{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Starting PPO training (visualize=False) ...\n",
      "Logging to ./ppo_tensorboard/PPO_12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 300      |\n",
      "|    ep_rew_mean     | 82.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 759      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 300          |\n",
      "|    ep_rew_mean          | 107          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 786          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 2048         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002526039 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -0.0208      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 230          |\n",
      "|    n_updates            | 4            |\n",
      "|    policy_gradient_loss | -0.000463    |\n",
      "|    std                  | 0.999        |\n",
      "|    value_loss           | 463          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 300          |\n",
      "|    ep_rew_mean          | 100          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 797          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 3072         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.225911e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.00259      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 268          |\n",
      "|    n_updates            | 8            |\n",
      "|    policy_gradient_loss | -0.000316    |\n",
      "|    std                  | 0.999        |\n",
      "|    value_loss           | 538          |\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Only needed if visualize=True\n",
    "try:\n",
    "    import pygame\n",
    "except ImportError:\n",
    "    pygame = None\n",
    "\n",
    "###############################################################################\n",
    "# ENVIRONMENT CONSTANTS\n",
    "###############################################################################\n",
    "FULL_VIEW_SIZE = (1200, 800)\n",
    "SCALING_FACTOR_X = FULL_VIEW_SIZE[0] / 600.0\n",
    "SCALING_FACTOR_Y = FULL_VIEW_SIZE[1] / 600.0\n",
    "SCALING_FACTOR   = (SCALING_FACTOR_X + SCALING_FACTOR_Y) / 2\n",
    "\n",
    "DOT_RADIUS       = int(15 * SCALING_FACTOR)\n",
    "TARGET_RADIUS    = int(10 * SCALING_FACTOR)\n",
    "OBSTACLE_RADIUS  = int(10 * SCALING_FACTOR)\n",
    "COLLISION_BUFFER = int(5  * SCALING_FACTOR)\n",
    "MAX_SPEED        = 3 * SCALING_FACTOR\n",
    "NOISE_MAGNITUDE  = 0.5\n",
    "RENDER_FPS       = 30\n",
    "\n",
    "# The start position is fixed in the middle.\n",
    "START_POS = np.array([FULL_VIEW_SIZE[0]//2, FULL_VIEW_SIZE[1]//2], dtype=np.float32)\n",
    "\n",
    "WHITE = (255,255,255)\n",
    "GRAY  = (128,128,128)\n",
    "YELLOW= (255,255,0)\n",
    "BLACK = (0,0,0)\n",
    "\n",
    "###############################################################################\n",
    "# UTILITY FUNCTIONS\n",
    "###############################################################################\n",
    "def distance(a, b):\n",
    "    return math.hypot(a[0]-b[0], a[1]-b[1])\n",
    "\n",
    "def check_line_collision(start, end, center, radius):\n",
    "    dx = end[0] - start[0]\n",
    "    dy = end[1] - start[1]\n",
    "    fx = center[0] - start[0]\n",
    "    fy = center[1] - start[1]\n",
    "    l2 = dx*dx + dy*dy\n",
    "    if l2 < 1e-9:\n",
    "        return distance(start, center) <= radius\n",
    "    t = max(0, min(1, (fx*dx + fy*dy) / l2))\n",
    "    px = start[0] + t*dx\n",
    "    py = start[1] + t*dy\n",
    "    return distance((px,py), center) <= radius\n",
    "\n",
    "def line_collision(pos, new_pos, obstacles):\n",
    "    for obs in obstacles:\n",
    "        if check_line_collision(pos, new_pos, obs, OBSTACLE_RADIUS + COLLISION_BUFFER):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def inside_obstacle(pos, obstacles):\n",
    "    for obs in obstacles:\n",
    "        if distance(pos, obs) <= (OBSTACLE_RADIUS + DOT_RADIUS):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def potential_field_dir(pos, goal, obstacles):\n",
    "    \"\"\"\n",
    "    Compute a normalized direction vector that is a combination of an attractive \n",
    "    force toward the goal and repulsive forces from obstacles.\n",
    "    \"\"\"\n",
    "    gx = goal[0] - pos[0]\n",
    "    gy = goal[1] - pos[1]\n",
    "    dg = math.hypot(gx, gy)\n",
    "    if dg < 1e-6:\n",
    "        return np.zeros(2, dtype=np.float32)\n",
    "    att = np.array([gx/dg, gy/dg], dtype=np.float32)\n",
    "\n",
    "    repulse_x = 0.0\n",
    "    repulse_y = 0.0\n",
    "    repulsion_radius = 150.0 * SCALING_FACTOR\n",
    "    repulsion_gain   = 15000.0\n",
    "\n",
    "    for obs in obstacles:\n",
    "        dx = pos[0] - obs[0]\n",
    "        dy = pos[1] - obs[1]\n",
    "        dobs = math.hypot(dx, dy)\n",
    "        if dobs < 1e-9:\n",
    "            continue\n",
    "        if dobs < repulsion_radius:\n",
    "            pushx   = dx/dobs\n",
    "            pushy   = dy/dobs\n",
    "            strength= repulsion_gain/(dobs**2)\n",
    "            repulse_x += pushx*strength\n",
    "            repulse_y += pushy*strength\n",
    "\n",
    "    px = att[0] + repulse_x\n",
    "    py = att[1] + repulse_y\n",
    "    mg = math.hypot(px, py)\n",
    "    if mg < 1e-9:\n",
    "        return np.zeros(2, dtype=np.float32)\n",
    "    return np.array([px/mg, py/mg], dtype=np.float32)\n",
    "\n",
    "###############################################################################\n",
    "# CALLBACK WITH MOVING AVERAGE METRICS\n",
    "###############################################################################\n",
    "class MetricsCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.episode_rewards = []\n",
    "        self.episode_gammas  = []\n",
    "        self.current_episode_gammas = []\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "        # Arrays to store training metrics after each update/rollout\n",
    "        self.training_steps = []\n",
    "        self.losses = []\n",
    "        self.value_losses = []\n",
    "        self.policy_losses = []\n",
    "        self.entropy_losses = []\n",
    "\n",
    "        self.n_collisions = 0\n",
    "        self.n_episodes   = 0\n",
    "        self.action_low  = None\n",
    "        self.action_high = None\n",
    "        self.n_updates = 0\n",
    "\n",
    "    def _on_training_start(self):\n",
    "        self.n_collisions = 0\n",
    "        self.n_episodes = 0\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"\n",
    "        Called at each environment step (i.e., for each action taken).\n",
    "        We collect reward and gamma stats here.\n",
    "        \"\"\"\n",
    "        if self.action_low is None:\n",
    "            # Just read from the model's action space once\n",
    "            self.action_low  = float(self.model.action_space.low)\n",
    "            self.action_high = float(self.model.action_space.high)\n",
    "\n",
    "        actions = self.locals['actions']\n",
    "        rewards = self.locals['rewards']\n",
    "        done    = self.locals['dones'][0]\n",
    "        infos   = self.locals['infos']\n",
    "\n",
    "        raw_action = float(actions[0])\n",
    "        raw_action = max(self.action_low, min(self.action_high, raw_action))\n",
    "        gamma = 0.5 * (raw_action + 1.0)\n",
    "\n",
    "        r = float(rewards[0])\n",
    "        self.total_reward += r\n",
    "\n",
    "        if done:\n",
    "            # Episode is finished, store stats\n",
    "            self.episode_rewards.append(self.total_reward)\n",
    "            avg_g = np.mean(self.current_episode_gammas) if self.current_episode_gammas else 0.0\n",
    "            self.episode_gammas.append(avg_g)\n",
    "            self.total_reward = 0.0\n",
    "            self.current_episode_gammas.clear()\n",
    "            self.n_episodes += 1\n",
    "\n",
    "            if infos[0].get(\"terminal_reason\") == \"collision\":\n",
    "                self.n_collisions += 1\n",
    "        else:\n",
    "            self.current_episode_gammas.append(gamma)\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        \"\"\"\n",
    "        Called after each rollout is completed and PPO has updated its policy.\n",
    "        Here, we can retrieve the training losses from the logger.\n",
    "        \"\"\"\n",
    "        # Grab the training logs from stable-baselines3\n",
    "        logs = self.model.logger.name_to_value\n",
    "\n",
    "        # If training was performed, these keys should exist:\n",
    "        # train/loss, train/policy_gradient_loss, train/value_loss, train/entropy_loss, etc.\n",
    "        # We'll store them if available:\n",
    "        if \"train/loss\" in logs:\n",
    "            self.losses.append(logs[\"train/loss\"])\n",
    "            self.n_updates += 1\n",
    "            self.training_steps.append(self.n_updates)\n",
    "\n",
    "        if \"train/policy_gradient_loss\" in logs:\n",
    "            self.policy_losses.append(logs[\"train/policy_gradient_loss\"])\n",
    "        if \"train/value_loss\" in logs:\n",
    "            self.value_losses.append(logs[\"train/value_loss\"])\n",
    "        if \"train/entropy_loss\" in logs:\n",
    "            self.entropy_losses.append(logs[\"train/entropy_loss\"])\n",
    "\n",
    "    def _moving_average(self, data, window=10):\n",
    "        if len(data) < window:\n",
    "            return np.array(data)\n",
    "        return np.convolve(data, np.ones(window)/window, mode='valid')\n",
    "\n",
    "    def save_metrics(self, save_dir=\"training_metrics\"):\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        # Plot Episode Reward with Moving Average\n",
    "        if self.episode_rewards:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(self.episode_rewards, label=\"Episode Reward\", alpha=0.6)\n",
    "            ma_rewards = self._moving_average(self.episode_rewards, window=10)\n",
    "            if len(ma_rewards) > 0:\n",
    "                # The moving average array is shorter, so align its x-axis\n",
    "                start_index = 10 - 1\n",
    "                plt.plot(range(start_index, start_index + len(ma_rewards)), \n",
    "                         ma_rewards, label=\"Moving Average (window=10)\", linewidth=2)\n",
    "            plt.xlabel(\"Episode\")\n",
    "            plt.ylabel(\"Reward\")\n",
    "            plt.title(\"Episode Reward Over Time\")\n",
    "            plt.grid(True)\n",
    "            plt.legend()\n",
    "            plt.savefig(os.path.join(save_dir, \"episode_reward.png\"))\n",
    "            plt.close()\n",
    "\n",
    "        # Plot Average Gamma with Moving Average\n",
    "        if self.episode_gammas:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(self.episode_gammas, label=\"Average Gamma\", alpha=0.6)\n",
    "            ma_gamma = self._moving_average(self.episode_gammas, window=10)\n",
    "            if len(ma_gamma) > 0:\n",
    "                start_index = 10 - 1\n",
    "                plt.plot(range(start_index, start_index + len(ma_gamma)), \n",
    "                         ma_gamma, label=\"Moving Average (window=10)\", linewidth=2)\n",
    "            plt.xlabel(\"Episode\")\n",
    "            plt.ylabel(\"Gamma\")\n",
    "            plt.title(\"Average Gamma per Episode\")\n",
    "            plt.grid(True)\n",
    "            plt.legend()\n",
    "            plt.savefig(os.path.join(save_dir, \"average_gamma.png\"))\n",
    "            plt.close()\n",
    "        \n",
    "        # Plot Loss with Moving Average (if we have loss data)\n",
    "        if self.losses:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(self.training_steps, self.losses, label=\"Training Loss\", alpha=0.6)\n",
    "            if len(self.losses) >= 10:\n",
    "                ma_loss = self._moving_average(self.losses, window=10)\n",
    "                # The moving average array is shorter, align x-axis:\n",
    "                plt.plot(range(self.training_steps[9], self.training_steps[9] + len(ma_loss)), \n",
    "                         ma_loss, label=\"Moving Average (window=10)\", linewidth=2)\n",
    "            plt.xlabel(\"Training Updates\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.title(\"Total Loss During Training\")\n",
    "            plt.grid(True)\n",
    "            plt.legend()\n",
    "            plt.savefig(os.path.join(save_dir, \"training_loss.png\"))\n",
    "            plt.close()\n",
    "        \n",
    "        # Plot value loss if available\n",
    "        if self.value_losses and len(self.value_losses) == len(self.losses):\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(self.training_steps, self.value_losses, label=\"Value Loss\", alpha=0.6, color='red')\n",
    "            if len(self.value_losses) >= 10:\n",
    "                ma_val_loss = self._moving_average(self.value_losses, window=10)\n",
    "                plt.plot(range(self.training_steps[9], self.training_steps[9] + len(ma_val_loss)),\n",
    "                         ma_val_loss, label=\"Moving Average (window=10)\", linewidth=2, color='darkred')\n",
    "            plt.xlabel(\"Training Updates\")\n",
    "            plt.ylabel(\"Value Loss\")\n",
    "            plt.title(\"Value Loss During Training\")\n",
    "            plt.grid(True)\n",
    "            plt.legend()\n",
    "            plt.savefig(os.path.join(save_dir, \"value_loss.png\"))\n",
    "            plt.close()\n",
    "        \n",
    "        # Plot policy loss if available\n",
    "        if self.policy_losses and len(self.policy_losses) == len(self.losses):\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(self.training_steps, self.policy_losses, label=\"Policy Loss\", alpha=0.6, color='green')\n",
    "            if len(self.policy_losses) >= 10:\n",
    "                ma_pol_loss = self._moving_average(self.policy_losses, window=10)\n",
    "                plt.plot(range(self.training_steps[9], self.training_steps[9] + len(ma_pol_loss)),\n",
    "                         ma_pol_loss, label=\"Moving Average (window=10)\", linewidth=2, color='darkgreen')\n",
    "            plt.xlabel(\"Training Updates\")\n",
    "            plt.ylabel(\"Policy Loss\")\n",
    "            plt.title(\"Policy Gradient Loss During Training\")\n",
    "            plt.grid(True)\n",
    "            plt.legend()\n",
    "            plt.savefig(os.path.join(save_dir, \"policy_loss.png\"))\n",
    "            plt.close()\n",
    "\n",
    "        # Plot entropy loss if available (not strictly asked, but often useful)\n",
    "        if self.entropy_losses and len(self.entropy_losses) == len(self.losses):\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(self.training_steps, self.entropy_losses, label=\"Entropy Loss\", alpha=0.6, color='purple')\n",
    "            if len(self.entropy_losses) >= 10:\n",
    "                ma_ent_loss = self._moving_average(self.entropy_losses, window=10)\n",
    "                plt.plot(range(self.training_steps[9], self.training_steps[9] + len(ma_ent_loss)),\n",
    "                         ma_ent_loss, label=\"Moving Average (window=10)\", linewidth=2, color='darkmagenta')\n",
    "            plt.xlabel(\"Training Updates\")\n",
    "            plt.ylabel(\"Entropy Loss\")\n",
    "            plt.title(\"Entropy Loss During Training\")\n",
    "            plt.grid(True)\n",
    "            plt.legend()\n",
    "            plt.savefig(os.path.join(save_dir, \"entropy_loss.png\"))\n",
    "            plt.close()\n",
    "\n",
    "        # Write out summary text file\n",
    "        with open(os.path.join(save_dir, \"training_summary.txt\"), 'w') as f:\n",
    "            f.write(f\"Total Episodes: {len(self.episode_rewards)}\\n\")\n",
    "            if self.episode_rewards:\n",
    "                f.write(f\"Mean Reward: {np.mean(self.episode_rewards):.3f}\\n\")\n",
    "                f.write(f\"Mean Gamma: {np.mean(self.episode_gammas):.3f}\\n\")\n",
    "            if self.losses:\n",
    "                f.write(f\"Final Loss: {self.losses[-1]:.6f}\\n\")\n",
    "                f.write(f\"Mean Loss: {np.mean(self.losses):.6f}\\n\")\n",
    "            if self.value_losses:\n",
    "                f.write(f\"Mean Value Loss: {np.mean(self.value_losses):.6f}\\n\")\n",
    "            if self.policy_losses:\n",
    "                f.write(f\"Mean Policy Loss: {np.mean(self.policy_losses):.6f}\\n\")\n",
    "            if self.entropy_losses:\n",
    "                f.write(f\"Mean Entropy Loss: {np.mean(self.entropy_losses):.6f}\\n\")\n",
    "            f.write(f\"Collisions Count: {self.n_collisions}\\n\")\n",
    "\n",
    "###############################################################################\n",
    "# ENVIRONMENT: DemoArbitrationEnv with Alternating Fixed Scenarios and Reward Shaping\n",
    "###############################################################################\n",
    "class DemoArbitrationEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    The dot must be extremely close to the goal to receive a positive shaping reward.\n",
    "    If the dot is in a \"risky\" region – near a goal or near an obstacle – the agent\n",
    "    is rewarded for a high gamma; otherwise, high gamma is penalized.\n",
    "    \n",
    "    This environment alternates among five fixed scenarios using seeds:\n",
    "       [0, 1, 2, 58, 487]\n",
    "    \"\"\"\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": RENDER_FPS}\n",
    "\n",
    "    def __init__(self, visualize=False, scenario_mode=True, fixed_scenario_seed=None):\n",
    "        super().__init__()\n",
    "        self.visualize = visualize\n",
    "\n",
    "        low  = np.array([0,0, -1,-1, 0,0, -1,-1, 0], dtype=np.float32)\n",
    "        high = np.array([\n",
    "            FULL_VIEW_SIZE[0], FULL_VIEW_SIZE[1],\n",
    "            1,1,\n",
    "            FULL_VIEW_SIZE[0], FULL_VIEW_SIZE[1],\n",
    "            1,1,\n",
    "            1\n",
    "        ], dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=low, high=high, shape=(9,), dtype=np.float32)\n",
    "        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(), dtype=np.float32)\n",
    "\n",
    "        self.dot_pos = None\n",
    "        self.goal_pos = None\n",
    "        self.step_count = 0\n",
    "        self.max_steps = 300\n",
    "        self.episode_reward = 0.0\n",
    "        self.max_dist = math.hypot(FULL_VIEW_SIZE[0], FULL_VIEW_SIZE[1])\n",
    "        self.close_threshold = 10.0\n",
    "        self.alpha = 3.0\n",
    "        self.beta  = 3.0\n",
    "\n",
    "        # Additional thresholds for reward shaping.\n",
    "        self.goal_threshold = 100.0   # if within 100 units of the goal, high gamma is good.\n",
    "        self.obs_threshold  = 100.0   # if within 100 units of any obstacle, high gamma is good.\n",
    "\n",
    "        # For reproducible scenarios:\n",
    "        self.scenario_mode = scenario_mode\n",
    "        self.fixed_scenario_seed = fixed_scenario_seed\n",
    "        self.scenario_seed = fixed_scenario_seed  # if fixed, use it always\n",
    "        self.last_scenario_seed = None\n",
    "        # Fixed seeds to alternate between (matching demo): 0, 1, 2, 58, 487.\n",
    "        self.SCENARIO_SEEDS = [0, 1, 2, 58, 487]\n",
    "        self.scenario_index = 0\n",
    "\n",
    "        # Initially randomize the environment.\n",
    "        self.randomize_env()\n",
    "\n",
    "        if self.visualize and pygame is not None:\n",
    "            pygame.init()\n",
    "            self.window = pygame.display.set_mode(FULL_VIEW_SIZE)\n",
    "            pygame.display.set_caption(\"Demo Arbitration Environment\")\n",
    "            self.clock = pygame.time.Clock()\n",
    "        else:\n",
    "            self.window = None\n",
    "            self.clock = None\n",
    "\n",
    "        self.episode_counter = 0\n",
    "\n",
    "    def randomize_env(self):\n",
    "        \"\"\"\n",
    "        Generate goals and obstacles exactly like in the demo code.\n",
    "        \"\"\"\n",
    "        if self.scenario_seed is not None:\n",
    "            random.seed(self.scenario_seed)\n",
    "            np.random.seed(self.scenario_seed)\n",
    "\n",
    "        N_GOALS = 8\n",
    "        N_OBSTACLES = 5\n",
    "        margin = 50 * SCALING_FACTOR\n",
    "        min_goal_distance = 300 * SCALING_FACTOR\n",
    "\n",
    "        new_goals = []\n",
    "        attempts = 0\n",
    "        while len(new_goals) < N_GOALS and attempts < 1000:\n",
    "            x = random.uniform(margin, FULL_VIEW_SIZE[0] - margin)\n",
    "            y = random.uniform(margin, FULL_VIEW_SIZE[1] - margin)\n",
    "            candidate = np.array([x, y], dtype=np.float32)\n",
    "            if distance(candidate, START_POS) >= min_goal_distance:\n",
    "                if all(distance(candidate, g) >= TARGET_RADIUS * 2 for g in new_goals):\n",
    "                    new_goals.append(candidate)\n",
    "            attempts += 1\n",
    "        self.goals = new_goals\n",
    "\n",
    "        new_obstacles = []\n",
    "        # For obstacles, we choose a subset of goals to attach obstacles.\n",
    "        if len(new_goals) > 1:\n",
    "            obstacle_goals = random.sample(new_goals, k=min(min(N_GOALS-1, N_OBSTACLES), len(new_goals)-1))\n",
    "        else:\n",
    "            obstacle_goals = new_goals\n",
    "        for goal in obstacle_goals:\n",
    "            # Place obstacle between start and goal: choose t in [0.6, 0.8]\n",
    "            t = random.uniform(0.6, 0.8)\n",
    "            base_point = START_POS + t * (goal - START_POS)\n",
    "            # Compute perpendicular vector.\n",
    "            vec = goal - START_POS\n",
    "            if np.linalg.norm(vec) < 1e-6:\n",
    "                perp = np.array([0, 0], dtype=np.float32)\n",
    "            else:\n",
    "                perp = np.array([-vec[1], vec[0]], dtype=np.float32)\n",
    "                perp /= np.linalg.norm(perp)\n",
    "            # Apply offset: push obstacle away from the direct line.\n",
    "            offset_mag = random.uniform(20 * SCALING_FACTOR, 40 * SCALING_FACTOR)\n",
    "            offset = perp * offset_mag * random.choice([-1,1])\n",
    "            candidate = base_point + offset\n",
    "            candidate[0] = np.clip(candidate[0], margin, FULL_VIEW_SIZE[0] - margin)\n",
    "            candidate[1] = np.clip(candidate[1], margin, FULL_VIEW_SIZE[1] - margin)\n",
    "            valid = True\n",
    "            # Ensure candidate is not too close to the start.\n",
    "            if distance(candidate, START_POS) < (DOT_RADIUS + OBSTACLE_RADIUS + 10):\n",
    "                valid = False\n",
    "            # Ensure candidate is not too close to the goal (to keep goal visible).\n",
    "            if distance(candidate, goal) < (TARGET_RADIUS + OBSTACLE_RADIUS + 20):\n",
    "                valid = False\n",
    "            for obs in new_obstacles:\n",
    "                if distance(candidate, obs) < (2 * OBSTACLE_RADIUS + 10):\n",
    "                    valid = False\n",
    "            if valid:\n",
    "                new_obstacles.append(candidate)\n",
    "        self.obstacles = new_obstacles\n",
    "\n",
    "        # Randomly choose a goal for the episode.\n",
    "        idx = random.randint(0, len(self.goals) - 1)\n",
    "        self.goal_pos = self.goals[idx].copy()\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.episode_counter += 1\n",
    "        # Alternate scenarios: if fixed seed is provided, always use it.\n",
    "        if self.fixed_scenario_seed is not None:\n",
    "            self.scenario_seed = self.fixed_scenario_seed\n",
    "            self.last_scenario_seed = self.scenario_seed\n",
    "            self.randomize_env()\n",
    "        elif self.scenario_mode:\n",
    "            if self.episode_counter % 10 == 0:\n",
    "                self.scenario_seed = self.SCENARIO_SEEDS[self.scenario_index]\n",
    "                self.last_scenario_seed = self.scenario_seed\n",
    "                self.scenario_index = (self.scenario_index + 1) % len(self.SCENARIO_SEEDS)\n",
    "                self.randomize_env()\n",
    "        else:\n",
    "            if self.episode_counter % 10 == 0:\n",
    "                self.randomize_env()\n",
    "\n",
    "        self.step_count = 0\n",
    "        self.episode_reward = 0.0\n",
    "        self.dot_pos = START_POS.copy()\n",
    "        if self.goals:\n",
    "            idx = random.randint(0, len(self.goals) - 1)\n",
    "            self.goal_pos = self.goals[idx].copy()\n",
    "        else:\n",
    "            self.goal_pos = np.array([\n",
    "                random.uniform(0.2*FULL_VIEW_SIZE[0], 0.8*FULL_VIEW_SIZE[0]),\n",
    "                random.uniform(0.2*FULL_VIEW_SIZE[1], 0.8*FULL_VIEW_SIZE[1])\n",
    "            ], dtype=np.float32)\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        raw_a = float(action)\n",
    "        raw_a = np.clip(raw_a, -1.0, 1.0)\n",
    "        gamma_val = 0.5*(raw_a+1.0)  # maps action [-1,1] to gamma in [0,1]\n",
    "        self.step_count += 1\n",
    "\n",
    "        # Compute the perfect direction.\n",
    "        w_dir = potential_field_dir(self.dot_pos, self.goal_pos, self.obstacles)\n",
    "        noise = np.random.normal(0, NOISE_MAGNITUDE, size=2)\n",
    "        h_dir = w_dir + noise\n",
    "        hm = np.hypot(h_dir[0], h_dir[1])\n",
    "        if hm > 1e-6:\n",
    "            h_dir /= hm\n",
    "\n",
    "        # Arbitration between perfect and noisy directions.\n",
    "        c_dir = gamma_val * w_dir + (1-gamma_val)*h_dir\n",
    "        cm = np.hypot(c_dir[0], c_dir[1])\n",
    "        if cm > 1e-6:\n",
    "            c_dir /= cm\n",
    "\n",
    "        move_vec = c_dir * MAX_SPEED\n",
    "        new_pos = self.dot_pos + move_vec\n",
    "        if not line_collision(self.dot_pos, new_pos, self.obstacles):\n",
    "            new_pos[0] = np.clip(new_pos[0], 0, FULL_VIEW_SIZE[0])\n",
    "            new_pos[1] = np.clip(new_pos[1], 0, FULL_VIEW_SIZE[1])\n",
    "            self.dot_pos = new_pos\n",
    "\n",
    "        collided = inside_obstacle(self.dot_pos, self.obstacles)\n",
    "        info = {}\n",
    "        if collided:\n",
    "            original_reward = -2.0\n",
    "            done = True\n",
    "            info[\"terminal_reason\"] = \"collision\"\n",
    "        else:\n",
    "            original_reward = 0.0\n",
    "            done = False\n",
    "            info[\"terminal_reason\"] = None\n",
    "\n",
    "        truncated = (self.step_count >= self.max_steps)\n",
    "        if truncated and not done:\n",
    "            info[\"terminal_reason\"] = \"timeout\"\n",
    "\n",
    "        # Reward shaping: if the dot is near the goal OR near any obstacle, reward high gamma.\n",
    "        d_goal = distance(self.dot_pos, self.goal_pos)\n",
    "        d_obs = min([distance(self.dot_pos, obs) for obs in self.obstacles]) if self.obstacles else 1e6\n",
    "        if d_goal < self.goal_threshold or d_obs < self.obs_threshold:\n",
    "            shaping_reward = self.alpha * gamma_val\n",
    "        else:\n",
    "            shaping_reward = -self.beta * gamma_val\n",
    "\n",
    "        reward = original_reward + shaping_reward\n",
    "        self.episode_reward += reward\n",
    "\n",
    "        obs = self._get_obs()\n",
    "        return obs, float(reward), done, truncated, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        to_g = self.goal_pos - self.dot_pos\n",
    "        d = math.hypot(to_g[0], to_g[1])\n",
    "        dist_ratio = d / self.max_dist if self.max_dist > 1e-6 else 0.0\n",
    "\n",
    "        w_dir = potential_field_dir(self.dot_pos, self.goal_pos, self.obstacles)\n",
    "        noise = np.random.normal(0, NOISE_MAGNITUDE, size=2)\n",
    "        h_dir = w_dir + noise\n",
    "        hm = math.hypot(h_dir[0], h_dir[1])\n",
    "        if hm > 1e-6:\n",
    "            h_dir /= hm\n",
    "\n",
    "        obs = np.concatenate([\n",
    "            self.dot_pos,\n",
    "            h_dir,\n",
    "            self.goal_pos,\n",
    "            w_dir,\n",
    "            [dist_ratio]\n",
    "        ]).astype(np.float32)\n",
    "        return obs\n",
    "\n",
    "    def _render(self):\n",
    "        if not self.window or not pygame:\n",
    "            return\n",
    "\n",
    "        self.window.fill(WHITE)\n",
    "        for obs in self.obstacles:\n",
    "            pygame.draw.circle(self.window, GRAY, (int(obs[0]), int(obs[1])), OBSTACLE_RADIUS)\n",
    "        for gpos in self.goals:\n",
    "            pygame.draw.circle(self.window, YELLOW, (int(gpos[0]), int(gpos[1])), TARGET_RADIUS)\n",
    "        pygame.draw.circle(self.window, BLACK, (int(self.goal_pos[0]), int(self.goal_pos[1])), TARGET_RADIUS+2, width=2)\n",
    "        pygame.draw.circle(self.window, BLACK, (int(self.dot_pos[0]), int(self.dot_pos[1])), DOT_RADIUS, width=2)\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(RENDER_FPS)\n",
    "\n",
    "    def close(self):\n",
    "        if self.visualize and pygame is not None:\n",
    "            pygame.quit()\n",
    "        super().close()\n",
    "\n",
    "###############################################################################\n",
    "# TRAINING FUNCTION\n",
    "###############################################################################\n",
    "def train(visualize=False, total_timesteps=300_000):\n",
    "    from stable_baselines3.common.callbacks import CallbackList\n",
    "    # Enable scenario_mode so that the environment alternates through our fixed seeds.\n",
    "    env = DemoArbitrationEnv(visualize=visualize, scenario_mode=True)\n",
    "    metrics_callback = MetricsCallback()\n",
    "    callback = CallbackList([metrics_callback])\n",
    "\n",
    "    model = PPO(\n",
    "        policy=\"MlpPolicy\",\n",
    "        env=env,\n",
    "        learning_rate=3e-4,\n",
    "        n_steps=1024,\n",
    "        batch_size=1024,\n",
    "        n_epochs=4,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        clip_range=0.2,\n",
    "        verbose=1,\n",
    "        tensorboard_log=\"./ppo_tensorboard/\"  # Add tensorboard logging\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        print(f\"Starting PPO training (visualize={visualize}) ...\")\n",
    "        model.learn(total_timesteps=total_timesteps, callback=callback, log_interval=1)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training interrupted; saving partial model...\")\n",
    "\n",
    "    os.makedirs(\"trained_models\", exist_ok=True)\n",
    "    model.save(\"trained_models/extreme_close_gamma_ppo2\")\n",
    "    print(\"Model saved to trained_models/extreme_close_gamma_ppo2.zip\")\n",
    "\n",
    "    metrics_callback.save_metrics(\"training_metrics\")\n",
    "    print(\"Metrics saved in training_metrics/\")\n",
    "    env.close()\n",
    "\n",
    "###############################################################################\n",
    "# MAIN\n",
    "###############################################################################\n",
    "if __name__==\"__main__\":\n",
    "    import sys\n",
    "    vis = (len(sys.argv)>1 and sys.argv[1].lower()==\"visualize\")\n",
    "    train(visualize=vis, total_timesteps=3_000_000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
