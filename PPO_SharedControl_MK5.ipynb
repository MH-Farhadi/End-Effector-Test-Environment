{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using cuda device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mhfar\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mhfar\\AppData\\Local\\Temp\\ipykernel_11476\\331671898.py:96: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  gamma = float(actions[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 2631 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 0    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 2596         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 1            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.749795e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.00237      |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 587          |\n",
      "|    n_updates            | 4            |\n",
      "|    policy_gradient_loss | -0.00089     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.18e+03     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 2893          |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 2             |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.4151872e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.0105        |\n",
      "|    learning_rate        | 3e-05         |\n",
      "|    loss                 | 492           |\n",
      "|    n_updates            | 8             |\n",
      "|    policy_gradient_loss | -0.000567     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 986           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 3063          |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 2             |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.8770836e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.0169        |\n",
      "|    learning_rate        | 3e-05         |\n",
      "|    loss                 | 534           |\n",
      "|    n_updates            | 12            |\n",
      "|    policy_gradient_loss | -9.86e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.07e+03      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 3174         |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.030193e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.0296       |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 570          |\n",
      "|    n_updates            | 16           |\n",
      "|    policy_gradient_loss | -6.14e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.14e+03     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 3216          |\n",
      "|    iterations           | 6             |\n",
      "|    time_elapsed         | 3             |\n",
      "|    total_timesteps      | 12288         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7538812e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.0307        |\n",
      "|    learning_rate        | 3e-05         |\n",
      "|    loss                 | 507           |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.000192     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.01e+03      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 3259         |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.196927e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.0388       |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 479          |\n",
      "|    n_updates            | 24           |\n",
      "|    policy_gradient_loss | -0.000101    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 960          |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 3319          |\n",
      "|    iterations           | 8             |\n",
      "|    time_elapsed         | 4             |\n",
      "|    total_timesteps      | 16384         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4614081e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.0142        |\n",
      "|    learning_rate        | 3e-05         |\n",
      "|    loss                 | 535           |\n",
      "|    n_updates            | 28            |\n",
      "|    policy_gradient_loss | -0.000214     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.07e+03      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 3361          |\n",
      "|    iterations           | 9             |\n",
      "|    time_elapsed         | 5             |\n",
      "|    total_timesteps      | 18432         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.2160467e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.0281        |\n",
      "|    learning_rate        | 3e-05         |\n",
      "|    loss                 | 517           |\n",
      "|    n_updates            | 32            |\n",
      "|    policy_gradient_loss | -0.000111     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.03e+03      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 3413          |\n",
      "|    iterations           | 10            |\n",
      "|    time_elapsed         | 6             |\n",
      "|    total_timesteps      | 20480         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.5387576e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.0486        |\n",
      "|    learning_rate        | 3e-05         |\n",
      "|    loss                 | 459           |\n",
      "|    n_updates            | 36            |\n",
      "|    policy_gradient_loss | -0.000162     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 919           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 3451          |\n",
      "|    iterations           | 11            |\n",
      "|    time_elapsed         | 6             |\n",
      "|    total_timesteps      | 22528         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.1449904e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.0388        |\n",
      "|    learning_rate        | 3e-05         |\n",
      "|    loss                 | 464           |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | 8.6e-06       |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 929           |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 3470        |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 4.74666e-06 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.0453      |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 480         |\n",
      "|    n_updates            | 44          |\n",
      "|    policy_gradient_loss | 7.63e-06    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 961         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 3475         |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 7            |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.392468e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.0208       |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 498          |\n",
      "|    n_updates            | 48           |\n",
      "|    policy_gradient_loss | 1.45e-05     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 996          |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 3420          |\n",
      "|    iterations           | 14            |\n",
      "|    time_elapsed         | 8             |\n",
      "|    total_timesteps      | 28672         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1533615e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.0269        |\n",
      "|    learning_rate        | 3e-05         |\n",
      "|    loss                 | 470           |\n",
      "|    n_updates            | 52            |\n",
      "|    policy_gradient_loss | -5.66e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 941           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 3385          |\n",
      "|    iterations           | 15            |\n",
      "|    time_elapsed         | 9             |\n",
      "|    total_timesteps      | 30720         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.6493147e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.0318        |\n",
      "|    learning_rate        | 3e-05         |\n",
      "|    loss                 | 533           |\n",
      "|    n_updates            | 56            |\n",
      "|    policy_gradient_loss | -1.17e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.07e+03      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 3368          |\n",
      "|    iterations           | 16            |\n",
      "|    time_elapsed         | 9             |\n",
      "|    total_timesteps      | 32768         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.9109792e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.0355        |\n",
      "|    learning_rate        | 3e-05         |\n",
      "|    loss                 | 527           |\n",
      "|    n_updates            | 60            |\n",
      "|    policy_gradient_loss | 0.000336      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.05e+03      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 3363          |\n",
      "|    iterations           | 17            |\n",
      "|    time_elapsed         | 10            |\n",
      "|    total_timesteps      | 34816         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5104335e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.0829        |\n",
      "|    learning_rate        | 3e-05         |\n",
      "|    loss                 | 480           |\n",
      "|    n_updates            | 64            |\n",
      "|    policy_gradient_loss | -0.000217     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 962           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 3368          |\n",
      "|    iterations           | 18            |\n",
      "|    time_elapsed         | 10            |\n",
      "|    total_timesteps      | 36864         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.6583217e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.0995        |\n",
      "|    learning_rate        | 3e-05         |\n",
      "|    loss                 | 462           |\n",
      "|    n_updates            | 68            |\n",
      "|    policy_gradient_loss | 6.66e-05      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 924           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 3368          |\n",
      "|    iterations           | 19            |\n",
      "|    time_elapsed         | 11            |\n",
      "|    total_timesteps      | 38912         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.0023639e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.102         |\n",
      "|    learning_rate        | 3e-05         |\n",
      "|    loss                 | 508           |\n",
      "|    n_updates            | 72            |\n",
      "|    policy_gradient_loss | 9.26e-05      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.02e+03      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 3382          |\n",
      "|    iterations           | 20            |\n",
      "|    time_elapsed         | 12            |\n",
      "|    total_timesteps      | 40960         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.4629898e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.0491        |\n",
      "|    learning_rate        | 3e-05         |\n",
      "|    loss                 | 481           |\n",
      "|    n_updates            | 76            |\n",
      "|    policy_gradient_loss | 0.000111      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 964           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 3382          |\n",
      "|    iterations           | 21            |\n",
      "|    time_elapsed         | 12            |\n",
      "|    total_timesteps      | 43008         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.5351113e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.00283       |\n",
      "|    learning_rate        | 3e-05         |\n",
      "|    loss                 | 575           |\n",
      "|    n_updates            | 80            |\n",
      "|    policy_gradient_loss | -0.000351     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.15e+03      |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 3388        |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 4.56181e-05 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.119       |\n",
      "|    learning_rate        | 3e-05       |\n",
      "|    loss                 | 556         |\n",
      "|    n_updates            | 84          |\n",
      "|    policy_gradient_loss | -0.000306   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 1.11e+03    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 3395         |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.787176e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.0787       |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 578          |\n",
      "|    n_updates            | 88           |\n",
      "|    policy_gradient_loss | 9.67e-05     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.16e+03     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 3408          |\n",
      "|    iterations           | 24            |\n",
      "|    time_elapsed         | 14            |\n",
      "|    total_timesteps      | 49152         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.1841122e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.0669        |\n",
      "|    learning_rate        | 3e-05         |\n",
      "|    loss                 | 483           |\n",
      "|    n_updates            | 92            |\n",
      "|    policy_gradient_loss | -0.000142     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 966           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 3425         |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.524761e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.0945       |\n",
      "|    learning_rate        | 3e-05        |\n",
      "|    loss                 | 471          |\n",
      "|    n_updates            | 96           |\n",
      "|    policy_gradient_loss | -0.000302    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 943          |\n",
      "------------------------------------------\n",
      "Model saved as dynamic_arbitration_ppo_exp_override.zip\n",
      "Metrics saved to training_metrics directory\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "###############################################################################\n",
    "# Constants & Hyperparameters\n",
    "###############################################################################\n",
    "FULL_VIEW_SIZE = (1200, 800)\n",
    "MAX_SPEED = 3\n",
    "DOT_RADIUS = 30\n",
    "TARGET_RADIUS = 10\n",
    "GOAL_DETECTION_RADIUS = DOT_RADIUS + TARGET_RADIUS\n",
    "START_POS = [FULL_VIEW_SIZE[0] // 2, FULL_VIEW_SIZE[1] // 2]\n",
    "\n",
    "NOISE_MAGNITUDE = 0.5  \n",
    "NUM_GOALS = 3  \n",
    "\n",
    "###############################################################################\n",
    "# Custom Features Extractor\n",
    "###############################################################################\n",
    "class OptimizedCustomFeaturesExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 128):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "        n_input = int(np.prod(observation_space.shape))\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(n_input, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Linear(256, features_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(features_dim)\n",
    "        )\n",
    "        \n",
    "        # Orthogonal init\n",
    "        for m in self.network.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.orthogonal_(m.weight, gain=np.sqrt(2))\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(observations)\n",
    "\n",
    "###############################################################################\n",
    "# Metrics Callback\n",
    "###############################################################################\n",
    "class MetricsCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Tracks:\n",
    "      - Episode total rewards\n",
    "      - Average gamma per episode\n",
    "      - Actor/critic losses\n",
    "    Saves these metrics as plots + a text summary.\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.episode_rewards = []\n",
    "        self.episode_gammas = []\n",
    "        self.current_episode_gammas = []\n",
    "        self.total_reward = 0\n",
    "        \n",
    "        self.actor_losses = []\n",
    "        self.critic_losses = []\n",
    "        self.action_low = None\n",
    "        self.action_high = None\n",
    "\n",
    "    def _on_step(self):\n",
    "        # Record action bounds once\n",
    "        if self.action_low is None:\n",
    "            self.action_low = self.model.action_space.low[0]\n",
    "            self.action_high = self.model.action_space.high[0]\n",
    "\n",
    "        # Current gamma\n",
    "        actions = self.locals['actions']\n",
    "        if torch.is_tensor(actions):\n",
    "            gamma = actions[0].item()\n",
    "        else:\n",
    "            gamma = float(actions[0])\n",
    "        gamma = np.clip(gamma, self.action_low, self.action_high)\n",
    "        self.current_episode_gammas.append(gamma)\n",
    "\n",
    "        # Current reward\n",
    "        rewards = self.locals['rewards']\n",
    "        if torch.is_tensor(rewards):\n",
    "            reward = rewards[0].item()\n",
    "        else:\n",
    "            reward = float(rewards[0])\n",
    "        self.total_reward += reward\n",
    "\n",
    "        # Actor/Critic losses from SB3 logger (if available)\n",
    "        if (hasattr(self.model, 'logger') \n",
    "            and hasattr(self.model.logger, 'name_to_value')):\n",
    "            logger_dict = self.model.logger.name_to_value\n",
    "            if 'train/policy_gradient_loss' in logger_dict:\n",
    "                self.actor_losses.append(logger_dict['train/policy_gradient_loss'])\n",
    "            if 'train/value_loss' in logger_dict:\n",
    "                self.critic_losses.append(logger_dict['train/value_loss'])\n",
    "\n",
    "        # Episode end?\n",
    "        if self.locals['dones'][0]:\n",
    "            self.episode_rewards.append(self.total_reward)\n",
    "            if len(self.current_episode_gammas) > 0:\n",
    "                avg_g = np.mean(self.current_episode_gammas)\n",
    "            else:\n",
    "                avg_g = 0.0\n",
    "            self.episode_gammas.append(avg_g)\n",
    "            \n",
    "            # Reset\n",
    "            self.current_episode_gammas = []\n",
    "            self.total_reward = 0\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def save_metrics(self, save_dir=\"training_metrics\"):\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        # 1) Episode reward\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(self.episode_rewards, label=\"Episode Reward\")\n",
    "        plt.title('Average Reward per Episode')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Reward')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(save_dir, 'average_reward.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        # 2) Average gamma\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(self.episode_gammas, label=\"Average Gamma\")\n",
    "        plt.title('Average Gamma per Episode')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Gamma')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(save_dir, 'average_gamma.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        # 3) Actor & critic loss\n",
    "        if self.actor_losses and self.critic_losses:\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            plt.plot(self.actor_losses, label=\"Actor Loss\")\n",
    "            plt.plot(self.critic_losses, label=\"Critic Loss\")\n",
    "            plt.title('Actor & Critic Loss')\n",
    "            plt.xlabel('Training Update')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.grid(True)\n",
    "            plt.legend()\n",
    "            plt.savefig(os.path.join(save_dir, 'actor_critic_loss.png'))\n",
    "            plt.close()\n",
    "\n",
    "        # Summary\n",
    "        with open(os.path.join(save_dir, 'training_summary.txt'), 'w') as f:\n",
    "            f.write(f\"Total Episodes: {len(self.episode_rewards)}\\n\")\n",
    "            if len(self.episode_rewards) > 0:\n",
    "                f.write(f\"Overall Average Reward: {np.mean(self.episode_rewards):.3f}\\n\")\n",
    "                f.write(f\"Overall Average Gamma: {np.mean(self.episode_gammas):.3f}\\n\")\n",
    "                f.write(f\"Best Episode Reward: {max(self.episode_rewards):.3f}\\n\")\n",
    "                f.write(f\"Worst Episode Reward: {min(self.episode_rewards):.3f}\\n\")\n",
    "\n",
    "###############################################################################\n",
    "# Environment with Exponential Ramp & Override Penalty\n",
    "###############################################################################\n",
    "class DynamicArbitrationEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Environment that:\n",
    "      - Has multiple goals,\n",
    "      - Chooses gamma in [0.15..0.7],\n",
    "      - Gains more reward for picking higher gamma only when close to the goal,\n",
    "      - Penalizes big changes in gamma, misalignment with \"human\" input, etc.\n",
    "    \"\"\"\n",
    "    def __init__(self, render_mode=None):\n",
    "        super().__init__()\n",
    "        self.render_mode = render_mode\n",
    "        self.max_dist = np.sqrt(FULL_VIEW_SIZE[0] ** 2 + FULL_VIEW_SIZE[1] ** 2)\n",
    "        \n",
    "        # Multi-goal setup\n",
    "        self.NUM_GOALS = NUM_GOALS\n",
    "        self.targets = []\n",
    "        self.current_target_idx = 0\n",
    "\n",
    "        # For continuity penalty on gamma\n",
    "        self.prev_gamma = None\n",
    "\n",
    "        # Observations: [dot_x, dot_y, human_x, human_y,\n",
    "        #                target_x, target_y, perfect_x, perfect_y,\n",
    "        #                normalized_dist]\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([0, 0, -1, -1, 0, 0, -1, -1, 0], dtype=np.float32),\n",
    "            high=np.array([\n",
    "                FULL_VIEW_SIZE[0], FULL_VIEW_SIZE[1],\n",
    "                1, 1,\n",
    "                FULL_VIEW_SIZE[0], FULL_VIEW_SIZE[1],\n",
    "                1, 1,\n",
    "                1\n",
    "            ], dtype=np.float32),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # Action: gamma in [0.15..0.7]\n",
    "        # ---------------------------------------------------------------------\n",
    "        self.action_space = spaces.Box(\n",
    "            low=np.array([0.15], dtype=np.float32),\n",
    "            high=np.array([0.7], dtype=np.float32),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.dot_pos = None\n",
    "        self.step_count = 0\n",
    "        self.max_steps = 300\n",
    "\n",
    "    def _generate_target(self):\n",
    "        \"\"\"Random target within screen boundaries.\"\"\"\n",
    "        x = np.random.uniform(100, FULL_VIEW_SIZE[0] - 100)\n",
    "        y = np.random.uniform(100, FULL_VIEW_SIZE[1] - 100)\n",
    "        return np.array([x, y], dtype=np.float32)\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\"\n",
    "        Synthetic 'human input' = perfect_dir + Gaussian noise, normalized.\n",
    "        Returns 9D observation.\n",
    "        \"\"\"\n",
    "        to_target = self.target_pos - self.dot_pos\n",
    "        dist = np.linalg.norm(to_target)\n",
    "        if dist > 1e-6:\n",
    "            perfect_dir = to_target / dist\n",
    "        else:\n",
    "            perfect_dir = np.array([0.0, 0.0], dtype=np.float32)\n",
    "        \n",
    "        normalized_dist = dist / self.max_dist\n",
    "\n",
    "        # \"human\" input is perfect_dir + noise\n",
    "        noise = np.random.normal(0, NOISE_MAGNITUDE, size=2)\n",
    "        human_input = perfect_dir + noise\n",
    "        norm_h = np.linalg.norm(human_input)\n",
    "        if norm_h > 1e-6:\n",
    "            human_input /= norm_h\n",
    "\n",
    "        # Combine\n",
    "        return np.concatenate([\n",
    "            self.dot_pos,        # (x, y)\n",
    "            human_input,         # (hx, hy)\n",
    "            self.target_pos,     # (tx, ty)\n",
    "            perfect_dir,         # (px, py)\n",
    "            [normalized_dist]\n",
    "        ]).astype(np.float32)\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.step_count = 0\n",
    "        self.prev_gamma = None\n",
    "\n",
    "        # Dot starts in the center\n",
    "        self.dot_pos = np.array(START_POS, dtype=np.float32)\n",
    "\n",
    "        # Generate multiple random targets\n",
    "        self.targets = [self._generate_target() for _ in range(self.NUM_GOALS)]\n",
    "        self.current_target_idx = 0\n",
    "        self.target_pos = self.targets[self.current_target_idx]\n",
    "\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        self.step_count += 1\n",
    "\n",
    "        # Clip gamma to [0.15..0.7]\n",
    "        gamma = float(np.clip(action[0], 0.15, 0.7))\n",
    "\n",
    "        # Occasionally switch targets mid-episode (2% chance)\n",
    "        if np.random.rand() < 0.02:\n",
    "            old_idx = self.current_target_idx\n",
    "            possible_indices = [i for i in range(self.NUM_GOALS) if i != old_idx]\n",
    "            if possible_indices:\n",
    "                self.current_target_idx = np.random.choice(possible_indices)\n",
    "            self.target_pos = self.targets[self.current_target_idx]\n",
    "\n",
    "        obs = self._get_obs()\n",
    "        human_dir = obs[2:4]       # (hx, hy)\n",
    "        perfect_dir = obs[6:8]     # (px, py)\n",
    "        dist_ratio = obs[8]\n",
    "\n",
    "        # Weighted combination\n",
    "        combined_dir = gamma * perfect_dir + (1 - gamma) * human_dir\n",
    "        norm_comb = np.linalg.norm(combined_dir)\n",
    "        if norm_comb > 1e-6:\n",
    "            combined_dir /= norm_comb\n",
    "\n",
    "        # Move the dot\n",
    "        move_vec = combined_dir * MAX_SPEED\n",
    "        new_pos = self.dot_pos + move_vec\n",
    "        self.dot_pos = np.clip(new_pos, [0, 0], [FULL_VIEW_SIZE[0], FULL_VIEW_SIZE[1]])\n",
    "\n",
    "        dist_to_target = np.linalg.norm(self.target_pos - self.dot_pos)\n",
    "\n",
    "        # ---------------------------------------------------------------------\n",
    "        # Reward Shaping\n",
    "        # ---------------------------------------------------------------------\n",
    "        \n",
    "        # (A) Exponential distance-based target gamma in [0.15..0.7]\n",
    "        alpha = 3.0\n",
    "        target_gamma = 0.15 + 0.55 * np.exp(-alpha * dist_ratio)\n",
    "        \n",
    "        # (B) penalty for deviation from target_gamma\n",
    "        gamma_penalty_coef = 20.0\n",
    "        gamma_penalty = -gamma_penalty_coef * (gamma - target_gamma)**2\n",
    "\n",
    "        # (C) smoothness penalty\n",
    "        if self.prev_gamma is None:\n",
    "            smoothness_penalty = 0.0\n",
    "        else:\n",
    "            smoothness_coef = 2.0\n",
    "            smoothness_penalty = -smoothness_coef * (gamma - self.prev_gamma)**2\n",
    "        self.prev_gamma = gamma\n",
    "\n",
    "        # (D) small alignment reward if final movement aligns with human_dir\n",
    "        alignment_reward = 0.05 * np.dot(combined_dir, human_dir)\n",
    "\n",
    "        # (E) override penalty if gamma is high but we’re ignoring the user\n",
    "        #     \"misalignment\" = 1 - dot( combined_dir, human_dir )\n",
    "        #     bigger gamma => bigger penalty if not aligned\n",
    "        dot_ch = np.dot(combined_dir, human_dir)\n",
    "        dot_ch = max(-1.0, min(1.0, dot_ch))  # clamp to [-1..1]\n",
    "        misalignment = 1.0 - dot_ch\n",
    "        override_penalty_coef = 5.0\n",
    "        override_penalty = -override_penalty_coef * gamma * misalignment\n",
    "\n",
    "        # (F) goal reward\n",
    "        goal_reward = 0.0\n",
    "        if dist_to_target < GOAL_DETECTION_RADIUS:\n",
    "            goal_reward = 10.0\n",
    "            # Optionally switch to a new target\n",
    "            old_idx = self.current_target_idx\n",
    "            possible_indices = [i for i in range(self.NUM_GOALS) if i != old_idx]\n",
    "            if possible_indices:\n",
    "                self.current_target_idx = np.random.choice(possible_indices)\n",
    "            self.target_pos = self.targets[self.current_target_idx]\n",
    "\n",
    "        # Combine final reward\n",
    "        reward = (\n",
    "            gamma_penalty\n",
    "            + smoothness_penalty\n",
    "            + alignment_reward\n",
    "            + override_penalty\n",
    "            + goal_reward\n",
    "        )\n",
    "\n",
    "        # Terminate or truncate\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        if self.step_count >= self.max_steps:\n",
    "            truncated = True\n",
    "\n",
    "        return self._get_obs(), reward, terminated, truncated, {}\n",
    "\n",
    "###############################################################################\n",
    "# VecEnv Helper\n",
    "###############################################################################\n",
    "def make_env(rank, seed=0):\n",
    "    def _init():\n",
    "        env = DynamicArbitrationEnv(render_mode=None)\n",
    "        env.reset(seed=seed + rank)\n",
    "        return env\n",
    "    set_random_seed(seed)\n",
    "    return _init\n",
    "\n",
    "###############################################################################\n",
    "# Training Routine\n",
    "###############################################################################\n",
    "def train():\n",
    "    model_save_dir = \"trained_models\"\n",
    "    metrics_dir = \"training_metrics\"\n",
    "    os.makedirs(model_save_dir, exist_ok=True)\n",
    "    os.makedirs(metrics_dir, exist_ok=True)\n",
    "    \n",
    "    # Number of parallel environments\n",
    "    n_envs = 8\n",
    "    env = DummyVecEnv([make_env(i) for i in range(n_envs)])\n",
    "    \n",
    "    policy_kwargs = dict(\n",
    "        features_extractor_class=OptimizedCustomFeaturesExtractor,\n",
    "        features_extractor_kwargs=dict(features_dim=128),\n",
    "        net_arch=dict(\n",
    "            pi=[256, 128],\n",
    "            vf=[256, 128]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Build the PPO model\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        learning_rate=3e-5,\n",
    "        n_steps=256,\n",
    "        batch_size=2048,\n",
    "        ent_coef=0.005,\n",
    "        n_epochs=4,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        clip_range=0.2,\n",
    "        normalize_advantage=True,\n",
    "        policy_kwargs=policy_kwargs,\n",
    "        device=device,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    metrics_callback = MetricsCallback()\n",
    "    \n",
    "    try:\n",
    "        print(\"Starting training...\")\n",
    "        model.learn(total_timesteps=5_0_000, callback=metrics_callback)\n",
    "        # ^ you may want to increase or decrease total_timesteps \n",
    "        #   e.g. 120_000_000, depending on your compute resources\n",
    "        \n",
    "        # Save final model\n",
    "        model.save(os.path.join(model_save_dir, \"dynamic_arbitration_ppo_exp_override\"))\n",
    "        print(\"Model saved as dynamic_arbitration_ppo_exp_override.zip\")\n",
    "        \n",
    "        # Save metrics\n",
    "        metrics_callback.save_metrics(save_dir=metrics_dir)\n",
    "        print(\"Metrics saved to training_metrics directory\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTraining interrupted. Saving current state...\")\n",
    "        model.save(os.path.join(model_save_dir, \"dynamic_arbitration_ppo_exp_override_interrupted\"))\n",
    "        metrics_callback.save_metrics(save_dir=\"training_metrics_interrupted\")\n",
    "        print(\"Intermediate state saved.\")\n",
    "    \n",
    "    finally:\n",
    "        env.close()\n",
    "\n",
    "###############################################################################\n",
    "# Main\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    mp.set_start_method('spawn', force=True)\n",
    "    train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
