{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Starting PPO training (visualize=False) ...\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 300      |\n",
      "|    ep_rew_mean     | 30.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 701      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | -116          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 708           |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 2             |\n",
      "|    total_timesteps      | 2048          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00019927562 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.0224        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 265           |\n",
      "|    n_updates            | 4             |\n",
      "|    policy_gradient_loss | -0.000524     |\n",
      "|    std                  | 0.999         |\n",
      "|    value_loss           | 532           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | 38.7          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 700           |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 4             |\n",
      "|    total_timesteps      | 3072          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00011448568 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | -0.00804      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 293           |\n",
      "|    n_updates            | 8             |\n",
      "|    policy_gradient_loss | -8.77e-05     |\n",
      "|    std                  | 0.998         |\n",
      "|    value_loss           | 586           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 300          |\n",
      "|    ep_rew_mean          | 81.3         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 691          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.316323e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.00141      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 256          |\n",
      "|    n_updates            | 12           |\n",
      "|    policy_gradient_loss | -6.43e-05    |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 513          |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | 41.8          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 688           |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 7             |\n",
      "|    total_timesteps      | 5120          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00014550792 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.0213        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 286           |\n",
      "|    n_updates            | 16            |\n",
      "|    policy_gradient_loss | -0.000785     |\n",
      "|    std                  | 0.997         |\n",
      "|    value_loss           | 575           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 300          |\n",
      "|    ep_rew_mean          | -30.5        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 596          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.582039e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.00619      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 307          |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | 9.44e-05     |\n",
      "|    std                  | 0.997        |\n",
      "|    value_loss           | 614          |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | -84.6         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 496           |\n",
      "|    iterations           | 7             |\n",
      "|    time_elapsed         | 14            |\n",
      "|    total_timesteps      | 7168          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.5382704e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.0194        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 289           |\n",
      "|    n_updates            | 24            |\n",
      "|    policy_gradient_loss | -0.000985     |\n",
      "|    std                  | 0.997         |\n",
      "|    value_loss           | 581           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | -84.5         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 442           |\n",
      "|    iterations           | 8             |\n",
      "|    time_elapsed         | 18            |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00033908384 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | -0.034        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 271           |\n",
      "|    n_updates            | 28            |\n",
      "|    policy_gradient_loss | -0.00236      |\n",
      "|    std                  | 0.997         |\n",
      "|    value_loss           | 544           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 300          |\n",
      "|    ep_rew_mean          | -48.7        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 410          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 22           |\n",
      "|    total_timesteps      | 9216         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003385101 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.00684      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 249          |\n",
      "|    n_updates            | 32           |\n",
      "|    policy_gradient_loss | -0.000674    |\n",
      "|    std                  | 0.996        |\n",
      "|    value_loss           | 498          |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | -30.5         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 386           |\n",
      "|    iterations           | 10            |\n",
      "|    time_elapsed         | 26            |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00014370354 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 0.0111        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 256           |\n",
      "|    n_updates            | 36            |\n",
      "|    policy_gradient_loss | 0.000439      |\n",
      "|    std                  | 0.995         |\n",
      "|    value_loss           | 512           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | -29.9         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 371           |\n",
      "|    iterations           | 11            |\n",
      "|    time_elapsed         | 30            |\n",
      "|    total_timesteps      | 11264         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.4720153e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | -0.00153      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 252           |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | 5.3e-05       |\n",
      "|    std                  | 0.995         |\n",
      "|    value_loss           | 505           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | -42.4         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 357           |\n",
      "|    iterations           | 12            |\n",
      "|    time_elapsed         | 34            |\n",
      "|    total_timesteps      | 12288         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.0420877e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 0.00982       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 244           |\n",
      "|    n_updates            | 44            |\n",
      "|    policy_gradient_loss | -9.6e-05      |\n",
      "|    std                  | 0.994         |\n",
      "|    value_loss           | 488           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 300          |\n",
      "|    ep_rew_mean          | -14.3        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 347          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 38           |\n",
      "|    total_timesteps      | 13312        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.628848e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.0277       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 251          |\n",
      "|    n_updates            | 48           |\n",
      "|    policy_gradient_loss | -3.17e-05    |\n",
      "|    std                  | 0.994        |\n",
      "|    value_loss           | 502          |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | -25.4         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 339           |\n",
      "|    iterations           | 14            |\n",
      "|    time_elapsed         | 42            |\n",
      "|    total_timesteps      | 14336         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2418779e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 0.0262        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 250           |\n",
      "|    n_updates            | 52            |\n",
      "|    policy_gradient_loss | -0.000166     |\n",
      "|    std                  | 0.994         |\n",
      "|    value_loss           | 503           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | -43.6         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 333           |\n",
      "|    iterations           | 15            |\n",
      "|    time_elapsed         | 46            |\n",
      "|    total_timesteps      | 15360         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.0641323e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 0.0373        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 237           |\n",
      "|    n_updates            | 56            |\n",
      "|    policy_gradient_loss | 1.4e-05       |\n",
      "|    std                  | 0.994         |\n",
      "|    value_loss           | 475           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | -39.7         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 329           |\n",
      "|    iterations           | 16            |\n",
      "|    time_elapsed         | 49            |\n",
      "|    total_timesteps      | 16384         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.0635664e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 0.0324        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 245           |\n",
      "|    n_updates            | 60            |\n",
      "|    policy_gradient_loss | -0.000202     |\n",
      "|    std                  | 0.994         |\n",
      "|    value_loss           | 491           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 300          |\n",
      "|    ep_rew_mean          | -29.8        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 326          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 53           |\n",
      "|    total_timesteps      | 17408        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.956636e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.0167       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 266          |\n",
      "|    n_updates            | 64           |\n",
      "|    policy_gradient_loss | -0.00021     |\n",
      "|    std                  | 0.995        |\n",
      "|    value_loss           | 533          |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | -28.2         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 324           |\n",
      "|    iterations           | 18            |\n",
      "|    time_elapsed         | 56            |\n",
      "|    total_timesteps      | 18432         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5977945e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 0.0156        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 257           |\n",
      "|    n_updates            | 68            |\n",
      "|    policy_gradient_loss | 8.74e-05      |\n",
      "|    std                  | 0.996         |\n",
      "|    value_loss           | 515           |\n",
      "-------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 300        |\n",
      "|    ep_rew_mean          | -24.4      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 322        |\n",
      "|    iterations           | 19         |\n",
      "|    time_elapsed         | 60         |\n",
      "|    total_timesteps      | 19456      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 4.4499e-05 |\n",
      "|    clip_fraction        | 0          |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.41      |\n",
      "|    explained_variance   | 0.018      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 238        |\n",
      "|    n_updates            | 72         |\n",
      "|    policy_gradient_loss | -0.000333  |\n",
      "|    std                  | 0.996      |\n",
      "|    value_loss           | 477        |\n",
      "----------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | -36.6         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 321           |\n",
      "|    iterations           | 20            |\n",
      "|    time_elapsed         | 63            |\n",
      "|    total_timesteps      | 20480         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.2555773e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.00563       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 237           |\n",
      "|    n_updates            | 76            |\n",
      "|    policy_gradient_loss | -3.08e-05     |\n",
      "|    std                  | 0.996         |\n",
      "|    value_loss           | 475           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | -42.8         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 318           |\n",
      "|    iterations           | 21            |\n",
      "|    time_elapsed         | 67            |\n",
      "|    total_timesteps      | 21504         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4824211e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.0224        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 227           |\n",
      "|    n_updates            | 80            |\n",
      "|    policy_gradient_loss | -6.17e-05     |\n",
      "|    std                  | 0.996         |\n",
      "|    value_loss           | 455           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 300          |\n",
      "|    ep_rew_mean          | -45.9        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 317          |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 71           |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.401795e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.0687       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 244          |\n",
      "|    n_updates            | 84           |\n",
      "|    policy_gradient_loss | -3.38e-05    |\n",
      "|    std                  | 0.996        |\n",
      "|    value_loss           | 489          |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | -54.2         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 314           |\n",
      "|    iterations           | 23            |\n",
      "|    time_elapsed         | 74            |\n",
      "|    total_timesteps      | 23552         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.3617405e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 0.0484        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 219           |\n",
      "|    n_updates            | 88            |\n",
      "|    policy_gradient_loss | -0.000217     |\n",
      "|    std                  | 0.996         |\n",
      "|    value_loss           | 438           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | -45           |\n",
      "| time/                   |               |\n",
      "|    fps                  | 313           |\n",
      "|    iterations           | 24            |\n",
      "|    time_elapsed         | 78            |\n",
      "|    total_timesteps      | 24576         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00022399105 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 0.0461        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 241           |\n",
      "|    n_updates            | 92            |\n",
      "|    policy_gradient_loss | -0.00064      |\n",
      "|    std                  | 0.996         |\n",
      "|    value_loss           | 483           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 300          |\n",
      "|    ep_rew_mean          | -38.5        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 309          |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 82           |\n",
      "|    total_timesteps      | 25600        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.056274e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.0298       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 224          |\n",
      "|    n_updates            | 96           |\n",
      "|    policy_gradient_loss | -5.54e-05    |\n",
      "|    std                  | 0.997        |\n",
      "|    value_loss           | 448          |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | -31.2         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 315           |\n",
      "|    iterations           | 26            |\n",
      "|    time_elapsed         | 84            |\n",
      "|    total_timesteps      | 26624         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.5156263e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.048         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 254           |\n",
      "|    n_updates            | 100           |\n",
      "|    policy_gradient_loss | -7.07e-05     |\n",
      "|    std                  | 0.998         |\n",
      "|    value_loss           | 509           |\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Only needed if visualize=True\n",
    "try:\n",
    "    import pygame\n",
    "except ImportError:\n",
    "    pygame = None\n",
    "\n",
    "###############################################################################\n",
    "# ENVIRONMENT CONSTANTS\n",
    "###############################################################################\n",
    "FULL_VIEW_SIZE = (1200, 800)\n",
    "SCALING_FACTOR_X = FULL_VIEW_SIZE[0] / 600.0\n",
    "SCALING_FACTOR_Y = FULL_VIEW_SIZE[1] / 600.0\n",
    "SCALING_FACTOR   = (SCALING_FACTOR_X + SCALING_FACTOR_Y) / 2\n",
    "\n",
    "DOT_RADIUS       = int(15 * SCALING_FACTOR)\n",
    "TARGET_RADIUS    = int(10 * SCALING_FACTOR)\n",
    "OBSTACLE_RADIUS  = int(10 * SCALING_FACTOR)\n",
    "COLLISION_BUFFER = int(5  * SCALING_FACTOR)\n",
    "MAX_SPEED        = 3 * SCALING_FACTOR\n",
    "NOISE_MAGNITUDE  = 0.5\n",
    "RENDER_FPS       = 30\n",
    "\n",
    "OBSTACLES = [\n",
    "    (200, 100),\n",
    "    (300, 700),\n",
    "    (1000, 150),\n",
    "    (1100, 600),\n",
    "    (200, 650),\n",
    "]\n",
    "\n",
    "GOALS = [\n",
    "    (600, 100),\n",
    "    (1100, 200),\n",
    "    (1100, 700),\n",
    "    (600, 700),\n",
    "    (100, 700),\n",
    "    (100, 200),\n",
    "    (900, 400),\n",
    "    (300, 400),\n",
    "]\n",
    "\n",
    "START_POS = np.array([FULL_VIEW_SIZE[0]//2, FULL_VIEW_SIZE[1]//2], dtype=np.float32)\n",
    "\n",
    "WHITE = (255,255,255)\n",
    "GRAY  = (128,128,128)\n",
    "YELLOW= (255,255,0)\n",
    "BLACK = (0,0,0)\n",
    "\n",
    "###############################################################################\n",
    "# UTILITY\n",
    "###############################################################################\n",
    "def distance(a, b):\n",
    "    return math.hypot(a[0]-b[0], a[1]-b[1])\n",
    "\n",
    "def check_line_collision(start, end, center, radius):\n",
    "    dx = end[0] - start[0]\n",
    "    dy = end[1] - start[1]\n",
    "    fx = center[0] - start[0]\n",
    "    fy = center[1] - start[1]\n",
    "    l2 = dx*dx + dy*dy\n",
    "    if l2 < 1e-9:\n",
    "        return distance(start, center) <= radius\n",
    "    t = max(0, min(1, (fx*dx + fy*dy) / l2))\n",
    "    px = start[0] + t*dx\n",
    "    py = start[1] + t*dy\n",
    "    return distance((px,py), center) <= radius\n",
    "\n",
    "def line_collision(pos, new_pos):\n",
    "    for obs in OBSTACLES:\n",
    "        if check_line_collision(pos, new_pos, obs, OBSTACLE_RADIUS + COLLISION_BUFFER):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def inside_obstacle(pos):\n",
    "    for obs in OBSTACLES:\n",
    "        if distance(pos, obs) <= (OBSTACLE_RADIUS + DOT_RADIUS):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def potential_field_dir(pos, goal):\n",
    "    gx = goal[0] - pos[0]\n",
    "    gy = goal[1] - pos[1]\n",
    "    dg = math.hypot(gx, gy)\n",
    "    if dg < 1e-6:\n",
    "        return np.zeros(2, dtype=np.float32)\n",
    "    att = np.array([gx/dg, gy/dg], dtype=np.float32)\n",
    "\n",
    "    repulse_x = 0.0\n",
    "    repulse_y = 0.0\n",
    "    repulsion_radius = 150.0 * SCALING_FACTOR\n",
    "    repulsion_gain   = 15000.0\n",
    "\n",
    "    for obs in OBSTACLES:\n",
    "        dx = pos[0] - obs[0]\n",
    "        dy = pos[1] - obs[1]\n",
    "        dobs = math.hypot(dx, dy)\n",
    "        if dobs<1e-6:\n",
    "            continue\n",
    "        if dobs < repulsion_radius:\n",
    "            pushx   = dx/dobs\n",
    "            pushy   = dy/dobs\n",
    "            strength= repulsion_gain/(dobs**2)\n",
    "            repulse_x += pushx*strength\n",
    "            repulse_y += pushy*strength\n",
    "\n",
    "    px = att[0] + repulse_x\n",
    "    py = att[1] + repulse_y\n",
    "    mg = math.hypot(px, py)\n",
    "    if mg < 1e-6:\n",
    "        return np.zeros(2, dtype=np.float32)\n",
    "    return np.array([px/mg, py/mg], dtype=np.float32)\n",
    "\n",
    "###############################################################################\n",
    "# CALLBACK\n",
    "###############################################################################\n",
    "class MetricsCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.episode_rewards = []\n",
    "        self.episode_gammas  = []\n",
    "        self.current_episode_gammas = []\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "        self.n_collisions = 0\n",
    "        self.n_episodes   = 0\n",
    "        self.action_low  = None\n",
    "        self.action_high = None\n",
    "\n",
    "    def _on_training_start(self):\n",
    "        self.n_collisions = 0\n",
    "        self.n_episodes   = 0\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.action_low is None:\n",
    "            self.action_low  = float(self.model.action_space.low)\n",
    "            self.action_high = float(self.model.action_space.high)\n",
    "\n",
    "        actions = self.locals['actions']\n",
    "        rewards = self.locals['rewards']\n",
    "        done    = self.locals['dones'][0]\n",
    "        infos   = self.locals['infos']\n",
    "\n",
    "        raw_action = float(actions[0])\n",
    "        raw_action = max(self.action_low, min(self.action_high, raw_action))\n",
    "        gamma = 0.5*(raw_action + 1.0)\n",
    "\n",
    "        r = float(rewards[0])\n",
    "        self.total_reward += r\n",
    "\n",
    "        if done:\n",
    "            self.episode_rewards.append(self.total_reward)\n",
    "            avg_g = np.mean(self.current_episode_gammas) if len(self.current_episode_gammas)>0 else 0.0\n",
    "            self.episode_gammas.append(avg_g)\n",
    "            self.total_reward = 0.0\n",
    "            self.current_episode_gammas.clear()\n",
    "            self.n_episodes += 1\n",
    "\n",
    "            if infos[0].get(\"terminal_reason\")== \"collision\":\n",
    "                self.n_collisions += 1\n",
    "        else:\n",
    "            self.current_episode_gammas.append(gamma)\n",
    "\n",
    "        return True\n",
    "\n",
    "    def save_metrics(self, save_dir=\"training_metrics\"):\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        plt.figure(figsize=(8,5))\n",
    "        plt.plot(self.episode_rewards, label=\"Episode Reward\")\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Reward\")\n",
    "        plt.title(\"Episode Reward Over Time\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(save_dir, \"episode_reward.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure(figsize=(8,5))\n",
    "        plt.plot(self.episode_gammas, label=\"Average Gamma\")\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Gamma\")\n",
    "        plt.title(\"Average Gamma per Episode\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(save_dir, \"average_gamma.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        with open(os.path.join(save_dir, \"training_summary.txt\"), 'w') as f:\n",
    "            f.write(f\"Total Episodes: {len(self.episode_rewards)}\\n\")\n",
    "            if len(self.episode_rewards)>0:\n",
    "                f.write(f\"Mean Reward: {np.mean(self.episode_rewards):.3f}\\n\")\n",
    "                f.write(f\"Mean Gamma: {np.mean(self.episode_gammas):.3f}\\n\")\n",
    "            f.write(f\"Collisions Count: {self.n_collisions}\\n\")\n",
    "\n",
    "###############################################################################\n",
    "# ENV: TINY THRESHOLD => +alpha*g if within 10 units, else -beta*g\n",
    "###############################################################################\n",
    "class DemoArbitrationEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    The dot must be extremely close to the goal (dist<10) to get +alpha*g.\n",
    "    Otherwise it sees -beta*g => big punish for high gamma away from the goal.\n",
    "    Collisions => -2 end the episode.\n",
    "\n",
    "    This ensures the agent basically never picks gamma=1 unless physically\n",
    "    hugging the goal center.\n",
    "    \"\"\"\n",
    "    metadata = {\"render_modes\":[\"human\"], \"render_fps\":RENDER_FPS}\n",
    "\n",
    "    def __init__(self, visualize=False):\n",
    "        super().__init__()\n",
    "        self.visualize= visualize\n",
    "\n",
    "        low  = np.array([0,0, -1,-1, 0,0, -1,-1, 0], dtype=np.float32)\n",
    "        high = np.array([\n",
    "            FULL_VIEW_SIZE[0], FULL_VIEW_SIZE[1],\n",
    "            1,1,\n",
    "            FULL_VIEW_SIZE[0], FULL_VIEW_SIZE[1],\n",
    "            1,1,\n",
    "            1\n",
    "        ], dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=low, high=high, shape=(9,), dtype=np.float32)\n",
    "\n",
    "        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(), dtype=np.float32)\n",
    "\n",
    "        self.dot_pos = None\n",
    "        self.goal_pos= None\n",
    "        self.step_count=0\n",
    "        self.max_steps=300\n",
    "        self.episode_reward=0.0\n",
    "\n",
    "        self.max_dist = math.hypot(FULL_VIEW_SIZE[0], FULL_VIEW_SIZE[1])\n",
    "\n",
    "        # extremely small threshold => basically on top of goal\n",
    "        self.close_threshold = 10.0\n",
    "        self.alpha = 3.0\n",
    "        self.beta  = 3.0\n",
    "\n",
    "        if self.visualize and pygame is not None:\n",
    "            pygame.init()\n",
    "            self.window = pygame.display.set_mode(FULL_VIEW_SIZE)\n",
    "            pygame.display.set_caption(\"Extremely Close => High Gamma\")\n",
    "            self.clock = pygame.time.Clock()\n",
    "        else:\n",
    "            self.window = None\n",
    "            self.clock  = None\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.step_count=0\n",
    "        self.episode_reward=0.0\n",
    "\n",
    "        self.dot_pos = START_POS.copy()\n",
    "        idx = np.random.randint(len(GOALS))\n",
    "        self.goal_pos= np.array(GOALS[idx], dtype=np.float32)\n",
    "\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        raw_a = float(action)\n",
    "        raw_a = np.clip(raw_a, -1.0, 1.0)\n",
    "        gamma = 0.5*(raw_a+1.0)\n",
    "\n",
    "        self.step_count +=1\n",
    "\n",
    "        # perfect dir\n",
    "        w_dir = potential_field_dir(self.dot_pos, self.goal_pos)\n",
    "        # \"human/noise\" dir\n",
    "        noise = np.random.normal(0,NOISE_MAGNITUDE,size=2)\n",
    "        h_dir = w_dir + noise\n",
    "        hm= np.hypot(h_dir[0], h_dir[1])\n",
    "        if hm>1e-6:\n",
    "            h_dir/= hm\n",
    "\n",
    "        c_dir = gamma*w_dir + (1-gamma)*h_dir\n",
    "        cm= np.hypot(c_dir[0], c_dir[1])\n",
    "        if cm>1e-6:\n",
    "            c_dir/=cm\n",
    "\n",
    "        move_vec = c_dir*MAX_SPEED\n",
    "        new_pos  = self.dot_pos + move_vec\n",
    "        if not line_collision(self.dot_pos,new_pos):\n",
    "            new_pos[0] = np.clip(new_pos[0],0,FULL_VIEW_SIZE[0])\n",
    "            new_pos[1] = np.clip(new_pos[1],0,FULL_VIEW_SIZE[1])\n",
    "            self.dot_pos= new_pos\n",
    "\n",
    "        collided= inside_obstacle(self.dot_pos)\n",
    "        info={}\n",
    "        if collided:\n",
    "            original_reward= -2.0\n",
    "            done= True\n",
    "            info[\"terminal_reason\"]=\"collision\"\n",
    "        else:\n",
    "            original_reward=0.0\n",
    "            done=False\n",
    "            info[\"terminal_reason\"]=None\n",
    "\n",
    "        truncated= (self.step_count>=self.max_steps)\n",
    "        if truncated and not done:\n",
    "            info[\"terminal_reason\"] = \"timeout\"\n",
    "\n",
    "        # If dist<10 => +alpha*g, else => -beta*g\n",
    "        dist_g= distance(self.dot_pos,self.goal_pos)\n",
    "        if dist_g< self.close_threshold:\n",
    "            shaping_reward= self.alpha*gamma\n",
    "        else:\n",
    "            shaping_reward= -self.beta*gamma\n",
    "\n",
    "        reward= original_reward + shaping_reward\n",
    "        self.episode_reward+= reward\n",
    "\n",
    "        if self.visualize:\n",
    "            self._render()\n",
    "\n",
    "        obs= self._get_obs()\n",
    "        return obs, float(reward), done, truncated, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        to_g= self.goal_pos - self.dot_pos\n",
    "        d= math.hypot(to_g[0],to_g[1])\n",
    "        dist_ratio= d/self.max_dist if self.max_dist>1e-6 else 0.0\n",
    "\n",
    "        w_dir= potential_field_dir(self.dot_pos, self.goal_pos)\n",
    "        noise= np.random.normal(0,NOISE_MAGNITUDE,size=2)\n",
    "        h_dir= w_dir+noise\n",
    "        hm= np.hypot(h_dir[0],h_dir[1])\n",
    "        if hm>1e-6:\n",
    "            h_dir/=hm\n",
    "\n",
    "        obs= np.concatenate([\n",
    "            self.dot_pos,\n",
    "            h_dir,\n",
    "            self.goal_pos,\n",
    "            w_dir,\n",
    "            [dist_ratio]\n",
    "        ]).astype(np.float32)\n",
    "        return obs\n",
    "\n",
    "    def _render(self):\n",
    "        if not self.window or not pygame:\n",
    "            return\n",
    "\n",
    "        self.window.fill(WHITE)\n",
    "        # obstacles\n",
    "        for obs in OBSTACLES:\n",
    "            pygame.draw.circle(self.window, GRAY, (int(obs[0]),int(obs[1])),\n",
    "                               OBSTACLE_RADIUS)\n",
    "        # all goals\n",
    "        for gpos in GOALS:\n",
    "            pygame.draw.circle(self.window, YELLOW, (int(gpos[0]),int(gpos[1])),\n",
    "                               TARGET_RADIUS)\n",
    "        # highlight current\n",
    "        pygame.draw.circle(self.window, BLACK,(int(self.goal_pos[0]),\n",
    "                                               int(self.goal_pos[1])),\n",
    "                          TARGET_RADIUS+2,width=2)\n",
    "\n",
    "        # dot\n",
    "        pygame.draw.circle(self.window, BLACK,(int(self.dot_pos[0]),\n",
    "                                               int(self.dot_pos[1])),\n",
    "                           DOT_RADIUS,width=2)\n",
    "\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(RENDER_FPS)\n",
    "\n",
    "    def close(self):\n",
    "        if self.visualize and pygame is not None:\n",
    "            pygame.quit()\n",
    "        super().close()\n",
    "\n",
    "###############################################################################\n",
    "# TRAIN\n",
    "###############################################################################\n",
    "def train(visualize=False, total_timesteps=300_000):\n",
    "    from stable_baselines3.common.callbacks import CallbackList\n",
    "    env = DemoArbitrationEnv(visualize=visualize)\n",
    "    metrics_callback = MetricsCallback()\n",
    "    callback = CallbackList([metrics_callback])\n",
    "\n",
    "    model = PPO(\n",
    "        policy=\"MlpPolicy\",\n",
    "        env=env,\n",
    "        learning_rate=3e-4,\n",
    "        n_steps=1024,\n",
    "        batch_size=1024,\n",
    "        n_epochs=4,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        clip_range=0.2,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        print(f\"Starting PPO training (visualize={visualize}) ...\")\n",
    "        model.learn(total_timesteps=total_timesteps, callback=callback)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training interrupted; saving partial model...\")\n",
    "\n",
    "    os.makedirs(\"trained_models\", exist_ok=True)\n",
    "    model.save(\"extreme_close_gamma_ppo\")\n",
    "    print(\"Model saved to trained_models/extreme_close_gamma_ppo.zip\")\n",
    "\n",
    "    metrics_callback.save_metrics(\"training_metrics\")\n",
    "    print(\"Metrics saved in training_metrics/\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "###############################################################################\n",
    "# MAIN\n",
    "###############################################################################\n",
    "if __name__==\"__main__\":\n",
    "    import sys\n",
    "    vis = (len(sys.argv)>1 and sys.argv[1].lower()==\"visualize\")\n",
    "    train(visualize=vis, total_timesteps=300_000)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
