{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Starting PPO training (visualize=False) ...\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 300      |\n",
      "|    ep_rew_mean     | 6.67     |\n",
      "| time/              |          |\n",
      "|    fps             | 339      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | 11.7          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 301           |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 6             |\n",
      "|    total_timesteps      | 2048          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00024563202 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | -0.0942       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.21          |\n",
      "|    n_updates            | 4             |\n",
      "|    policy_gradient_loss | -0.000554     |\n",
      "|    std                  | 0.999         |\n",
      "|    value_loss           | 2.51          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | 13            |\n",
      "| time/                   |               |\n",
      "|    fps                  | 298           |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 10            |\n",
      "|    total_timesteps      | 3072          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00026113307 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.0624        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.06          |\n",
      "|    n_updates            | 8             |\n",
      "|    policy_gradient_loss | -0.000461     |\n",
      "|    std                  | 0.998         |\n",
      "|    value_loss           | 4.24          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 300          |\n",
      "|    ep_rew_mean          | 11.5         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 300          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.192678e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.0364       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.63         |\n",
      "|    n_updates            | 12           |\n",
      "|    policy_gradient_loss | -0.00029     |\n",
      "|    std                  | 0.997        |\n",
      "|    value_loss           | 5.38         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | 8.82          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 301           |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 16            |\n",
      "|    total_timesteps      | 5120          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.7837756e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.11          |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.388         |\n",
      "|    n_updates            | 16            |\n",
      "|    policy_gradient_loss | 2.38e-05      |\n",
      "|    std                  | 0.997         |\n",
      "|    value_loss           | 0.793         |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | 9.5           |\n",
      "| time/                   |               |\n",
      "|    fps                  | 297           |\n",
      "|    iterations           | 6             |\n",
      "|    time_elapsed         | 20            |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00017348875 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.819         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.0195        |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.000226     |\n",
      "|    std                  | 0.998         |\n",
      "|    value_loss           | 0.0473        |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | 9.13          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 296           |\n",
      "|    iterations           | 7             |\n",
      "|    time_elapsed         | 24            |\n",
      "|    total_timesteps      | 7168          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00020937202 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.116         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.83          |\n",
      "|    n_updates            | 24            |\n",
      "|    policy_gradient_loss | -0.000103     |\n",
      "|    std                  | 0.998         |\n",
      "|    value_loss           | 3.69          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | 11.1          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 296           |\n",
      "|    iterations           | 8             |\n",
      "|    time_elapsed         | 27            |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00031486427 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.101         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.687         |\n",
      "|    n_updates            | 28            |\n",
      "|    policy_gradient_loss | -0.000526     |\n",
      "|    std                  | 0.999         |\n",
      "|    value_loss           | 1.39          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | 11            |\n",
      "| time/                   |               |\n",
      "|    fps                  | 295           |\n",
      "|    iterations           | 9             |\n",
      "|    time_elapsed         | 31            |\n",
      "|    total_timesteps      | 9216          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00035328337 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.164         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.88          |\n",
      "|    n_updates            | 32            |\n",
      "|    policy_gradient_loss | -5.47e-05     |\n",
      "|    std                  | 0.999         |\n",
      "|    value_loss           | 5.9           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | 11.5          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 294           |\n",
      "|    iterations           | 10            |\n",
      "|    time_elapsed         | 34            |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 9.4046234e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.221         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.691         |\n",
      "|    n_updates            | 36            |\n",
      "|    policy_gradient_loss | -0.000276     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.41          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | 11.6          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 289           |\n",
      "|    iterations           | 11            |\n",
      "|    time_elapsed         | 38            |\n",
      "|    total_timesteps      | 11264         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00012967008 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.0702        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.87          |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | 0.000295      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 5.82          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | 11            |\n",
      "| time/                   |               |\n",
      "|    fps                  | 291           |\n",
      "|    iterations           | 12            |\n",
      "|    time_elapsed         | 42            |\n",
      "|    total_timesteps      | 12288         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00014749583 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.147         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.68          |\n",
      "|    n_updates            | 44            |\n",
      "|    policy_gradient_loss | -0.000441     |\n",
      "|    std                  | 0.999         |\n",
      "|    value_loss           | 3.43          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | 10.9          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 305           |\n",
      "|    iterations           | 13            |\n",
      "|    time_elapsed         | 43            |\n",
      "|    total_timesteps      | 13312         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00032795238 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.186         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.711         |\n",
      "|    n_updates            | 48            |\n",
      "|    policy_gradient_loss | -0.000135     |\n",
      "|    std                  | 0.999         |\n",
      "|    value_loss           | 1.44          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | 11.5          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 318           |\n",
      "|    iterations           | 14            |\n",
      "|    time_elapsed         | 45            |\n",
      "|    total_timesteps      | 14336         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.0070467e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.137         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.79          |\n",
      "|    n_updates            | 52            |\n",
      "|    policy_gradient_loss | 9.54e-05      |\n",
      "|    std                  | 0.998         |\n",
      "|    value_loss           | 3.62          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 300          |\n",
      "|    ep_rew_mean          | 11.4         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 331          |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 46           |\n",
      "|    total_timesteps      | 15360        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.707727e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.11         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.8          |\n",
      "|    n_updates            | 56           |\n",
      "|    policy_gradient_loss | -8.36e-05    |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 7.74         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | 11.3          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 342           |\n",
      "|    iterations           | 16            |\n",
      "|    time_elapsed         | 47            |\n",
      "|    total_timesteps      | 16384         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00020505278 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.112         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.422         |\n",
      "|    n_updates            | 60            |\n",
      "|    policy_gradient_loss | -0.000497     |\n",
      "|    std                  | 0.998         |\n",
      "|    value_loss           | 0.849         |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 300          |\n",
      "|    ep_rew_mean          | 11.9         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 353          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 49           |\n",
      "|    total_timesteps      | 17408        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003733767 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.3          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.59         |\n",
      "|    n_updates            | 64           |\n",
      "|    policy_gradient_loss | -0.000327    |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 1.2          |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | 12.1          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 363           |\n",
      "|    iterations           | 18            |\n",
      "|    time_elapsed         | 50            |\n",
      "|    total_timesteps      | 18432         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00021930545 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.131         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.92          |\n",
      "|    n_updates            | 68            |\n",
      "|    policy_gradient_loss | -9.95e-06     |\n",
      "|    std                  | 0.998         |\n",
      "|    value_loss           | 13.9          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 300          |\n",
      "|    ep_rew_mean          | 12.8         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 372          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 52           |\n",
      "|    total_timesteps      | 19456        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.616614e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.191        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.9          |\n",
      "|    n_updates            | 72           |\n",
      "|    policy_gradient_loss | -0.000146    |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 3.85         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | 12.5          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 382           |\n",
      "|    iterations           | 20            |\n",
      "|    time_elapsed         | 53            |\n",
      "|    total_timesteps      | 20480         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.0988124e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.109         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.89          |\n",
      "|    n_updates            | 76            |\n",
      "|    policy_gradient_loss | -5.57e-05     |\n",
      "|    std                  | 0.998         |\n",
      "|    value_loss           | 7.88          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 300          |\n",
      "|    ep_rew_mean          | 12.3         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 391          |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 54           |\n",
      "|    total_timesteps      | 21504        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.072014e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.278        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.14         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -7.01e-05    |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 2.32         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | 11.7          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 399           |\n",
      "|    iterations           | 22            |\n",
      "|    time_elapsed         | 56            |\n",
      "|    total_timesteps      | 22528         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.6442285e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.428         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.328         |\n",
      "|    n_updates            | 84            |\n",
      "|    policy_gradient_loss | -8.12e-05     |\n",
      "|    std                  | 0.998         |\n",
      "|    value_loss           | 0.664         |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 300          |\n",
      "|    ep_rew_mean          | 11.9         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 407          |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 57           |\n",
      "|    total_timesteps      | 23552        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.039475e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.201        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.41         |\n",
      "|    n_updates            | 88           |\n",
      "|    policy_gradient_loss | -2.7e-05     |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 0.832        |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | 11.7          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 414           |\n",
      "|    iterations           | 24            |\n",
      "|    time_elapsed         | 59            |\n",
      "|    total_timesteps      | 24576         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.2783602e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.146         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.36          |\n",
      "|    n_updates            | 92            |\n",
      "|    policy_gradient_loss | -0.00011      |\n",
      "|    std                  | 0.997         |\n",
      "|    value_loss           | 6.73          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 300          |\n",
      "|    ep_rew_mean          | 11.5         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 420          |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 60           |\n",
      "|    total_timesteps      | 25600        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.692216e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.33         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.651        |\n",
      "|    n_updates            | 96           |\n",
      "|    policy_gradient_loss | -1.72e-05    |\n",
      "|    std                  | 0.997        |\n",
      "|    value_loss           | 1.32         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | 11.9          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 428           |\n",
      "|    iterations           | 26            |\n",
      "|    time_elapsed         | 62            |\n",
      "|    total_timesteps      | 26624         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.5055215e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.306         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.24          |\n",
      "|    n_updates            | 100           |\n",
      "|    policy_gradient_loss | -9.83e-05     |\n",
      "|    std                  | 0.996         |\n",
      "|    value_loss           | 2.5           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | 11.5          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 427           |\n",
      "|    iterations           | 27            |\n",
      "|    time_elapsed         | 64            |\n",
      "|    total_timesteps      | 27648         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.7987233e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.222         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.4           |\n",
      "|    n_updates            | 104           |\n",
      "|    policy_gradient_loss | -9.47e-05     |\n",
      "|    std                  | 0.996         |\n",
      "|    value_loss           | 6.83          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 300          |\n",
      "|    ep_rew_mean          | 11.8         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 415          |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 69           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.175208e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.567        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.245        |\n",
      "|    n_updates            | 108          |\n",
      "|    policy_gradient_loss | -5.64e-05    |\n",
      "|    std                  | 0.996        |\n",
      "|    value_loss           | 0.504        |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | 11.6          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 403           |\n",
      "|    iterations           | 29            |\n",
      "|    time_elapsed         | 73            |\n",
      "|    total_timesteps      | 29696         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.4125213e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.265         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.9           |\n",
      "|    n_updates            | 112           |\n",
      "|    policy_gradient_loss | -9.64e-05     |\n",
      "|    std                  | 0.997         |\n",
      "|    value_loss           | 3.85          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | 11.9          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 395           |\n",
      "|    iterations           | 30            |\n",
      "|    time_elapsed         | 77            |\n",
      "|    total_timesteps      | 30720         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2145552e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.151         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.735         |\n",
      "|    n_updates            | 116           |\n",
      "|    policy_gradient_loss | -2.86e-05     |\n",
      "|    std                  | 0.997         |\n",
      "|    value_loss           | 1.48          |\n",
      "-------------------------------------------\n",
      "Model saved to trained_models/demo_arbitration_ppo.zip\n",
      "Metrics saved in training_metrics/\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train a PPO agent in a single environment that matches your \"demo\":\n",
    " - Potential-field perfect movement\n",
    " - Same obstacles/goals\n",
    " - Action = scalar in [-1..1], mapped to gamma in [0..1]\n",
    " - Saves final training metrics + trained model\n",
    "\n",
    "Now includes a 'visualize' parameter so you can disable rendering for faster training.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Try to import pygame safely. If you plan to use visualize=False only,\n",
    "# you could skip installing pygame entirely.\n",
    "try:\n",
    "    import pygame\n",
    "except ImportError:\n",
    "    pygame = None\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Global config (mirroring your final \"demo\" environment)\n",
    "###############################################################################\n",
    "FULL_VIEW_SIZE = (1200, 800)\n",
    "\n",
    "SCALING_FACTOR_X  = FULL_VIEW_SIZE[0] / 600.0\n",
    "SCALING_FACTOR_Y  = FULL_VIEW_SIZE[1] / 600.0\n",
    "SCALING_FACTOR    = (SCALING_FACTOR_X + SCALING_FACTOR_Y) / 2\n",
    "\n",
    "WHITE  = (255, 255, 255)\n",
    "BLACK  = (0, 0, 0)\n",
    "GRAY   = (128, 128, 128)\n",
    "YELLOW = (255, 255, 0)\n",
    "GREEN  = (0, 200, 0)\n",
    "BLUE   = (0, 0, 255)\n",
    "RED    = (255, 0, 0)\n",
    "\n",
    "FONT_SIZE = int(24 * SCALING_FACTOR)\n",
    "FONT = None  # we'll init in __init__ if visualize=True\n",
    "\n",
    "DOT_RADIUS           = int(15 * SCALING_FACTOR)\n",
    "TARGET_RADIUS        = int(10 * SCALING_FACTOR)\n",
    "OBSTACLE_RADIUS      = int(10 * SCALING_FACTOR)\n",
    "COLLISION_BUFFER     = int(5 * SCALING_FACTOR)\n",
    "MAX_SPEED            = 3 * SCALING_FACTOR\n",
    "\n",
    "GOAL_DETECTION_RADIUS= DOT_RADIUS + TARGET_RADIUS\n",
    "START_POS = np.array([FULL_VIEW_SIZE[0] // 2, FULL_VIEW_SIZE[1] // 2], dtype=np.float32)\n",
    "\n",
    "NOISE_MAGNITUDE = 0.5\n",
    "RENDER_FPS = 30\n",
    "\n",
    "# Obstacles\n",
    "OBSTACLES = [\n",
    "    (200, 100),\n",
    "    (300, 700),\n",
    "    (1000, 150),\n",
    "    (1100, 600),\n",
    "    (200, 650),\n",
    "]\n",
    "\n",
    "# Goals\n",
    "GOALS = [\n",
    "    (600, 100),   # top center\n",
    "    (1100, 200),  # top-right\n",
    "    (1100, 700),  # bottom-right\n",
    "    (600, 700),   # bottom center\n",
    "    (100, 700),   # bottom-left\n",
    "    (100, 200),   # top-left\n",
    "    (900, 400),   # mid-right\n",
    "    (300, 400),   # mid-left\n",
    "]\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Metrics callback: same as before\n",
    "###############################################################################\n",
    "class MetricsCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.episode_rewards = []\n",
    "        self.episode_gammas = []\n",
    "        self.current_episode_gammas = []\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "        self.action_low = None\n",
    "        self.action_high = None\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.action_low is None:\n",
    "            # action_space is shape=(), so read as float\n",
    "            self.action_low = float(self.model.action_space.low)\n",
    "            self.action_high = float(self.model.action_space.high)\n",
    "\n",
    "        actions = self.locals['actions']\n",
    "        raw_action = float(actions[0])\n",
    "        # clamp\n",
    "        raw_action = max(self.action_low, min(self.action_high, raw_action))\n",
    "\n",
    "        # map raw in [-1,1] => gamma in [0,1]\n",
    "        gamma = 0.5*(raw_action+1.0)\n",
    "\n",
    "        # reward\n",
    "        rewards = self.locals['rewards']\n",
    "        r = float(rewards[0])\n",
    "        self.total_reward += r\n",
    "\n",
    "        done = self.locals['dones'][0]\n",
    "        if done:\n",
    "            self.episode_rewards.append(self.total_reward)\n",
    "            avg_g = np.mean(self.current_episode_gammas) if len(self.current_episode_gammas)>0 else 0.0\n",
    "            self.episode_gammas.append(avg_g)\n",
    "\n",
    "            self.total_reward = 0.0\n",
    "            self.current_episode_gammas = []\n",
    "        else:\n",
    "            self.current_episode_gammas.append(gamma)\n",
    "\n",
    "        return True\n",
    "\n",
    "    def save_metrics(self, save_dir=\"training_metrics\"):\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        # Plot episode rewards\n",
    "        plt.figure(figsize=(8,5))\n",
    "        plt.plot(self.episode_rewards, label=\"Episode Reward\")\n",
    "        plt.title(\"Episode Reward Over Time\")\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Reward\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(save_dir, \"episode_reward.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        # Plot average gamma\n",
    "        plt.figure(figsize=(8,5))\n",
    "        plt.plot(self.episode_gammas, label=\"Average Gamma\")\n",
    "        plt.title(\"Average Gamma per Episode\")\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Gamma\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(save_dir, \"average_gamma.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        # Summary text\n",
    "        with open(os.path.join(save_dir, \"training_summary.txt\"), 'w') as f:\n",
    "            f.write(f\"Total Episodes: {len(self.episode_rewards)}\\n\")\n",
    "            if len(self.episode_rewards)>0:\n",
    "                f.write(f\"Mean Reward: {np.mean(self.episode_rewards):.3f}\\n\")\n",
    "                f.write(f\"Mean Gamma: {np.mean(self.episode_gammas):.3f}\\n\")\n",
    "                f.write(f\"Best Episode: {max(self.episode_rewards):.3f}\\n\")\n",
    "                f.write(f\"Worst Episode: {min(self.episode_rewards):.3f}\\n\")\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Utility\n",
    "###############################################################################\n",
    "def distance(a, b):\n",
    "    return math.hypot(a[0]-b[0], a[1]-b[1])\n",
    "\n",
    "def check_line_collision(start, end, center, radius):\n",
    "    dx = end[0] - start[0]\n",
    "    dy = end[1] - start[1]\n",
    "    fx = center[0] - start[0]\n",
    "    fy = center[1] - start[1]\n",
    "    l2 = dx*dx + dy*dy\n",
    "    if l2<1e-9:\n",
    "        return distance(start, center)<=radius\n",
    "    t = max(0, min(1, (fx*dx + fy*dy)/l2))\n",
    "    px= start[0]+ t*dx\n",
    "    py= start[1]+ t*dy\n",
    "    return distance((px,py), center)<=radius\n",
    "\n",
    "def line_collision(pos, new_pos):\n",
    "    for obs in OBSTACLES:\n",
    "        if check_line_collision(pos, new_pos, obs, OBSTACLE_RADIUS+COLLISION_BUFFER):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def inside_obstacle(pos):\n",
    "    for obs in OBSTACLES:\n",
    "        if distance(pos, obs)<= (OBSTACLE_RADIUS+DOT_RADIUS):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def potential_field_dir(pos, goal):\n",
    "    gx = goal[0]-pos[0]\n",
    "    gy = goal[1]-pos[1]\n",
    "    dg = math.hypot(gx,gy)\n",
    "    if dg<1e-6:\n",
    "        return np.zeros(2,dtype=np.float32)\n",
    "    att = np.array([gx/dg, gy/dg], dtype=np.float32)\n",
    "\n",
    "    repulse_x=0.0\n",
    "    repulse_y=0.0\n",
    "    repulsion_radius=150.0*SCALING_FACTOR\n",
    "    repulsion_gain=30000.0\n",
    "    for obs in OBSTACLES:\n",
    "        dx = pos[0]-obs[0]\n",
    "        dy = pos[1]-obs[1]\n",
    "        dobs = math.hypot(dx,dy)\n",
    "        if dobs<1e-6:\n",
    "            continue\n",
    "        if dobs<repulsion_radius:\n",
    "            pushx = dx/dobs\n",
    "            pushy = dy/dobs\n",
    "            strength= repulsion_gain/(dobs**2)\n",
    "            repulse_x+= pushx*strength\n",
    "            repulse_y+= pushy*strength\n",
    "\n",
    "    px = att[0]+repulse_x\n",
    "    py = att[1]+repulse_y\n",
    "    mg= math.hypot(px,py)\n",
    "    if mg<1e-6:\n",
    "        return np.zeros(2,dtype=np.float32)\n",
    "    return np.array([px/mg, py/mg],dtype=np.float32)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# The Real-Time Env (with visualize param)\n",
    "###############################################################################\n",
    "class DemoArbitrationEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Single environment, shape=() action => scalar in [-1,1], mapped to gamma [0..1].\n",
    "    Observations: 9D [dot_x, dot_y, h_dir_x, h_dir_y, goal_x, goal_y, w_dir_x, w_dir_y, dist_norm].\n",
    "    If visualize=True, we open a Pygame window and do real-time rendering (slow).\n",
    "    If visualize=False, we skip all rendering for faster training.\n",
    "    \"\"\"\n",
    "    metadata = {\"render_modes\":[\"human\"], \"render_fps\":RENDER_FPS}\n",
    "\n",
    "    def __init__(self, visualize=True):\n",
    "        super().__init__()\n",
    "        self.visualize= visualize\n",
    "        # Obs space\n",
    "        low  = np.array([0,0, -1,-1, 0,0, -1,-1, 0], dtype=np.float32)\n",
    "        high = np.array([\n",
    "            FULL_VIEW_SIZE[0], FULL_VIEW_SIZE[1],\n",
    "            1,1,\n",
    "            FULL_VIEW_SIZE[0], FULL_VIEW_SIZE[1],\n",
    "            1,1,\n",
    "            1\n",
    "        ], dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=low, high=high, shape=(9,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # Action space: shape=(), in [-1..1]\n",
    "        self.action_space = spaces.Box(\n",
    "            low=-1.0, high=1.0, shape=(), dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.dot_pos = None\n",
    "        self.goal_pos= None\n",
    "        self.step_count=0\n",
    "        self.max_steps=300\n",
    "        self.episode_reward=0.0\n",
    "        self.max_dist= math.hypot(FULL_VIEW_SIZE[0], FULL_VIEW_SIZE[1])\n",
    "\n",
    "        self.recent_positions=[]\n",
    "        self.last_reset_time= time.time()\n",
    "\n",
    "        # only do pygame init if visualize==True\n",
    "        if self.visualize and pygame is not None:\n",
    "            pygame.init()\n",
    "            global FONT\n",
    "            FONT = pygame.font.Font(None, FONT_SIZE)\n",
    "            self.window = pygame.display.set_mode(FULL_VIEW_SIZE)\n",
    "            pygame.display.set_caption(\"PPO Real-time Env [action shape=()]\")\n",
    "            self.clock = pygame.time.Clock()\n",
    "        else:\n",
    "            self.window = None\n",
    "            self.clock = None\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.step_count=0\n",
    "        self.episode_reward=0.0\n",
    "        self.dot_pos= START_POS.copy()\n",
    "        idx= np.random.randint(len(GOALS))\n",
    "        self.goal_pos= np.array(GOALS[idx], dtype=np.float32)\n",
    "        self.recent_positions.clear()\n",
    "        self.last_reset_time= time.time()\n",
    "\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        raw_a = float(action)\n",
    "        raw_a= max(-1.0, min(1.0, raw_a))\n",
    "        gamma= 0.5*(raw_a+1.0) # => [0..1]\n",
    "\n",
    "        self.step_count+=1\n",
    "\n",
    "        w_dir= potential_field_dir(self.dot_pos, self.goal_pos)\n",
    "        noise= np.random.normal(0, NOISE_MAGNITUDE, size=2)\n",
    "        h_dir= w_dir+noise\n",
    "        hm= np.hypot(h_dir[0], h_dir[1])\n",
    "        if hm>1e-6:\n",
    "            h_dir/=hm\n",
    "\n",
    "        c_dir= gamma*w_dir+(1-gamma)*h_dir\n",
    "        cm= np.hypot(c_dir[0], c_dir[1])\n",
    "        if cm>1e-6:\n",
    "            c_dir/=cm\n",
    "\n",
    "        old_pos= self.dot_pos.copy()\n",
    "        move= c_dir*MAX_SPEED\n",
    "        new_pos= self.dot_pos+ move\n",
    "        if not line_collision(self.dot_pos, new_pos):\n",
    "            new_pos[0]= np.clip(new_pos[0],0,FULL_VIEW_SIZE[0])\n",
    "            new_pos[1]= np.clip(new_pos[1],0,FULL_VIEW_SIZE[1])\n",
    "            self.dot_pos= new_pos\n",
    "\n",
    "        collided= inside_obstacle(self.dot_pos)\n",
    "        if collided:\n",
    "            reward= -20.0\n",
    "            done= True\n",
    "        else:\n",
    "            dist_goal= distance(self.dot_pos, self.goal_pos)\n",
    "            if dist_goal< GOAL_DETECTION_RADIUS:\n",
    "                reward=10.0\n",
    "                idx= np.random.randint(len(GOALS))\n",
    "                self.goal_pos= np.array(GOALS[idx], dtype=np.float32)\n",
    "            else:\n",
    "                reward=0.0\n",
    "            done=False\n",
    "\n",
    "        truncated= (self.step_count>=self.max_steps)\n",
    "        self.episode_reward+= reward\n",
    "\n",
    "        # Only render if visualize=True\n",
    "        if self.visualize:\n",
    "            self._render(old_pos, w_dir, h_dir, c_dir, collided)\n",
    "\n",
    "        obs= self._get_obs()\n",
    "        info={}\n",
    "        return obs, float(reward), done, truncated, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        to_g= self.goal_pos- self.dot_pos\n",
    "        d= math.hypot(to_g[0], to_g[1])\n",
    "        dist_ratio= d/self.max_dist if self.max_dist>1e-6 else 0.0\n",
    "\n",
    "        # for obs only => w_dir + noise\n",
    "        w_dir= potential_field_dir(self.dot_pos, self.goal_pos)\n",
    "        noise= np.random.normal(0, NOISE_MAGNITUDE, size=2)\n",
    "        h_dir= w_dir+ noise\n",
    "        hm= np.hypot(h_dir[0], h_dir[1])\n",
    "        if hm>1e-6:\n",
    "            h_dir/=hm\n",
    "\n",
    "        obs= np.concatenate([\n",
    "            self.dot_pos,\n",
    "            h_dir,\n",
    "            self.goal_pos,\n",
    "            w_dir,\n",
    "            [dist_ratio]\n",
    "        ]).astype(np.float32)\n",
    "        return obs\n",
    "\n",
    "    def _render(self, old_pos, w_dir, h_dir, c_dir, collided):\n",
    "        now= time.time()\n",
    "        self.recent_positions.append((self.dot_pos[0], self.dot_pos[1], now))\n",
    "        while len(self.recent_positions)>1 and (now - self.recent_positions[0][2])>3.0:\n",
    "            self.recent_positions.pop(0)\n",
    "\n",
    "        self.window.fill(WHITE)\n",
    "\n",
    "        # obstacles\n",
    "        for obs in OBSTACLES:\n",
    "            pygame.draw.circle(self.window, GRAY, obs, OBSTACLE_RADIUS)\n",
    "\n",
    "        # goals\n",
    "        for i,gpos in enumerate(GOALS):\n",
    "            pygame.draw.circle(self.window, YELLOW, (int(gpos[0]), int(gpos[1])), TARGET_RADIUS)\n",
    "            textimg= FONT.render(str(i+1), True, BLACK)\n",
    "            self.window.blit(textimg, (gpos[0]-5, gpos[1]-12))\n",
    "\n",
    "        # highlight\n",
    "        pygame.draw.circle(self.window, BLACK, (int(self.goal_pos[0]),int(self.goal_pos[1])),\n",
    "                           TARGET_RADIUS+2, width=2)\n",
    "\n",
    "        # ghost path\n",
    "        if len(self.recent_positions)>1:\n",
    "            for k in range(len(self.recent_positions)-1):\n",
    "                x1,y1,t1= self.recent_positions[k]\n",
    "                x2,y2,t2= self.recent_positions[k+1]\n",
    "                pygame.draw.line(self.window, (200,200,200), (x1,y1), (x2,y2), 2)\n",
    "\n",
    "        # dot\n",
    "        pygame.draw.circle(self.window, BLACK, (int(self.dot_pos[0]),int(self.dot_pos[1])),\n",
    "                           DOT_RADIUS, width=2)\n",
    "\n",
    "        def draw_arrow(surf, color, start, vec):\n",
    "            dx,dy=vec\n",
    "            mg= math.hypot(dx,dy)\n",
    "            if mg<1e-6:\n",
    "                return\n",
    "            dx/=mg\n",
    "            dy/=mg\n",
    "            length= int(60*SCALING_FACTOR)\n",
    "            endx= start[0]+ dx*length\n",
    "            endy= start[1]+ dy*length\n",
    "            pygame.draw.line(surf, color, start, (endx,endy), width=2)\n",
    "            arrow_size= 7*SCALING_FACTOR\n",
    "            angle= math.atan2(dy,dx)\n",
    "            ax1= endx- arrow_size*math.cos(angle+math.pi/6)\n",
    "            ay1= endy- arrow_size*math.sin(angle+math.pi/6)\n",
    "            ax2= endx- arrow_size*math.cos(angle-math.pi/6)\n",
    "            ay2= endy- arrow_size*math.sin(angle-math.pi/6)\n",
    "            pygame.draw.line(surf, color, (endx,endy), (ax1,ay1), width=2)\n",
    "            pygame.draw.line(surf, color, (endx,endy), (ax2,ay2), width=2)\n",
    "\n",
    "        cx, cy= int(self.dot_pos[0]), int(self.dot_pos[1])\n",
    "        draw_arrow(self.window, GREEN, (cx,cy), w_dir)\n",
    "        draw_arrow(self.window, BLUE,  (cx,cy), h_dir)\n",
    "        draw_arrow(self.window, RED,   (cx,cy), c_dir)\n",
    "\n",
    "        steps_txt= FONT.render(f\"Steps: {self.step_count}/{self.max_steps}\", True, BLACK)\n",
    "        self.window.blit(steps_txt, (10,10))\n",
    "        rew_txt= FONT.render(f\"Episode Reward: {self.episode_reward:.1f}\", True, BLACK)\n",
    "        self.window.blit(rew_txt, (10,40))\n",
    "        if collided:\n",
    "            col_msg= FONT.render(\"Collision => -20.0 reward, done!\", True, RED)\n",
    "            self.window.blit(col_msg, (250,20))\n",
    "\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(RENDER_FPS)\n",
    "\n",
    "    def render(self):\n",
    "        # no-op, we do everything in _render if visualize==True\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        if self.visualize and pygame is not None:\n",
    "            pygame.quit()\n",
    "        super().close()\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Training\n",
    "###############################################################################\n",
    "def train(visualize=False):\n",
    "    \"\"\"\n",
    "    If visualize=True => shows a Pygame window in real time (slow).\n",
    "    If visualize=False => no rendering, faster training.\n",
    "    \"\"\"\n",
    "    from stable_baselines3.common.callbacks import CallbackList\n",
    "\n",
    "    env = DemoArbitrationEnv(visualize=visualize)\n",
    "\n",
    "    # Build PPO\n",
    "    model = PPO(\n",
    "        policy=\"MlpPolicy\",\n",
    "        env=env,\n",
    "        learning_rate=3e-4,\n",
    "        n_steps=1024,\n",
    "        batch_size=1024,\n",
    "        n_epochs=4,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        clip_range=0.2,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    metrics_callback = MetricsCallback()\n",
    "    callback = CallbackList([metrics_callback])\n",
    "\n",
    "    total_timesteps = 30_000\n",
    "\n",
    "    try:\n",
    "        print(f\"Starting PPO training (visualize={visualize}) ...\")\n",
    "        model.learn(total_timesteps=total_timesteps, callback=callback)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training interrupted; saving partial model...\")\n",
    "\n",
    "    # Save final model\n",
    "    os.makedirs(\"trained_models\", exist_ok=True)\n",
    "    model.save(\"trained_models/demo_arbitration_ppo\")\n",
    "    print(\"Model saved to trained_models/demo_arbitration_ppo.zip\")\n",
    "\n",
    "    # Save metrics\n",
    "    metrics_callback.save_metrics(\"training_metrics\")\n",
    "    print(\"Metrics saved in training_metrics/\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "###############################################################################\n",
    "# Main\n",
    "###############################################################################\n",
    "if __name__==\"__main__\":\n",
    "    # Example: pass visualize=False for fast/no-window\n",
    "    # or visualize=True to see the environment in real time\n",
    "    train(visualize=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
