{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Starting PPO training (visualize=False) ...\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 300      |\n",
      "|    ep_rew_mean     | 27.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 709      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | 30.7          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 730           |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 2             |\n",
      "|    total_timesteps      | 2048          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00046892982 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | -0.0308       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.18          |\n",
      "|    n_updates            | 4             |\n",
      "|    policy_gradient_loss | -0.000795     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 8.67          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 300          |\n",
      "|    ep_rew_mean          | 32.4         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 738          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 3072         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.620187e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -0.0546      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.53         |\n",
      "|    n_updates            | 8            |\n",
      "|    policy_gradient_loss | 0.000205     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 11.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 300         |\n",
      "|    ep_rew_mean          | 31.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 740         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 2.78168e-05 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | -0.0796     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.17        |\n",
      "|    n_updates            | 12          |\n",
      "|    policy_gradient_loss | -0.000143   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 14.8        |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | 30.3          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 741           |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 6             |\n",
      "|    total_timesteps      | 5120          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.1136657e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | -0.084        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.39          |\n",
      "|    n_updates            | 16            |\n",
      "|    policy_gradient_loss | -0.000171     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 7.04          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | 30.9          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 744           |\n",
      "|    iterations           | 6             |\n",
      "|    time_elapsed         | 8             |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.1455944e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | -0.0107       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.21          |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.000182     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 6.69          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 300          |\n",
      "|    ep_rew_mean          | 30.4         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 746          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 9            |\n",
      "|    total_timesteps      | 7168         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.777427e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -0.0111      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.18         |\n",
      "|    n_updates            | 24           |\n",
      "|    policy_gradient_loss | -0.000152    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 12.6         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | 29.6          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 747           |\n",
      "|    iterations           | 8             |\n",
      "|    time_elapsed         | 10            |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00012221182 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.00926       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.69          |\n",
      "|    n_updates            | 28            |\n",
      "|    policy_gradient_loss | -0.000284     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 7.63          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 300          |\n",
      "|    ep_rew_mean          | 30.4         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 749          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 9216         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.509176e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.059        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.53         |\n",
      "|    n_updates            | 32           |\n",
      "|    policy_gradient_loss | 8.14e-05     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 5.27         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | 31.6          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 744           |\n",
      "|    iterations           | 10            |\n",
      "|    time_elapsed         | 13            |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.9981736e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.0238        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.87          |\n",
      "|    n_updates            | 36            |\n",
      "|    policy_gradient_loss | -0.000139     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 14.1          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | 31.5          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 740           |\n",
      "|    iterations           | 11            |\n",
      "|    time_elapsed         | 15            |\n",
      "|    total_timesteps      | 11264         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5578873e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.0397        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.13          |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | 2.17e-05      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 12.6          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 300          |\n",
      "|    ep_rew_mean          | 32           |\n",
      "| time/                   |              |\n",
      "|    fps                  | 741          |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.545293e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.0209       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.01         |\n",
      "|    n_updates            | 44           |\n",
      "|    policy_gradient_loss | -0.000252    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 8.24         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | 32.5          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 741           |\n",
      "|    iterations           | 13            |\n",
      "|    time_elapsed         | 17            |\n",
      "|    total_timesteps      | 13312         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.0618895e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | -0.00852      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.89          |\n",
      "|    n_updates            | 48            |\n",
      "|    policy_gradient_loss | -0.000199     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 16.1          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 300          |\n",
      "|    ep_rew_mean          | 32           |\n",
      "| time/                   |              |\n",
      "|    fps                  | 742          |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 19           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.639073e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.0837       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.97         |\n",
      "|    n_updates            | 52           |\n",
      "|    policy_gradient_loss | -0.000166    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 6.19         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 300          |\n",
      "|    ep_rew_mean          | 32           |\n",
      "| time/                   |              |\n",
      "|    fps                  | 742          |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 20           |\n",
      "|    total_timesteps      | 15360        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001787031 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.025        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.85         |\n",
      "|    n_updates            | 56           |\n",
      "|    policy_gradient_loss | -0.000471    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.83         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | 31.6          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 740           |\n",
      "|    iterations           | 16            |\n",
      "|    time_elapsed         | 22            |\n",
      "|    total_timesteps      | 16384         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00019860413 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.00964       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.2           |\n",
      "|    n_updates            | 60            |\n",
      "|    policy_gradient_loss | -7.82e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 8.63          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | 31            |\n",
      "| time/                   |               |\n",
      "|    fps                  | 736           |\n",
      "|    iterations           | 17            |\n",
      "|    time_elapsed         | 23            |\n",
      "|    total_timesteps      | 17408         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00012664509 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.197         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.88          |\n",
      "|    n_updates            | 64            |\n",
      "|    policy_gradient_loss | -8.16e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 3.91          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 300          |\n",
      "|    ep_rew_mean          | 31.1         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 735          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 25           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005605351 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.164        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.6          |\n",
      "|    n_updates            | 68           |\n",
      "|    policy_gradient_loss | -0.00179     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.32         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | 31            |\n",
      "| time/                   |               |\n",
      "|    fps                  | 735           |\n",
      "|    iterations           | 19            |\n",
      "|    time_elapsed         | 26            |\n",
      "|    total_timesteps      | 19456         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00086280995 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.0961        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.82          |\n",
      "|    n_updates            | 72            |\n",
      "|    policy_gradient_loss | -0.000581     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 7.86          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 300          |\n",
      "|    ep_rew_mean          | 31.3         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 733          |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 27           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003300989 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.155        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.61         |\n",
      "|    n_updates            | 76           |\n",
      "|    policy_gradient_loss | 0.000244     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 5.4          |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | 31.3          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 730           |\n",
      "|    iterations           | 21            |\n",
      "|    time_elapsed         | 29            |\n",
      "|    total_timesteps      | 21504         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00019696698 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.0951        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.46          |\n",
      "|    n_updates            | 80            |\n",
      "|    policy_gradient_loss | -0.000476     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 7.11          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | 31.1          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 729           |\n",
      "|    iterations           | 22            |\n",
      "|    time_elapsed         | 30            |\n",
      "|    total_timesteps      | 22528         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.1654544e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.093         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.38          |\n",
      "|    n_updates            | 84            |\n",
      "|    policy_gradient_loss | 0.00011       |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 6.91          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 300          |\n",
      "|    ep_rew_mean          | 31           |\n",
      "| time/                   |              |\n",
      "|    fps                  | 728          |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 32           |\n",
      "|    total_timesteps      | 23552        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.785333e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.0804       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.97         |\n",
      "|    n_updates            | 88           |\n",
      "|    policy_gradient_loss | -8.4e-05     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 6.07         |\n",
      "------------------------------------------\n",
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 300            |\n",
      "|    ep_rew_mean          | 31             |\n",
      "| time/                   |                |\n",
      "|    fps                  | 728            |\n",
      "|    iterations           | 24             |\n",
      "|    time_elapsed         | 33             |\n",
      "|    total_timesteps      | 24576          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.000109659566 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -1.42          |\n",
      "|    explained_variance   | 0.111          |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | 2.92           |\n",
      "|    n_updates            | 92             |\n",
      "|    policy_gradient_loss | -0.000328      |\n",
      "|    std                  | 1              |\n",
      "|    value_loss           | 5.95           |\n",
      "--------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | 30.6          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 728           |\n",
      "|    iterations           | 25            |\n",
      "|    time_elapsed         | 35            |\n",
      "|    total_timesteps      | 25600         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00021845079 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.189         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.82          |\n",
      "|    n_updates            | 96            |\n",
      "|    policy_gradient_loss | -0.000336     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 5.76          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | 31            |\n",
      "| time/                   |               |\n",
      "|    fps                  | 727           |\n",
      "|    iterations           | 26            |\n",
      "|    time_elapsed         | 36            |\n",
      "|    total_timesteps      | 26624         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00014759839 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.113         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.88          |\n",
      "|    n_updates            | 100           |\n",
      "|    policy_gradient_loss | 3.32e-05      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 3.83          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | 30.8          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 726           |\n",
      "|    iterations           | 27            |\n",
      "|    time_elapsed         | 38            |\n",
      "|    total_timesteps      | 27648         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.0479143e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.0879        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.88          |\n",
      "|    n_updates            | 104           |\n",
      "|    policy_gradient_loss | -2.97e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 13.9          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | 30.9          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 725           |\n",
      "|    iterations           | 28            |\n",
      "|    time_elapsed         | 39            |\n",
      "|    total_timesteps      | 28672         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00013178907 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.14          |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.47          |\n",
      "|    n_updates            | 108           |\n",
      "|    policy_gradient_loss | -0.00044      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 5.02          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | 30.9          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 724           |\n",
      "|    iterations           | 29            |\n",
      "|    time_elapsed         | 41            |\n",
      "|    total_timesteps      | 29696         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00017927837 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.147         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.58          |\n",
      "|    n_updates            | 112           |\n",
      "|    policy_gradient_loss | 0.000174      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 7.28          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 300           |\n",
      "|    ep_rew_mean          | 31.1          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 724           |\n",
      "|    iterations           | 30            |\n",
      "|    time_elapsed         | 42            |\n",
      "|    total_timesteps      | 30720         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.1949061e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.0477        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.92          |\n",
      "|    n_updates            | 116           |\n",
      "|    policy_gradient_loss | -0.0001       |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 5.92          |\n",
      "-------------------------------------------\n",
      "Model saved to trained_models/demo_arbitration_ppo.zip\n",
      "Metrics + summary saved in training_metrics/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Try to import pygame only if available (for optional visualize=True)\n",
    "try:\n",
    "    import pygame\n",
    "except ImportError:\n",
    "    pygame = None\n",
    "\n",
    "###############################################################################\n",
    "# Unified Environment Constants (matching your demo code)\n",
    "###############################################################################\n",
    "FULL_VIEW_SIZE = (1200, 800)\n",
    "SCALING_FACTOR_X = FULL_VIEW_SIZE[0] / 600.0\n",
    "SCALING_FACTOR_Y = FULL_VIEW_SIZE[1] / 600.0\n",
    "SCALING_FACTOR   = (SCALING_FACTOR_X + SCALING_FACTOR_Y) / 2\n",
    "\n",
    "DOT_RADIUS        = int(15 * SCALING_FACTOR)\n",
    "TARGET_RADIUS     = int(10 * SCALING_FACTOR)\n",
    "OBSTACLE_RADIUS   = int(10 * SCALING_FACTOR)\n",
    "COLLISION_BUFFER  = int(5  * SCALING_FACTOR)\n",
    "MAX_SPEED         = 3 * SCALING_FACTOR\n",
    "NOISE_MAGNITUDE   = 0.5\n",
    "RENDER_FPS        = 30\n",
    "\n",
    "OBSTACLES = [\n",
    "    (200, 100),\n",
    "    (300, 700),\n",
    "    (1000, 150),\n",
    "    (1100, 600),\n",
    "    (200, 650),\n",
    "]\n",
    "\n",
    "GOALS = [\n",
    "    (600, 100),   # top center\n",
    "    (1100, 200),  # top-right\n",
    "    (1100, 700),  # bottom-right\n",
    "    (600, 700),   # bottom center\n",
    "    (100, 700),   # bottom-left\n",
    "    (100, 200),   # top-left\n",
    "    (900, 400),   # mid-right\n",
    "    (300, 400),   # mid-left\n",
    "]\n",
    "\n",
    "START_POS = np.array([FULL_VIEW_SIZE[0] // 2, FULL_VIEW_SIZE[1] // 2], dtype=np.float32)\n",
    "\n",
    "WHITE = (255,255,255)\n",
    "BLACK = (0,0,0)\n",
    "GRAY  = (128,128,128)\n",
    "YELLOW= (255,255,0)\n",
    "GREEN = (0,200,0)\n",
    "BLUE  = (0,0,255)\n",
    "RED   = (255,0,0)\n",
    "\n",
    "###############################################################################\n",
    "# Utility functions\n",
    "###############################################################################\n",
    "def distance(a, b):\n",
    "    return math.hypot(a[0] - b[0], a[1] - b[1])\n",
    "\n",
    "def check_line_collision(start, end, center, radius):\n",
    "    dx = end[0] - start[0]\n",
    "    dy = end[1] - start[1]\n",
    "    fx = center[0] - start[0]\n",
    "    fy = center[1] - start[1]\n",
    "    l2 = dx*dx + dy*dy\n",
    "    if l2 < 1e-9:\n",
    "        return distance(start, center) <= radius\n",
    "    t = max(0, min(1, (fx*dx + fy*dy) / l2))\n",
    "    px = start[0] + t*dx\n",
    "    py = start[1] + t*dy\n",
    "    return distance((px,py), center) <= radius\n",
    "\n",
    "def line_collision(pos, new_pos):\n",
    "    for obs in OBSTACLES:\n",
    "        if check_line_collision(pos, new_pos, obs, OBSTACLE_RADIUS+COLLISION_BUFFER):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def inside_obstacle(pos):\n",
    "    for obs in OBSTACLES:\n",
    "        if distance(pos, obs) <= (OBSTACLE_RADIUS + DOT_RADIUS):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def potential_field_dir(pos, goal):\n",
    "    # Basic \"goal attraction\" + obstacle \"repulsion\"\n",
    "    gx = goal[0] - pos[0]\n",
    "    gy = goal[1] - pos[1]\n",
    "    dg = math.hypot(gx, gy)\n",
    "    if dg < 1e-6:\n",
    "        return np.zeros(2, dtype=np.float32)\n",
    "    att = np.array([gx/dg, gy/dg], dtype=np.float32)\n",
    "\n",
    "    repulse_x = 0.0\n",
    "    repulse_y = 0.0\n",
    "    # For a less \"excessively wide margin,\" reduce repulsion_gain\n",
    "    repulsion_radius = 150.0 * SCALING_FACTOR\n",
    "    repulsion_gain   = 15000.0  # was 30000.0; lowered to reduce huge repulsion\n",
    "\n",
    "    for obs in OBSTACLES:\n",
    "        dx = pos[0] - obs[0]\n",
    "        dy = pos[1] - obs[1]\n",
    "        dobs = math.hypot(dx, dy)\n",
    "        if dobs < 1e-6:\n",
    "            continue\n",
    "        if dobs < repulsion_radius:\n",
    "            pushx   = dx / dobs\n",
    "            pushy   = dy / dobs\n",
    "            strength= repulsion_gain / (dobs**2)\n",
    "            repulse_x += pushx * strength\n",
    "            repulse_y += pushy * strength\n",
    "\n",
    "    px = att[0] + repulse_x\n",
    "    py = att[1] + repulse_y\n",
    "    mg = math.hypot(px, py)\n",
    "    if mg < 1e-6:\n",
    "        return np.zeros(2, dtype=np.float32)\n",
    "    return np.array([px/mg, py/mg], dtype=np.float32)\n",
    "\n",
    "###############################################################################\n",
    "# Extended Callback with extra metrics\n",
    "###############################################################################\n",
    "class MetricsCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.episode_rewards = []\n",
    "        self.episode_gammas  = []\n",
    "        self.current_episode_gammas = []\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "        # Additional counts\n",
    "        self.n_collisions = 0\n",
    "        self.n_goals      = 0\n",
    "        self.n_timeouts   = 0\n",
    "\n",
    "        self.action_low = None\n",
    "        self.action_high= None\n",
    "\n",
    "    def _on_training_start(self):\n",
    "        # Reset counters at start of training\n",
    "        self.n_collisions = 0\n",
    "        self.n_goals      = 0\n",
    "        self.n_timeouts   = 0\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.action_low is None:\n",
    "            self.action_low  = float(self.model.action_space.low)\n",
    "            self.action_high = float(self.model.action_space.high)\n",
    "\n",
    "        actions  = self.locals['actions']\n",
    "        rewards  = self.locals['rewards']\n",
    "        done     = self.locals['dones'][0]\n",
    "        infos    = self.locals['infos']\n",
    "\n",
    "        raw_action = float(actions[0])\n",
    "        raw_action = max(self.action_low, min(self.action_high, raw_action))\n",
    "        gamma      = 0.5*(raw_action + 1.0)\n",
    "\n",
    "        r = float(rewards[0])\n",
    "        self.total_reward += r\n",
    "\n",
    "        # Check if \"done\" => an episode ended. \n",
    "        # Info might contain \"terminal_reason\" we can log\n",
    "        if done:\n",
    "            info0 = infos[0]\n",
    "            terminal_reason = info0.get(\"terminal_reason\", None)\n",
    "            if terminal_reason == \"collision\":\n",
    "                self.n_collisions += 1\n",
    "            elif terminal_reason == \"goal\":\n",
    "                self.n_goals += 1\n",
    "            elif terminal_reason == \"timeout\":\n",
    "                self.n_timeouts += 1\n",
    "\n",
    "            self.episode_rewards.append(self.total_reward)\n",
    "            avg_g = np.mean(self.current_episode_gammas) if len(self.current_episode_gammas) > 0 else 0.0\n",
    "            self.episode_gammas.append(avg_g)\n",
    "            self.total_reward = 0.0\n",
    "            self.current_episode_gammas.clear()\n",
    "        else:\n",
    "            self.current_episode_gammas.append(gamma)\n",
    "\n",
    "        return True\n",
    "\n",
    "    def save_metrics(self, save_dir=\"training_metrics\"):\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        # Plot episode rewards\n",
    "        plt.figure(figsize=(8,5))\n",
    "        plt.plot(self.episode_rewards, label=\"Episode Reward\")\n",
    "        plt.title(\"Episode Reward Over Time\")\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Reward\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(save_dir, \"episode_reward.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        # Plot average gamma\n",
    "        plt.figure(figsize=(8,5))\n",
    "        plt.plot(self.episode_gammas, label=\"Average Gamma\")\n",
    "        plt.title(\"Average Gamma per Episode\")\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Gamma\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(save_dir, \"average_gamma.png\"))\n",
    "        plt.close()\n",
    "\n",
    "        # Summary text\n",
    "        with open(os.path.join(save_dir, \"training_summary.txt\"), 'w') as f:\n",
    "            f.write(f\"Total Episodes: {len(self.episode_rewards)}\\n\")\n",
    "            if len(self.episode_rewards) > 0:\n",
    "                f.write(f\"Mean Reward: {np.mean(self.episode_rewards):.3f}\\n\")\n",
    "                f.write(f\"Mean Gamma: {np.mean(self.episode_gammas):.3f}\\n\")\n",
    "                f.write(f\"Best Episode: {max(self.episode_rewards):.3f}\\n\")\n",
    "                f.write(f\"Worst Episode: {min(self.episode_rewards):.3f}\\n\")\n",
    "\n",
    "            # Additional info\n",
    "            f.write(f\"\\nCollisions Count: {self.n_collisions}\\n\")\n",
    "            f.write(f\"Goals Reached:    {self.n_goals}\\n\")\n",
    "            f.write(f\"Timeouts:        {self.n_timeouts}\\n\")\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Training environment with shaping reward + terminal_reason\n",
    "###############################################################################\n",
    "class DemoArbitrationEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    A single environment where the action is a scalar in [-1,1], mapped to gamma in [0,1].\n",
    "    The dot is placed in a 2D field with obstacles and multiple goals.\n",
    "    Reward shaping is added: higher gamma is favored near obstacles/goals, lower gamma away from them.\n",
    "    Now also sets 'info[\"terminal_reason\"]' so we can track collisions, timeouts, goals in callback.\n",
    "    \"\"\"\n",
    "    metadata = {\"render_modes\":[\"human\"], \"render_fps\":RENDER_FPS}\n",
    "\n",
    "    def __init__(self, visualize=False):\n",
    "        super().__init__()\n",
    "        self.visualize= visualize\n",
    "\n",
    "        low  = np.array([0, 0, -1, -1, 0, 0, -1, -1, 0], dtype=np.float32)\n",
    "        high = np.array([\n",
    "            FULL_VIEW_SIZE[0], FULL_VIEW_SIZE[1],\n",
    "            1, 1,\n",
    "            FULL_VIEW_SIZE[0], FULL_VIEW_SIZE[1],\n",
    "            1, 1,\n",
    "            1\n",
    "        ], dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=low, high=high, shape=(9,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(), dtype=np.float32)\n",
    "\n",
    "        self.dot_pos = None\n",
    "        self.goal_pos= None\n",
    "        self.step_count=0\n",
    "        self.max_steps=300\n",
    "        self.episode_reward=0.0\n",
    "        self.max_dist= math.hypot(FULL_VIEW_SIZE[0], FULL_VIEW_SIZE[1])\n",
    "        self.recent_positions = []\n",
    "\n",
    "        # Pygame stuff only if visualize=True\n",
    "        if self.visualize and pygame is not None:\n",
    "            pygame.init()\n",
    "            self.window = pygame.display.set_mode(FULL_VIEW_SIZE)\n",
    "            pygame.display.set_caption(\"Train PPO with Gamma Shaping\")\n",
    "            self.clock = pygame.time.Clock()\n",
    "        else:\n",
    "            self.window = None\n",
    "            self.clock = None\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.step_count=0\n",
    "        self.episode_reward=0.0\n",
    "        self.dot_pos = START_POS.copy()\n",
    "        idx= np.random.randint(len(GOALS))\n",
    "        self.goal_pos= np.array(GOALS[idx], dtype=np.float32)\n",
    "        self.recent_positions.clear()\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        raw_a = float(action)\n",
    "        raw_a = np.clip(raw_a, -1.0, 1.0)\n",
    "        gamma = 0.5*(raw_a + 1.0)  # -> [0..1]\n",
    "\n",
    "        self.step_count += 1\n",
    "\n",
    "        # \"Perfect\" direction to current goal\n",
    "        w_dir = potential_field_dir(self.dot_pos, self.goal_pos)\n",
    "\n",
    "        # \"Human/noisy\" direction = w_dir + random noise\n",
    "        noise = np.random.normal(0, NOISE_MAGNITUDE, size=2)\n",
    "        h_dir = w_dir + noise\n",
    "        h_mag = np.hypot(h_dir[0], h_dir[1])\n",
    "        if h_mag > 1e-6:\n",
    "            h_dir /= h_mag\n",
    "\n",
    "        # final direction\n",
    "        c_dir = gamma * w_dir + (1 - gamma) * h_dir\n",
    "        c_mag = np.hypot(c_dir[0], c_dir[1])\n",
    "        if c_mag > 1e-6:\n",
    "            c_dir /= c_mag\n",
    "\n",
    "        old_pos = self.dot_pos.copy()\n",
    "        move_vec= c_dir * MAX_SPEED\n",
    "        new_pos = self.dot_pos + move_vec\n",
    "\n",
    "        # collision check on line old_pos->new_pos\n",
    "        blocked = line_collision(self.dot_pos, new_pos)\n",
    "        if not blocked:\n",
    "            # clip to boundaries\n",
    "            new_pos[0] = np.clip(new_pos[0], 0, FULL_VIEW_SIZE[0])\n",
    "            new_pos[1] = np.clip(new_pos[1], 0, FULL_VIEW_SIZE[1])\n",
    "            self.dot_pos= new_pos\n",
    "\n",
    "        collided = inside_obstacle(self.dot_pos)\n",
    "        info = {}  # to store \"terminal_reason\"\n",
    "        if collided:\n",
    "            original_reward = -20.0\n",
    "            done = True\n",
    "            info[\"terminal_reason\"] = \"collision\"\n",
    "        else:\n",
    "            dist_goal = distance(self.dot_pos, self.goal_pos)\n",
    "            if dist_goal < (DOT_RADIUS + TARGET_RADIUS):\n",
    "                original_reward = 10.0\n",
    "                done = False\n",
    "                info[\"terminal_reason\"] = \"goal\"\n",
    "                # new random goal for continuing\n",
    "                idx= np.random.randint(len(GOALS))\n",
    "                self.goal_pos= np.array(GOALS[idx], dtype=np.float32)\n",
    "            else:\n",
    "                original_reward = 0.0\n",
    "                done = False\n",
    "                info[\"terminal_reason\"] = None\n",
    "\n",
    "        truncated = (self.step_count >= self.max_steps)\n",
    "        if truncated and not done:\n",
    "            # If we haven't collided or reached goal, but time is up\n",
    "            info[\"terminal_reason\"] = \"timeout\"\n",
    "\n",
    "        # === Reward shaping logic: encourage high gamma near obstacles/goals, low gamma otherwise\n",
    "        near_obstacle = False\n",
    "        obstacle_threshold = 150.0\n",
    "        for obs in OBSTACLES:\n",
    "            if distance(self.dot_pos, obs) < obstacle_threshold:\n",
    "                near_obstacle = True\n",
    "                break\n",
    "\n",
    "        dist_to_goal = distance(self.dot_pos, self.goal_pos)\n",
    "        near_goal = (dist_to_goal < 150.0)\n",
    "\n",
    "        shaping_reward = 0.0\n",
    "        if near_obstacle or near_goal:\n",
    "            shaping_reward += 0.05 * gamma\n",
    "        else:\n",
    "            shaping_reward += 0.05 * (1.0 - gamma)\n",
    "\n",
    "        reward = original_reward + shaping_reward\n",
    "        self.episode_reward += reward\n",
    "\n",
    "        # Optionally render\n",
    "        if self.visualize:\n",
    "            self._render(old_pos, w_dir, h_dir, c_dir, collided)\n",
    "\n",
    "        obs = self._get_obs()\n",
    "        return obs, float(reward), done, truncated, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        to_g = self.goal_pos - self.dot_pos\n",
    "        d = math.hypot(to_g[0], to_g[1])\n",
    "        dist_ratio = d / self.max_dist if self.max_dist > 1e-6 else 0.0\n",
    "\n",
    "        # 9D observation:\n",
    "        # [dot_x, dot_y, h_dir_x, h_dir_y, goal_x, goal_y, w_dir_x, w_dir_y, dist_ratio]\n",
    "        w_dir = potential_field_dir(self.dot_pos, self.goal_pos)\n",
    "        noise = np.random.normal(0, NOISE_MAGNITUDE, size=2)\n",
    "        h_dir = w_dir + noise\n",
    "        h_mag = np.hypot(h_dir[0], h_dir[1])\n",
    "        if h_mag > 1e-6:\n",
    "            h_dir /= h_mag\n",
    "\n",
    "        obs = np.concatenate([\n",
    "            self.dot_pos,\n",
    "            h_dir,\n",
    "            self.goal_pos,\n",
    "            w_dir,\n",
    "            [dist_ratio]\n",
    "        ]).astype(np.float32)\n",
    "        return obs\n",
    "\n",
    "    def _render(self, old_pos, w_dir, h_dir, c_dir, collided):\n",
    "        if not self.window or not pygame:\n",
    "            return\n",
    "\n",
    "        self.window.fill(WHITE)\n",
    "\n",
    "        # obstacles\n",
    "        for obs in OBSTACLES:\n",
    "            pygame.draw.circle(self.window, GRAY, (int(obs[0]), int(obs[1])),\n",
    "                               OBSTACLE_RADIUS)\n",
    "\n",
    "        # goals\n",
    "        for i, gpos in enumerate(GOALS):\n",
    "            pygame.draw.circle(self.window, YELLOW, (int(gpos[0]), int(gpos[1])),\n",
    "                               TARGET_RADIUS)\n",
    "        # highlight the current goal\n",
    "        pygame.draw.circle(self.window, BLACK,\n",
    "                           (int(self.goal_pos[0]), int(self.goal_pos[1])),\n",
    "                           TARGET_RADIUS+2, width=2)\n",
    "\n",
    "        # dot\n",
    "        pygame.draw.circle(self.window, BLACK,\n",
    "                           (int(self.dot_pos[0]), int(self.dot_pos[1])),\n",
    "                           DOT_RADIUS, width=2)\n",
    "\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(RENDER_FPS)\n",
    "\n",
    "    def close(self):\n",
    "        if self.visualize and pygame is not None:\n",
    "            pygame.quit()\n",
    "        super().close()\n",
    "\n",
    "###############################################################################\n",
    "# Training loop\n",
    "###############################################################################\n",
    "def train(visualize=True, total_timesteps=30_000):\n",
    "    from stable_baselines3.common.callbacks import CallbackList\n",
    "\n",
    "    env = DemoArbitrationEnv(visualize=visualize)\n",
    "    metrics_callback = MetricsCallback()\n",
    "    callback = CallbackList([metrics_callback])\n",
    "\n",
    "    model = PPO(\n",
    "        policy=\"MlpPolicy\",\n",
    "        env=env,\n",
    "        learning_rate=3e-4,\n",
    "        n_steps=1024,\n",
    "        batch_size=1024,\n",
    "        n_epochs=4,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        clip_range=0.2,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        print(f\"Starting PPO training (visualize={visualize}) ...\")\n",
    "        model.learn(total_timesteps=total_timesteps, callback=callback)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Training interrupted; saving partial model...\")\n",
    "\n",
    "    # Save the final model\n",
    "    os.makedirs(\"trained_models\", exist_ok=True)\n",
    "    model.save(\"demo_arbitration_ppo\")\n",
    "    print(\"Model saved to trained_models/demo_arbitration_ppo.zip\")\n",
    "\n",
    "    # Save training metrics\n",
    "    metrics_callback.save_metrics(\"training_metrics\")\n",
    "    print(\"Metrics + summary saved in training_metrics/\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "###############################################################################\n",
    "# Main\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    vis = (len(sys.argv) > 1 and sys.argv[1].lower() == \"visualize\")\n",
    "    train(visualize=vis, total_timesteps=30_000)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
