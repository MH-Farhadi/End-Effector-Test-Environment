{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.0 (SDL 2.28.4, Python 3.10.9)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Episode 1/1000, Reward: -19.29, Avg Reward (100): -19.29\n",
      "Episode 2/1000, Reward: -12.90, Avg Reward (100): -16.10\n",
      "Episode 3/1000, Reward: -19.15, Avg Reward (100): -17.11\n",
      "Episode 4/1000, Reward: -16.77, Avg Reward (100): -17.03\n",
      "Episode 5/1000, Reward: -10.19, Avg Reward (100): -15.66\n",
      "\n",
      "Training interrupted by user\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "from collections import deque\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "class SharedControlEnv:\n",
    "    \"\"\"Wrapper around the pygame environment for RL training\"\"\"\n",
    "    \n",
    "    def __init__(self, window_size=(1200, 800), render_mode=None):\n",
    "        self.window_size = window_size\n",
    "        self.render_mode = render_mode\n",
    "        \n",
    "        # Constants from original environment\n",
    "        self.max_speed = 3\n",
    "        self.dot_radius = 30\n",
    "        self.target_radius = 10\n",
    "        self.goal_detection_radius = self.dot_radius + self.target_radius\n",
    "        \n",
    "        # Initialize pygame if rendering\n",
    "        if self.render_mode == 'human':\n",
    "            try:\n",
    "                pygame.init()\n",
    "                self.screen = pygame.display.set_mode(window_size)\n",
    "                self.clock = pygame.time.Clock()\n",
    "            except pygame.error as e:\n",
    "                print(f\"Failed to initialize pygame: {e}\")\n",
    "                self.render_mode = None\n",
    "        \n",
    "        # State space components\n",
    "        self.num_lidar_rays = 16  # Number of rays for simplified lidar\n",
    "        self.state_history_len = 5  # Number of past states to keep\n",
    "        \n",
    "        # Environment state\n",
    "        self.dot_pos = None\n",
    "        self.targets = None\n",
    "        self.current_target_idx = None\n",
    "        self.reached_goal = False\n",
    "        self.state_history = deque(maxlen=self.state_history_len)\n",
    "        \n",
    "        # Define observation and action spaces\n",
    "        self.observation_dim = (self.num_lidar_rays +  # Lidar readings\n",
    "                              2 +  # dot position\n",
    "                              2 +  # current target position\n",
    "                              2 +  # human input\n",
    "                              1)   # previous gamma\n",
    "        self.action_dim = 1  # gamma value\n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "    def get_state(self, human_input):\n",
    "        \"\"\"Convert environment state to RL observation\"\"\"\n",
    "        # Validate human input\n",
    "        if not isinstance(human_input, (list, np.ndarray)) or len(human_input) != 2:\n",
    "            raise ValueError(\"Human input must be a list or array of length 2\")\n",
    "            \n",
    "        # Simulate lidar readings\n",
    "        lidar_readings = self._get_lidar_readings()\n",
    "        \n",
    "        # Normalize positions to [0,1]\n",
    "        norm_dot_pos = [\n",
    "            self.dot_pos[0] / self.window_size[0],\n",
    "            self.dot_pos[1] / self.window_size[1]\n",
    "        ]\n",
    "        \n",
    "        curr_target = self.targets[self.current_target_idx]\n",
    "        norm_target_pos = [\n",
    "            curr_target[0] / self.window_size[0],\n",
    "            curr_target[1] / self.window_size[1]\n",
    "        ]\n",
    "        \n",
    "        # Normalize and clip human input\n",
    "        norm_human_input = [\n",
    "            np.clip(human_input[0] / self.max_speed, -1, 1),\n",
    "            np.clip(human_input[1] / self.max_speed, -1, 1)\n",
    "        ]\n",
    "        \n",
    "        # Combine all state components\n",
    "        state = np.concatenate([\n",
    "            lidar_readings,\n",
    "            norm_dot_pos,\n",
    "            norm_target_pos,\n",
    "            norm_human_input,\n",
    "            [self.current_gamma]\n",
    "        ])\n",
    "        \n",
    "        return state.astype(np.float32)\n",
    "\n",
    "    def _get_lidar_readings(self):\n",
    "        \"\"\"Simulate simplified lidar readings\"\"\"\n",
    "        readings = []\n",
    "        for i in range(self.num_lidar_rays):\n",
    "            angle = 2 * math.pi * i / self.num_lidar_rays\n",
    "            reading = self._cast_ray(angle)\n",
    "            readings.append(reading / max(self.window_size))  # Normalize\n",
    "        return np.array(readings, dtype=np.float32)\n",
    "\n",
    "    def _cast_ray(self, angle, max_dist=None):\n",
    "        \"\"\"Cast a ray and return distance to nearest obstacle/wall\"\"\"\n",
    "        if max_dist is None:\n",
    "            max_dist = math.hypot(*self.window_size)\n",
    "            \n",
    "        dir_x = math.cos(angle)\n",
    "        dir_y = math.sin(angle)\n",
    "        \n",
    "        # Start from dot position\n",
    "        start_x, start_y = self.dot_pos\n",
    "        \n",
    "        # Check wall intersections\n",
    "        wall_dist = max_dist\n",
    "        \n",
    "        # Check intersection with each wall\n",
    "        # Left wall\n",
    "        if dir_x < 0:\n",
    "            t = -start_x / dir_x\n",
    "            y = start_y + t * dir_y\n",
    "            if 0 <= y <= self.window_size[1]:\n",
    "                wall_dist = min(wall_dist, abs(t))\n",
    "                \n",
    "        # Right wall\n",
    "        elif dir_x > 0:\n",
    "            t = (self.window_size[0] - start_x) / dir_x\n",
    "            y = start_y + t * dir_y\n",
    "            if 0 <= y <= self.window_size[1]:\n",
    "                wall_dist = min(wall_dist, abs(t))\n",
    "                \n",
    "        # Top wall\n",
    "        if dir_y < 0:\n",
    "            t = -start_y / dir_y\n",
    "            x = start_x + t * dir_x\n",
    "            if 0 <= x <= self.window_size[0]:\n",
    "                wall_dist = min(wall_dist, abs(t))\n",
    "                \n",
    "        # Bottom wall\n",
    "        elif dir_y > 0:\n",
    "            t = (self.window_size[1] - start_y) / dir_y\n",
    "            x = start_x + t * dir_x\n",
    "            if 0 <= x <= self.window_size[0]:\n",
    "                wall_dist = min(wall_dist, abs(t))\n",
    "        \n",
    "        return wall_dist\n",
    "\n",
    "    def step(self, action, human_input):\n",
    "        \"\"\"\n",
    "        Execute one environment step\n",
    "        \n",
    "        Args:\n",
    "            action: gamma value between 0 and 1\n",
    "            human_input: [dx, dy] from keyboard/joystick\n",
    "        \"\"\"\n",
    "        # Validate inputs\n",
    "        action = np.clip(float(action), 0.0, 1.0)\n",
    "        human_input = np.array(human_input, dtype=np.float32)\n",
    "        if human_input.shape != (2,):\n",
    "            raise ValueError(\"Human input must be a 2D vector\")\n",
    "            \n",
    "        self.current_gamma = action\n",
    "        \n",
    "        # Move dot using original move_dot logic\n",
    "        h_dx, h_dy = human_input\n",
    "        h_mag = math.hypot(h_dx, h_dy)\n",
    "        h_dir = [h_dx / h_mag, h_dy / h_mag] if h_mag > 0 else [0, 0]\n",
    "\n",
    "        target_pos = self.targets[self.current_target_idx]\n",
    "        w_dx = target_pos[0] - self.dot_pos[0]\n",
    "        w_dy = target_pos[1] - self.dot_pos[1]\n",
    "        w_mag = math.hypot(w_dx, w_dy)\n",
    "        w_dir = [w_dx / w_mag, w_dy / w_mag] if w_mag > 0 else [0, 0]\n",
    "\n",
    "        # Scale movement\n",
    "        step_size = self.max_speed * min(max(h_mag / self.max_speed, 0), 1)\n",
    "        \n",
    "        # Calculate movement components\n",
    "        w_move = [\n",
    "            self.current_gamma * w_dir[0] * step_size,\n",
    "            self.current_gamma * w_dir[1] * step_size\n",
    "        ]\n",
    "        \n",
    "        h_move = [\n",
    "            (1 - self.current_gamma) * h_dir[0] * step_size,\n",
    "            (1 - self.current_gamma) * h_dir[1] * step_size\n",
    "        ]\n",
    "\n",
    "        # Update position\n",
    "        new_pos = [\n",
    "            self.dot_pos[0] + w_move[0] + h_move[0],\n",
    "            self.dot_pos[1] + w_move[1] + h_move[1]\n",
    "        ]\n",
    "        \n",
    "        # Clip to window boundaries\n",
    "        self.dot_pos = [\n",
    "            max(0, min(self.window_size[0], new_pos[0])),\n",
    "            max(0, min(self.window_size[1], new_pos[1]))\n",
    "        ]\n",
    "\n",
    "        # Check if goal reached\n",
    "        dist_to_goal = math.hypot(\n",
    "            self.dot_pos[0] - target_pos[0],\n",
    "            self.dot_pos[1] - target_pos[1]\n",
    "        )\n",
    "        self.reached_goal = dist_to_goal < self.goal_detection_radius\n",
    "\n",
    "        # Calculate reward\n",
    "        reward = self._compute_reward(dist_to_goal)\n",
    "        \n",
    "        # Get new state\n",
    "        state = self.get_state(human_input)\n",
    "        self.state_history.append(state)\n",
    "        \n",
    "        # Check if episode is done\n",
    "        done = self.reached_goal\n",
    "        \n",
    "        # Additional info\n",
    "        info = {\n",
    "            'distance_to_goal': dist_to_goal,\n",
    "            'reached_goal': self.reached_goal,\n",
    "            'gamma': self.current_gamma\n",
    "        }\n",
    "        \n",
    "        return state, reward, done, info\n",
    "\n",
    "    def _compute_reward(self, dist_to_goal):\n",
    "        \"\"\"Compute reward based on paper's reward function with better scaling\"\"\"\n",
    "        # Safety reward (normalized to [-1, 0])\n",
    "        if dist_to_goal >= 0.8:\n",
    "            rsafe = 0\n",
    "        elif 0.5 < dist_to_goal < 0.8:\n",
    "            rsafe = -5 * (0.8 - dist_to_goal)  # Scales linearly from 0 to -1.5\n",
    "        else:\n",
    "            rsafe = -1  # Cap at -1 instead of -500\n",
    "\n",
    "        # Smoothness reward (normalized to [-1, 0])\n",
    "        if len(self.state_history) > 1:\n",
    "            prev_gamma = self.state_history[-1][-1]\n",
    "            rsm = -abs(self.current_gamma - prev_gamma)  # Already in [0,1] range\n",
    "        else:\n",
    "            rsm = 0\n",
    "\n",
    "        # Goal reward (normalized to [-0.1, 1])\n",
    "        rgoal = 1.0 if self.reached_goal else -0.1\n",
    "        \n",
    "        # Weight the components\n",
    "        return 0.4 * rsafe + 0.2 * rsm + 0.4 * rgoal\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment state\"\"\"\n",
    "        # Reset dot position to center\n",
    "        self.dot_pos = [self.window_size[0] // 2, self.window_size[1] // 2]\n",
    "        \n",
    "        # Generate new targets\n",
    "        self.targets = []\n",
    "        margin = 100  # Keep targets away from edges\n",
    "        for _ in range(3):  # 3 targets like original environment\n",
    "            self.targets.append([\n",
    "                random.randint(margin, self.window_size[0] - margin),\n",
    "                random.randint(margin, self.window_size[1] - margin)\n",
    "            ])\n",
    "        \n",
    "        self.current_target_idx = 0\n",
    "        self.reached_goal = False\n",
    "        self.current_gamma = 0.2  # Initial gamma value\n",
    "        \n",
    "        # Clear history\n",
    "        self.state_history.clear()\n",
    "        \n",
    "        # Get initial state\n",
    "        state = self.get_state([0, 0])  # No initial human input\n",
    "        self.state_history.append(state)\n",
    "        \n",
    "        return state\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Render environment if render_mode is 'human'\"\"\"\n",
    "        if self.render_mode != 'human':\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            self.screen.fill((255, 255, 255))\n",
    "            \n",
    "            # Draw targets\n",
    "            for i, target in enumerate(self.targets):\n",
    "                color = (255, 255, 0) if i > self.current_target_idx else (200, 200, 200)\n",
    "                pygame.draw.circle(self.screen, color, \n",
    "                                (int(target[0]), int(target[1])), \n",
    "                                self.target_radius)\n",
    "                \n",
    "            # Highlight current target\n",
    "            current_target = self.targets[self.current_target_idx]\n",
    "            pygame.draw.circle(self.screen, (0, 0, 0),\n",
    "                             (int(current_target[0]), int(current_target[1])),\n",
    "                             self.target_radius + 2, 2)\n",
    "            \n",
    "            # Draw dot\n",
    "            pygame.draw.circle(self.screen, (0, 0, 0),\n",
    "                             (int(self.dot_pos[0]), int(self.dot_pos[1])),\n",
    "                             self.dot_radius, 2)\n",
    "            \n",
    "            # Draw gamma value\n",
    "            font = pygame.font.Font(None, 36)\n",
    "            gamma_text = font.render(f'γ: {self.current_gamma:.2f}', True, (0, 0, 0))\n",
    "            self.screen.blit(gamma_text, (10, 10))\n",
    "            \n",
    "            pygame.display.flip()\n",
    "            self.clock.tick(60)\n",
    "            \n",
    "        except pygame.error as e:\n",
    "            print(f\"Render error: {e}\")\n",
    "            self.render_mode = None\n",
    "\n",
    "    def close(self):\n",
    "        if self.render_mode == 'human':\n",
    "            pygame.quit()\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        # Shared feature extractor\n",
    "        self.feature_net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Actor network (policy)\n",
    "        self.actor_mean = nn.Linear(hidden_dim, action_dim)\n",
    "        self.actor_log_std = nn.Parameter(torch.zeros(1, action_dim))\n",
    "        \n",
    "        # Critic network (value function)\n",
    "        self.critic = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.orthogonal_(module.weight, gain=np.sqrt(2))\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "    \n",
    "    def forward(self, state):\n",
    "        features = self.feature_net(state)\n",
    "        \n",
    "        # Actor: get mean and std of action distribution\n",
    "        action_mean = torch.sigmoid(self.actor_mean(features))  # Ensure gamma is between 0 and 1\n",
    "        action_std = torch.exp(self.actor_log_std)\n",
    "        \n",
    "        # Critic: get state value\n",
    "        value = self.critic(features)\n",
    "        \n",
    "        return action_mean, action_std, value\n",
    "    \n",
    "    def get_action_distribution(self, state):\n",
    "        action_mean, action_std, _ = self(state)\n",
    "        return torch.distributions.Normal(action_mean, action_std)\n",
    "\n",
    "class PPOSharedControl:\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128, lr=3e-4, gamma=0.99, \n",
    "                 epsilon=0.2, c1=1.0, c2=0.01):\n",
    "        \"\"\"\n",
    "        Initialize PPO agent for shared control\n",
    "        \n",
    "        Args:\n",
    "            state_dim: Dimension of state space\n",
    "            action_dim: Dimension of action space\n",
    "            hidden_dim: Hidden layer dimension\n",
    "            lr: Learning rate\n",
    "            gamma: Discount factor\n",
    "            epsilon: PPO clipping parameter\n",
    "            c1: Value function loss coefficient\n",
    "            c2: Entropy bonus coefficient\n",
    "        \"\"\"\n",
    "        self.actor_critic = ActorCritic(state_dim, action_dim, hidden_dim)\n",
    "        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=lr)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"Sample action from current policy\"\"\"\n",
    "        with torch.no_grad():\n",
    "            dist = self.actor_critic.get_action_distribution(state)\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action)\n",
    "            \n",
    "            # Clip action to [0, 1] since it represents gamma\n",
    "            action = torch.clamp(action, 0.0, 1.0)\n",
    "            \n",
    "        return action, log_prob\n",
    "    \n",
    "    def get_value(self, state):\n",
    "        \"\"\"Get value estimate for state\"\"\"\n",
    "        with torch.no_grad():\n",
    "            _, _, value = self.actor_critic(state)\n",
    "        return value\n",
    "    \n",
    "    def update(self, states, actions, old_log_probs, returns, advantages, \n",
    "              epochs=10, batch_size=64):\n",
    "        \"\"\"Update policy using PPO\"\"\"\n",
    "        # Convert to tensors if they aren't already\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.FloatTensor(actions)\n",
    "        old_log_probs = torch.FloatTensor(old_log_probs)\n",
    "        returns = torch.FloatTensor(returns)\n",
    "        advantages = torch.FloatTensor(advantages)\n",
    "        \n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # PPO update over multiple epochs\n",
    "        for _ in range(epochs):\n",
    "            # Generate random mini-batches\n",
    "            indices = torch.randperm(len(states))\n",
    "            for start_idx in range(0, len(states), batch_size):\n",
    "                # Get mini-batch\n",
    "                idx = indices[start_idx:start_idx + batch_size]\n",
    "                batch_states = states[idx]\n",
    "                batch_actions = actions[idx]\n",
    "                batch_old_log_probs = old_log_probs[idx]\n",
    "                batch_returns = returns[idx]\n",
    "                batch_advantages = advantages[idx]\n",
    "                \n",
    "                # Get current action distribution and value\n",
    "                dist = self.actor_critic.get_action_distribution(batch_states)\n",
    "                _, _, values = self.actor_critic(batch_states)\n",
    "                \n",
    "                # Calculate ratios and surrogate losses\n",
    "                new_log_probs = dist.log_prob(batch_actions)\n",
    "                ratio = torch.exp(new_log_probs - batch_old_log_probs)\n",
    "                \n",
    "                # PPO policy loss\n",
    "                surr1 = ratio * batch_advantages\n",
    "                surr2 = torch.clamp(ratio, 1.0 - self.epsilon, 1.0 + self.epsilon) * batch_advantages\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "                \n",
    "                # Value function loss\n",
    "                value_loss = F.mse_loss(values.squeeze(), batch_returns)\n",
    "                \n",
    "                # Entropy bonus for exploration\n",
    "                entropy = dist.entropy().mean()\n",
    "                \n",
    "                # Total loss\n",
    "                total_loss = policy_loss + self.c1 * value_loss - self.c2 * entropy\n",
    "                \n",
    "                # Update network\n",
    "                self.optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.actor_critic.parameters(), 0.5)\n",
    "                self.optimizer.step()\n",
    "    \n",
    "    def save(self, path):\n",
    "        \"\"\"Save model\"\"\"\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        torch.save({\n",
    "            'actor_critic_state_dict': self.actor_critic.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "        }, path)\n",
    "    \n",
    "    def load(self, path):\n",
    "        \"\"\"Load model\"\"\"\n",
    "        checkpoint = torch.load(path)\n",
    "        self.actor_critic.load_state_dict(checkpoint['actor_critic_state_dict'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "def train_ppo(env, episodes=1000, steps_per_episode=1000, checkpoint_freq=100):\n",
    "    \"\"\"\n",
    "    Train PPO agent on shared control environment\n",
    "    \n",
    "    Args:\n",
    "        env: SharedControlEnv instance\n",
    "        episodes: Number of training episodes\n",
    "        steps_per_episode: Maximum steps per episode\n",
    "        checkpoint_freq: Save checkpoint every n episodes\n",
    "    \"\"\"\n",
    "    state_dim = env.observation_dim\n",
    "    action_dim = env.action_dim\n",
    "    \n",
    "    # Initialize PPO agent\n",
    "    agent = PPOSharedControl(state_dim, action_dim)\n",
    "    \n",
    "    # Create directory for checkpoints\n",
    "    checkpoint_dir = f'checkpoints_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}'\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # Training metrics\n",
    "    best_reward = float('-inf')\n",
    "    episode_rewards = []\n",
    "    \n",
    "    # Training loop\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        \n",
    "        # Storage for episode data\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        log_probs = []\n",
    "        \n",
    "        for step in range(steps_per_episode):\n",
    "            # Simulate human input (can be replaced with real human data)\n",
    "            human_input = simulate_human_input(env)\n",
    "            \n",
    "            # Get action from policy\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            action, log_prob = agent.get_action(state_tensor)\n",
    "            \n",
    "            # Step environment\n",
    "            next_state, reward, done, info = env.step(action.item(), human_input)\n",
    "            \n",
    "            # Store transition\n",
    "            states.append(state)\n",
    "            actions.append(action.squeeze().cpu().numpy())\n",
    "            rewards.append(reward)\n",
    "            log_probs.append(log_prob.squeeze().cpu().numpy())\n",
    "            \n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if env.render_mode == 'human':\n",
    "                env.render()\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Store episode reward\n",
    "        episode_rewards.append(episode_reward)\n",
    "        \n",
    "        # Convert lists to arrays for batch processing\n",
    "        states = np.array(states, dtype=np.float32)\n",
    "        actions = np.array(actions, dtype=np.float32)\n",
    "        rewards = np.array(rewards, dtype=np.float32)\n",
    "        log_probs = np.array(log_probs, dtype=np.float32)\n",
    "        \n",
    "        # Calculate returns and advantages\n",
    "        returns = compute_returns(rewards, agent.gamma)\n",
    "        advantages = compute_advantages(returns, states, agent)\n",
    "        \n",
    "        # Update policy\n",
    "        agent.update(states, actions, log_probs, returns, advantages)\n",
    "        \n",
    "        # Save checkpoint if best performance\n",
    "        if episode_reward > best_reward:\n",
    "            best_reward = episode_reward\n",
    "            agent.save(os.path.join(checkpoint_dir, 'best_model.pth'))\n",
    "            \n",
    "        # Regular checkpoint saving\n",
    "        if (episode + 1) % checkpoint_freq == 0:\n",
    "            agent.save(os.path.join(checkpoint_dir, f'checkpoint_{episode+1}.pth'))\n",
    "        \n",
    "        # Print progress\n",
    "        avg_reward = np.mean(episode_rewards[-100:]) if len(episode_rewards) >= 100 else np.mean(episode_rewards)\n",
    "        print(f\"Episode {episode+1}/{episodes}, Reward: {episode_reward:.2f}, Avg Reward (100): {avg_reward:.2f}\")\n",
    "        \n",
    "        # Early stopping if solved\n",
    "        if avg_reward > 800 and len(episode_rewards) >= 100:\n",
    "            print(\"Environment solved!\")\n",
    "            agent.save(os.path.join(checkpoint_dir, 'solved_model.pth'))\n",
    "            break\n",
    "    \n",
    "    return agent, episode_rewards\n",
    "\n",
    "def simulate_human_input(env):\n",
    "    \"\"\"Simulate human input for training\"\"\"\n",
    "    # Get vector to current target\n",
    "    target = env.targets[env.current_target_idx]\n",
    "    dx = target[0] - env.dot_pos[0]\n",
    "    dy = target[1] - env.dot_pos[1]\n",
    "    \n",
    "    # Add some noise to simulate imperfect human input\n",
    "    dx += np.random.normal(0, 0.2)\n",
    "    dy += np.random.normal(0, 0.2)\n",
    "    \n",
    "    # Normalize and scale\n",
    "    mag = math.hypot(dx, dy)\n",
    "    if mag > 0:\n",
    "        dx = dx / mag * env.max_speed\n",
    "        dy = dy / mag * env.max_speed\n",
    "    \n",
    "    return np.array([dx, dy], dtype=np.float32)\n",
    "\n",
    "def compute_returns(rewards, gamma):\n",
    "    \"\"\"Compute discounted returns\"\"\"\n",
    "    returns = []\n",
    "    R = 0\n",
    "    for r in reversed(rewards):\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns, dtype=torch.float32)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + 1e-5)\n",
    "    return returns\n",
    "\n",
    "def compute_advantages(returns, states, agent):\n",
    "    \"\"\"Compute advantages (returns - value estimates)\"\"\"\n",
    "    states = torch.FloatTensor(states)\n",
    "    values = agent.get_value(states)\n",
    "    advantages = returns - values.detach().squeeze()\n",
    "    return advantages\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create environment\n",
    "    env = SharedControlEnv(render_mode='human')\n",
    "    \n",
    "    try:\n",
    "        # Train agent\n",
    "        trained_agent, rewards_history = train_ppo(env)\n",
    "        \n",
    "        # Final save\n",
    "        trained_agent.save('final_model.pth')\n",
    "        \n",
    "        # Save training history\n",
    "        np.save('training_rewards.npy', np.array(rewards_history))\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTraining interrupted by user\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {e}\")\n",
    "    finally:\n",
    "        env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
