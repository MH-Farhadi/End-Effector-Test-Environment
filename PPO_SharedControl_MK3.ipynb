{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mhfar\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\policies.py:486: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advancing to curriculum level 1\n",
      "Advancing to curriculum level 2\n",
      "Advancing to curriculum level 3\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1897 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 986           |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 4             |\n",
      "|    total_timesteps      | 4096          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00012035112 |\n",
      "|    clip_fraction        | 0.000488      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.00692       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.77          |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | 1.83e-05      |\n",
      "|    std                  | 0.995         |\n",
      "|    value_loss           | 9.72          |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 827         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001446913 |\n",
      "|    clip_fraction        | 0.0141      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.186       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.79        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00134    |\n",
      "|    std                  | 0.977       |\n",
      "|    value_loss           | 3.83        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 775          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007493858 |\n",
      "|    clip_fraction        | 0.0151       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.771        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.454        |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00117     |\n",
      "|    std                  | 0.986        |\n",
      "|    value_loss           | 2.3          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 746          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033173736 |\n",
      "|    clip_fraction        | 0.0324       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.877        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.11         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00313     |\n",
      "|    std                  | 0.983        |\n",
      "|    value_loss           | 3.01         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 724           |\n",
      "|    iterations           | 6             |\n",
      "|    time_elapsed         | 16            |\n",
      "|    total_timesteps      | 12288         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00065466424 |\n",
      "|    clip_fraction        | 0.0019        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.4          |\n",
      "|    explained_variance   | 0.917         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.01          |\n",
      "|    n_updates            | 50            |\n",
      "|    policy_gradient_loss | 2.12e-05      |\n",
      "|    std                  | 0.972         |\n",
      "|    value_loss           | 2.21          |\n",
      "-------------------------------------------\n",
      "\n",
      "Training interrupted. Saving current model...\n",
      "Saved 206 demonstrations to training_demonstrations_interrupted.json\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "import pygame\n",
    "import math\n",
    "import time\n",
    "from collections import deque\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "# Constants\n",
    "FULL_VIEW_SIZE = (1200, 800)\n",
    "MAX_SPEED = 3\n",
    "DOT_RADIUS = 30\n",
    "TARGET_RADIUS = 10\n",
    "GOAL_DETECTION_RADIUS = DOT_RADIUS + TARGET_RADIUS\n",
    "START_POS = [FULL_VIEW_SIZE[0] // 2, FULL_VIEW_SIZE[1] // 2]\n",
    "\n",
    "# Colors\n",
    "WHITE = (255, 255, 255)\n",
    "BLACK = (0, 0, 0)\n",
    "RED = (255, 0, 0)\n",
    "GREEN = (0, 200, 0)\n",
    "BLUE = (0, 0, 255)\n",
    "YELLOW = (255, 255, 0)\n",
    "\n",
    "# Curriculum levels\n",
    "CURRICULUM_LEVELS = {\n",
    "    0: {\"radius\": 200, \"noise\": 0.1, \"obstacles\": 0},  # Easy: Close targets\n",
    "    1: {\"radius\": 400, \"noise\": 0.2, \"obstacles\": 0},  # Medium\n",
    "    2: {\"radius\": None, \"noise\": 0.3, \"obstacles\": 0}, # Hard\n",
    "    3: {\"radius\": None, \"noise\": 0.3, \"obstacles\": 3}  # Expert\n",
    "}\n",
    "\n",
    "# Initialize Pygame\n",
    "pygame.init()\n",
    "FONT_SIZE = 24\n",
    "font = pygame.font.Font(None, FONT_SIZE)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        assert input_dim % num_heads == 0, \"input_dim must be divisible by num_heads\"\n",
    "        self.head_dim = input_dim // num_heads\n",
    "        \n",
    "        self.query = nn.Linear(input_dim, input_dim)\n",
    "        self.key = nn.Linear(input_dim, input_dim)\n",
    "        self.value = nn.Linear(input_dim, input_dim)\n",
    "        self.proj = nn.Linear(input_dim, input_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Linear projections and reshape\n",
    "        q = self.query(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        k = self.key(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        v = self.value(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # Transpose for attention computation\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        context = torch.matmul(attn, v)\n",
    "        context = context.transpose(1, 2).contiguous()\n",
    "        context = context.view(batch_size, seq_len, -1)\n",
    "        \n",
    "        return self.proj(context)\n",
    "\n",
    "\n",
    "\n",
    "class CustomFeaturesExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 64):\n",
    "        super().__init__(observation_space, features_dim)\n",
    "        \n",
    "        n_input = int(np.prod(observation_space.shape))\n",
    "        \n",
    "        # Simple feed-forward network\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(n_input, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, features_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(observations)\n",
    "\n",
    "class CurriculumCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.env = env\n",
    "        self.success_rate = deque(maxlen=100)\n",
    "        self.current_level = 0\n",
    "        \n",
    "    def _on_step(self):\n",
    "        if self.env.episode_done:\n",
    "            self.success_rate.append(1 if self.env.episode_success else 0)\n",
    "            \n",
    "            if len(self.success_rate) == self.success_rate.maxlen:\n",
    "                success_rate = sum(self.success_rate) / len(self.success_rate)\n",
    "                if success_rate > 0.8 and self.current_level < len(CURRICULUM_LEVELS) - 1:\n",
    "                    self.current_level += 1\n",
    "                    self.env.set_curriculum_level(self.current_level)\n",
    "                    print(f\"Advancing to curriculum level {self.current_level}\")\n",
    "        return True\n",
    "\n",
    "class DynamicArbitrationEnv(gym.Env):\n",
    "    def __init__(self, render_mode=None, record_demonstrations=False):\n",
    "        super().__init__()\n",
    "        self.render_mode = render_mode\n",
    "        self.record_demonstrations = record_demonstrations\n",
    "        self.demonstrations = []\n",
    "        self.current_episode_states = []\n",
    "        self.curriculum_level = 0\n",
    "        self.episode_success = False\n",
    "        self.episode_done = False\n",
    "        \n",
    "        if render_mode == \"human\":\n",
    "            self.screen = pygame.display.set_mode(FULL_VIEW_SIZE)\n",
    "            pygame.display.set_caption(\"PPO Training Visualization\")\n",
    "        \n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([0, 0, -MAX_SPEED, -MAX_SPEED, \n",
    "                         -1, -1, \n",
    "                         0, 0,\n",
    "                         -1, -1]),\n",
    "            high=np.array([FULL_VIEW_SIZE[0], FULL_VIEW_SIZE[1], MAX_SPEED, MAX_SPEED,\n",
    "                          1, 1,\n",
    "                          FULL_VIEW_SIZE[0], FULL_VIEW_SIZE[1],\n",
    "                          1, 1]),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        self.action_space = spaces.Box(\n",
    "            low=np.array([0]),\n",
    "            high=np.array([1]),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        self.dot_pos = None\n",
    "        self.dot_vel = None\n",
    "        self.target_pos = None\n",
    "        self.step_count = 0\n",
    "        self.max_steps = 500\n",
    "        self.episode_reward = 0\n",
    "        self.episode_count = 0\n",
    "        self.last_render_time = time.time()\n",
    "\n",
    "    def draw_arrow(self, start_pos, direction, color, length=60):\n",
    "        if direction[0] == 0 and direction[1] == 0:\n",
    "            return\n",
    "            \n",
    "        end_x = start_pos[0] + direction[0] * length\n",
    "        end_y = start_pos[1] + direction[1] * length\n",
    "        \n",
    "        pygame.draw.line(self.screen, color, \n",
    "                        (int(start_pos[0]), int(start_pos[1])), \n",
    "                        (int(end_x), int(end_y)), 2)\n",
    "        \n",
    "        angle = math.atan2(direction[1], direction[0])\n",
    "        arrow_size = 10\n",
    "        arrow1_x = end_x - arrow_size * math.cos(angle + math.pi/6)\n",
    "        arrow1_y = end_y - arrow_size * math.sin(angle + math.pi/6)\n",
    "        arrow2_x = end_x - arrow_size * math.cos(angle - math.pi/6)\n",
    "        arrow2_y = end_y - arrow_size * math.sin(angle - math.pi/6)\n",
    "        \n",
    "        pygame.draw.line(self.screen, color, \n",
    "                        (int(end_x), int(end_y)), \n",
    "                        (int(arrow1_x), int(arrow1_y)), 2)\n",
    "        pygame.draw.line(self.screen, color, \n",
    "                        (int(end_x), int(end_y)), \n",
    "                        (int(arrow2_x), int(arrow2_y)), 2)\n",
    "\n",
    "    def render(self, human_input, perfect_dir, combined_dir, gamma):\n",
    "        if self.render_mode != \"human\":\n",
    "            return\n",
    "            \n",
    "        current_time = time.time()\n",
    "        if current_time - self.last_render_time < 1/60:\n",
    "            return\n",
    "        self.last_render_time = current_time\n",
    "        \n",
    "        self.screen.fill(WHITE)\n",
    "        \n",
    "        # Draw target\n",
    "        pygame.draw.circle(self.screen, YELLOW, \n",
    "                         (int(self.target_pos[0]), int(self.target_pos[1])), \n",
    "                         TARGET_RADIUS)\n",
    "        \n",
    "        # Draw dot\n",
    "        pygame.draw.circle(self.screen, BLACK, \n",
    "                         (int(self.dot_pos[0]), int(self.dot_pos[1])), \n",
    "                         DOT_RADIUS, 2)\n",
    "        \n",
    "        # Draw arrows\n",
    "        if np.any(human_input):\n",
    "            self.draw_arrow((self.dot_pos[0], self.dot_pos[1]), human_input, BLUE)\n",
    "        if np.any(perfect_dir):\n",
    "            self.draw_arrow((self.dot_pos[0], self.dot_pos[1]), perfect_dir, GREEN)\n",
    "        if np.any(combined_dir):\n",
    "            self.draw_arrow((self.dot_pos[0], self.dot_pos[1]), combined_dir, RED)\n",
    "        \n",
    "        # Draw info text\n",
    "        texts = [\n",
    "            f\"Episode: {self.episode_count}\",\n",
    "            f\"Step: {self.step_count}\",\n",
    "            f\"Gamma: {gamma:.2f}\",\n",
    "            f\"Reward: {self.episode_reward:.1f}\",\n",
    "            f\"Curriculum Level: {self.curriculum_level}\"\n",
    "        ]\n",
    "        \n",
    "        for i, text in enumerate(texts):\n",
    "            text_surface = font.render(text, True, BLACK)\n",
    "            self.screen.blit(text_surface, (10, 10 + i*25))\n",
    "        \n",
    "        pygame.display.flip()\n",
    "\n",
    "    def set_curriculum_level(self, level):\n",
    "        self.curriculum_level = level\n",
    "\n",
    "    def _generate_target(self):\n",
    "        curr_params = CURRICULUM_LEVELS[self.curriculum_level]\n",
    "        if curr_params[\"radius\"] is None:\n",
    "            x = np.random.uniform(100, FULL_VIEW_SIZE[0]-100)\n",
    "            y = np.random.uniform(100, FULL_VIEW_SIZE[1]-100)\n",
    "        else:\n",
    "            angle = np.random.uniform(0, 2*np.pi)\n",
    "            r = np.random.uniform(0, curr_params[\"radius\"])\n",
    "            x = START_POS[0] + r * np.cos(angle)\n",
    "            y = START_POS[1] + r * np.sin(angle)\n",
    "        return [np.clip(x, 100, FULL_VIEW_SIZE[0]-100),\n",
    "                np.clip(y, 100, FULL_VIEW_SIZE[1]-100)]\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.dot_pos = np.array(START_POS, dtype=np.float32)\n",
    "        self.dot_vel = np.array([0.0, 0.0], dtype=np.float32)\n",
    "        self.target_pos = np.array(self._generate_target(), dtype=np.float32)\n",
    "        self.step_count = 0\n",
    "        self.episode_reward = 0\n",
    "        self.episode_count += 1\n",
    "        \n",
    "        return self._get_obs(), {}\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        to_target = self.target_pos - self.dot_pos\n",
    "        dist = np.linalg.norm(to_target)\n",
    "        perfect_dir = to_target / dist if dist > 0 else np.array([0, 0])\n",
    "        \n",
    "        curr_noise = CURRICULUM_LEVELS[self.curriculum_level][\"noise\"]\n",
    "        noise = np.random.normal(0, curr_noise, size=2)\n",
    "        human_input = perfect_dir + noise\n",
    "        human_input = human_input / np.linalg.norm(human_input) if np.linalg.norm(human_input) > 0 else np.array([0, 0])\n",
    "        \n",
    "        return np.concatenate([\n",
    "            self.dot_pos,\n",
    "            self.dot_vel,\n",
    "            human_input,\n",
    "            self.target_pos,\n",
    "            perfect_dir\n",
    "        ])\n",
    "\n",
    "    def step(self, action):\n",
    "        self.step_count += 1\n",
    "        gamma = float(action[0])\n",
    "        \n",
    "        obs = self._get_obs()\n",
    "        human_input = obs[4:6]\n",
    "        perfect_dir = obs[8:10]\n",
    "        \n",
    "        if self.record_demonstrations:\n",
    "            self.current_episode_states.append({\n",
    "                'state': obs.tolist(),\n",
    "                'action': gamma,\n",
    "                'human_input': human_input.tolist(),\n",
    "                'perfect_dir': perfect_dir.tolist()\n",
    "            })\n",
    "        \n",
    "        combined_dir = gamma * perfect_dir + (1 - gamma) * human_input\n",
    "        if np.linalg.norm(combined_dir) > 0:\n",
    "            combined_dir = combined_dir / np.linalg.norm(combined_dir)\n",
    "        \n",
    "        self.dot_vel = combined_dir * MAX_SPEED\n",
    "        new_pos = self.dot_pos + self.dot_vel\n",
    "        self.dot_pos = np.clip(new_pos, [0, 0], [FULL_VIEW_SIZE[0], FULL_VIEW_SIZE[1]])\n",
    "        \n",
    "        dist_to_target = np.linalg.norm(self.target_pos - self.dot_pos)\n",
    "        progress_reward = -dist_to_target / np.sqrt(FULL_VIEW_SIZE[0]**2 + FULL_VIEW_SIZE[1]**2)\n",
    "        gamma_penalty = -0.1 * (abs(gamma - 0.5) ** 2)\n",
    "        \n",
    "        if hasattr(self, 'last_gamma'):\n",
    "            smoothness_penalty = -0.1 * abs(gamma - self.last_gamma)\n",
    "        else:\n",
    "            smoothness_penalty = 0\n",
    "        self.last_gamma = gamma\n",
    "        \n",
    "        entropy_bonus = -0.01 * (gamma * np.log(gamma + 1e-10) + (1-gamma) * np.log(1-gamma + 1e-10))\n",
    "        \n",
    "        reward = progress_reward + gamma_penalty + smoothness_penalty + entropy_bonus\n",
    "        self.episode_reward += reward\n",
    "        \n",
    "        done = False\n",
    "        self.episode_success = False\n",
    "        if dist_to_target < GOAL_DETECTION_RADIUS:\n",
    "            done = True\n",
    "            self.episode_success = True\n",
    "            reward += 10.0\n",
    "        elif self.step_count >= self.max_steps:\n",
    "            done = True\n",
    "            reward -= 5.0\n",
    "            \n",
    "        self.episode_done = done\n",
    "        \n",
    "        # Save demonstration if episode is complete\n",
    "        if done and self.record_demonstrations:\n",
    "            self.demonstrations.append({\n",
    "                'states': self.current_episode_states,\n",
    "                'success': self.episode_success,\n",
    "                'total_reward': self.episode_reward\n",
    "            })\n",
    "            self.current_episode_states = []\n",
    "        \n",
    "        self.render(human_input, perfect_dir, combined_dir, gamma)\n",
    "        return self._get_obs(), reward, done, False, {}\n",
    "\n",
    "    def save_demonstrations(self, filename):\n",
    "        if self.demonstrations:\n",
    "            with open(filename, 'w') as f:\n",
    "                json.dump(self.demonstrations, f)\n",
    "            print(f\"Saved {len(self.demonstrations)} demonstrations to {filename}\")\n",
    "\n",
    "def train_ppo_with_viz(demonstration_file=None):\n",
    "    # Create environment with visualization\n",
    "    env = DynamicArbitrationEnv(render_mode=\"human\", record_demonstrations=True)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    \n",
    "    # Policy kwargs with custom feature extractor\n",
    "    policy_kwargs = dict(\n",
    "        features_extractor_class=CustomFeaturesExtractor,\n",
    "        features_extractor_kwargs=dict(features_dim=64),\n",
    "        net_arch=[dict(pi=[64, 64], vf=[64, 64])]  # Separate networks for policy and value function\n",
    "    )\n",
    "    \n",
    "    # Initialize PPO with custom settings\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        learning_rate=3e-4,\n",
    "        n_steps=2048,\n",
    "        batch_size=64,\n",
    "        n_epochs=10,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        clip_range=0.2,\n",
    "        policy_kwargs=policy_kwargs,\n",
    "        device='cpu',  # Force CPU usage\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Add curriculum learning callback\n",
    "    curriculum_callback = CurriculumCallback(env.envs[0])\n",
    "    \n",
    "    try:\n",
    "        # Train the model\n",
    "        model.learn(\n",
    "            total_timesteps=1_000_000,\n",
    "            callback=curriculum_callback\n",
    "        )\n",
    "        \n",
    "        # Save the model and demonstrations\n",
    "        model.save(\"dynamic_arbitration_ppo\")\n",
    "        env.envs[0].save_demonstrations(\"training_demonstrations.json\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTraining interrupted. Saving current model...\")\n",
    "        model.save(\"dynamic_arbitration_ppo_interrupted\")\n",
    "        env.envs[0].save_demonstrations(\"training_demonstrations_interrupted.json\")\n",
    "    finally:\n",
    "        pygame.quit()\n",
    "\n",
    "def load_and_run_model():\n",
    "    \"\"\"Function to load and run a trained model\"\"\"\n",
    "    env = DynamicArbitrationEnv(render_mode=\"human\")\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    \n",
    "    try:\n",
    "        model = PPO.load(\"dynamic_arbitration_ppo\")\n",
    "        \n",
    "        obs, _ = env.reset()\n",
    "        while True:\n",
    "            action, _states = model.predict(obs)\n",
    "            obs, rewards, dones, _, infos = env.step(action)\n",
    "            \n",
    "            if dones:\n",
    "                obs, _ = env.reset()\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nVisualization stopped by user\")\n",
    "    finally:\n",
    "        pygame.quit()\n",
    "\n",
    "def analyze_model_performance():\n",
    "    \"\"\"Function to analyze model performance\"\"\"\n",
    "    env = DynamicArbitrationEnv(render_mode=None)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    model = PPO.load(\"dynamic_arbitration_ppo\")\n",
    "    \n",
    "    n_episodes = 100\n",
    "    rewards = []\n",
    "    success_rate = 0\n",
    "    gamma_values = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_gammas = []\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action, _states = model.predict(obs)\n",
    "            obs, reward, done, _, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "            episode_gammas.append(action[0])\n",
    "        \n",
    "        rewards.append(episode_reward)\n",
    "        gamma_values.extend(episode_gammas)\n",
    "        if episode_reward > 0:  # Consider it successful if reward is positive\n",
    "            success_rate += 1\n",
    "    \n",
    "    print(f\"Average reward: {np.mean(rewards):.2f}\")\n",
    "    print(f\"Success rate: {success_rate/n_episodes*100:.2f}%\")\n",
    "    print(f\"Average gamma: {np.mean(gamma_values):.2f}\")\n",
    "    print(f\"Gamma std: {np.std(gamma_values):.2f}\")\n",
    "\n",
    "def main(mode='train'):\n",
    "    \"\"\"Main function to run the program in different modes\"\"\"\n",
    "    try:\n",
    "        if mode == 'train':\n",
    "            train_ppo_with_viz()\n",
    "        elif mode == 'run':\n",
    "            load_and_run_model()\n",
    "        elif mode == 'analyze':\n",
    "            analyze_model_performance()\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    finally:\n",
    "        pygame.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_ppo_with_viz()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
