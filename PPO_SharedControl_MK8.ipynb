{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Logging to ./ppo_tensorboard/PPO_41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tnlab\\AppData\\Roaming\\Python\\Python312\\site-packages\\stable_baselines3\\common\\policies.py:486: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])\n",
      "  warnings.warn(\n",
      "C:\\Users\\tnlab\\AppData\\Local\\Temp\\ipykernel_18624\\2604965491.py:494: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  raw_a = float(action)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 759  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 1024 |\n",
      "-----------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 760       |\n",
      "|    iterations           | 2         |\n",
      "|    time_elapsed         | 2         |\n",
      "|    total_timesteps      | 2048      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 3.2384377 |\n",
      "|    clip_fraction        | 0.756     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.42     |\n",
      "|    explained_variance   | 0.861     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 241       |\n",
      "|    n_updates            | 4         |\n",
      "|    policy_gradient_loss | 0.22      |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 702       |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 772        |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 3072       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.43211567 |\n",
      "|    clip_fraction        | 0.56       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.42      |\n",
      "|    explained_variance   | 0.95       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 109        |\n",
      "|    n_updates            | 8          |\n",
      "|    policy_gradient_loss | 0.0579     |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 534        |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 779       |\n",
      "|    iterations           | 4         |\n",
      "|    time_elapsed         | 5         |\n",
      "|    total_timesteps      | 4096      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 18.025255 |\n",
      "|    clip_fraction        | 0.851     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.42     |\n",
      "|    explained_variance   | 0.793     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 245       |\n",
      "|    n_updates            | 12        |\n",
      "|    policy_gradient_loss | 0.0345    |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 445       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 779       |\n",
      "|    iterations           | 5         |\n",
      "|    time_elapsed         | 6         |\n",
      "|    total_timesteps      | 5120      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 20.192886 |\n",
      "|    clip_fraction        | 0.851     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.42     |\n",
      "|    explained_variance   | 0.967     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 58.1      |\n",
      "|    n_updates            | 16        |\n",
      "|    policy_gradient_loss | 0.0872    |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 135       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 784       |\n",
      "|    iterations           | 6         |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 6144      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 29.243984 |\n",
      "|    clip_fraction        | 0.823     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.42     |\n",
      "|    explained_variance   | 0.886     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 64.3      |\n",
      "|    n_updates            | 20        |\n",
      "|    policy_gradient_loss | -0.0101   |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 128       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 779       |\n",
      "|    iterations           | 7         |\n",
      "|    time_elapsed         | 9         |\n",
      "|    total_timesteps      | 7168      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 33.661327 |\n",
      "|    clip_fraction        | 0.908     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.42     |\n",
      "|    explained_variance   | 0.931     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 30        |\n",
      "|    n_updates            | 24        |\n",
      "|    policy_gradient_loss | 0.0596    |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 61.8      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 779       |\n",
      "|    iterations           | 8         |\n",
      "|    time_elapsed         | 10        |\n",
      "|    total_timesteps      | 8192      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 18.849806 |\n",
      "|    clip_fraction        | 0.637     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.42     |\n",
      "|    explained_variance   | 0.889     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 24.4      |\n",
      "|    n_updates            | 28        |\n",
      "|    policy_gradient_loss | -0.0673   |\n",
      "|    std                  | 0.999     |\n",
      "|    value_loss           | 62.7      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 780       |\n",
      "|    iterations           | 9         |\n",
      "|    time_elapsed         | 11        |\n",
      "|    total_timesteps      | 9216      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 12.570861 |\n",
      "|    clip_fraction        | 0.506     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.42     |\n",
      "|    explained_variance   | 0.954     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 33.7      |\n",
      "|    n_updates            | 32        |\n",
      "|    policy_gradient_loss | -0.058    |\n",
      "|    std                  | 0.998     |\n",
      "|    value_loss           | 69.4      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 10        |\n",
      "|    time_elapsed         | 13        |\n",
      "|    total_timesteps      | 10240     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 10.684946 |\n",
      "|    clip_fraction        | 0.624     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.42     |\n",
      "|    explained_variance   | 0.894     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 51.4      |\n",
      "|    n_updates            | 36        |\n",
      "|    policy_gradient_loss | -0.0597   |\n",
      "|    std                  | 0.997     |\n",
      "|    value_loss           | 123       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 778       |\n",
      "|    iterations           | 11        |\n",
      "|    time_elapsed         | 14        |\n",
      "|    total_timesteps      | 11264     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 10.493797 |\n",
      "|    clip_fraction        | 0.699     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.42     |\n",
      "|    explained_variance   | 0.945     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 36        |\n",
      "|    n_updates            | 40        |\n",
      "|    policy_gradient_loss | -0.039    |\n",
      "|    std                  | 0.996     |\n",
      "|    value_loss           | 83.7      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                   |          |\n",
      "|    fps                  | 779      |\n",
      "|    iterations           | 12       |\n",
      "|    time_elapsed         | 15       |\n",
      "|    total_timesteps      | 12288    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 7.447732 |\n",
      "|    clip_fraction        | 0.57     |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -1.41    |\n",
      "|    explained_variance   | 0.973    |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 36.1     |\n",
      "|    n_updates            | 44       |\n",
      "|    policy_gradient_loss | -0.0133  |\n",
      "|    std                  | 0.995    |\n",
      "|    value_loss           | 93.7     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                   |          |\n",
      "|    fps                  | 778      |\n",
      "|    iterations           | 13       |\n",
      "|    time_elapsed         | 17       |\n",
      "|    total_timesteps      | 13312    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 8.202104 |\n",
      "|    clip_fraction        | 0.747    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -1.41    |\n",
      "|    explained_variance   | 0.985    |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 27.7     |\n",
      "|    n_updates            | 48       |\n",
      "|    policy_gradient_loss | 0.0419   |\n",
      "|    std                  | 0.994    |\n",
      "|    value_loss           | 52.8     |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 779       |\n",
      "|    iterations           | 14        |\n",
      "|    time_elapsed         | 18        |\n",
      "|    total_timesteps      | 14336     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 10.102821 |\n",
      "|    clip_fraction        | 0.648     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.41     |\n",
      "|    explained_variance   | 0.989     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 24.3      |\n",
      "|    n_updates            | 52        |\n",
      "|    policy_gradient_loss | 0.0276    |\n",
      "|    std                  | 0.993     |\n",
      "|    value_loss           | 50.6      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                   |          |\n",
      "|    fps                  | 782      |\n",
      "|    iterations           | 15       |\n",
      "|    time_elapsed         | 19       |\n",
      "|    total_timesteps      | 15360    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 8.663657 |\n",
      "|    clip_fraction        | 0.584    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -1.41    |\n",
      "|    explained_variance   | 0.981    |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 43.2     |\n",
      "|    n_updates            | 56       |\n",
      "|    policy_gradient_loss | -0.0578  |\n",
      "|    std                  | 0.992    |\n",
      "|    value_loss           | 96.2     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                   |          |\n",
      "|    fps                  | 782      |\n",
      "|    iterations           | 16       |\n",
      "|    time_elapsed         | 20       |\n",
      "|    total_timesteps      | 16384    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 6.306292 |\n",
      "|    clip_fraction        | 0.567    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -1.41    |\n",
      "|    explained_variance   | 0.986    |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 46.9     |\n",
      "|    n_updates            | 60       |\n",
      "|    policy_gradient_loss | -0.00615 |\n",
      "|    std                  | 0.991    |\n",
      "|    value_loss           | 89.4     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                   |          |\n",
      "|    fps                  | 781      |\n",
      "|    iterations           | 17       |\n",
      "|    time_elapsed         | 22       |\n",
      "|    total_timesteps      | 17408    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 14.49204 |\n",
      "|    clip_fraction        | 0.775    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -1.41    |\n",
      "|    explained_variance   | 0.992    |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 22.7     |\n",
      "|    n_updates            | 64       |\n",
      "|    policy_gradient_loss | 0.165    |\n",
      "|    std                  | 0.99     |\n",
      "|    value_loss           | 48.7     |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 783       |\n",
      "|    iterations           | 18        |\n",
      "|    time_elapsed         | 23        |\n",
      "|    total_timesteps      | 18432     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 17.652527 |\n",
      "|    clip_fraction        | 0.802     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.41     |\n",
      "|    explained_variance   | 0.992     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 19.4      |\n",
      "|    n_updates            | 68        |\n",
      "|    policy_gradient_loss | 0.185     |\n",
      "|    std                  | 0.99      |\n",
      "|    value_loss           | 44.5      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                   |          |\n",
      "|    fps                  | 784      |\n",
      "|    iterations           | 19       |\n",
      "|    time_elapsed         | 24       |\n",
      "|    total_timesteps      | 19456    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 8.859436 |\n",
      "|    clip_fraction        | 0.795    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -1.41    |\n",
      "|    explained_variance   | 0.993    |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 20.6     |\n",
      "|    n_updates            | 72       |\n",
      "|    policy_gradient_loss | 0.0114   |\n",
      "|    std                  | 0.989    |\n",
      "|    value_loss           | 44.6     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                   |          |\n",
      "|    fps                  | 784      |\n",
      "|    iterations           | 20       |\n",
      "|    time_elapsed         | 26       |\n",
      "|    total_timesteps      | 20480    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 93.76756 |\n",
      "|    clip_fraction        | 0.99     |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -1.41    |\n",
      "|    explained_variance   | 0.943    |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 29       |\n",
      "|    n_updates            | 76       |\n",
      "|    policy_gradient_loss | 0.24     |\n",
      "|    std                  | 0.989    |\n",
      "|    value_loss           | 48.9     |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 783       |\n",
      "|    iterations           | 21        |\n",
      "|    time_elapsed         | 27        |\n",
      "|    total_timesteps      | 21504     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 139.14368 |\n",
      "|    clip_fraction        | 1         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.41     |\n",
      "|    explained_variance   | 0.967     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 28.6      |\n",
      "|    n_updates            | 80        |\n",
      "|    policy_gradient_loss | 0.308     |\n",
      "|    std                  | 0.989     |\n",
      "|    value_loss           | 116       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 784       |\n",
      "|    iterations           | 22        |\n",
      "|    time_elapsed         | 28        |\n",
      "|    total_timesteps      | 22528     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 183.57224 |\n",
      "|    clip_fraction        | 1         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.41     |\n",
      "|    explained_variance   | 0.962     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 36.1      |\n",
      "|    n_updates            | 84        |\n",
      "|    policy_gradient_loss | 0.291     |\n",
      "|    std                  | 0.988     |\n",
      "|    value_loss           | 75.5      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 783       |\n",
      "|    iterations           | 23        |\n",
      "|    time_elapsed         | 30        |\n",
      "|    total_timesteps      | 23552     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 211.81375 |\n",
      "|    clip_fraction        | 1         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.41     |\n",
      "|    explained_variance   | 0.979     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 47.1      |\n",
      "|    n_updates            | 88        |\n",
      "|    policy_gradient_loss | 0.303     |\n",
      "|    std                  | 0.988     |\n",
      "|    value_loss           | 72.6      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 784       |\n",
      "|    iterations           | 24        |\n",
      "|    time_elapsed         | 31        |\n",
      "|    total_timesteps      | 24576     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 235.93945 |\n",
      "|    clip_fraction        | 1         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.41     |\n",
      "|    explained_variance   | 0.986     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 29.6      |\n",
      "|    n_updates            | 92        |\n",
      "|    policy_gradient_loss | 0.281     |\n",
      "|    std                  | 0.988     |\n",
      "|    value_loss           | 41.8      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 781       |\n",
      "|    iterations           | 25        |\n",
      "|    time_elapsed         | 32        |\n",
      "|    total_timesteps      | 25600     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 305.87878 |\n",
      "|    clip_fraction        | 1         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.41     |\n",
      "|    explained_variance   | 0.966     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 17.2      |\n",
      "|    n_updates            | 96        |\n",
      "|    policy_gradient_loss | 0.282     |\n",
      "|    std                  | 0.988     |\n",
      "|    value_loss           | 40.7      |\n",
      "---------------------------------------\n",
      "Model saved to trained_models\\gamma_ppo_model.zip\n",
      "Metrics saved to 'training_metrics/'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# For rendering (optional):\n",
    "try:\n",
    "    import pygame\n",
    "except ImportError:\n",
    "    pygame = None\n",
    "\n",
    "###############################################################################\n",
    "# CUSTOM POLICY: GammaMlpPolicy\n",
    "###############################################################################\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "\n",
    "class GammaMlpPolicy(ActorCriticPolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(GammaMlpPolicy, self).__init__(*args, **kwargs)\n",
    "    \n",
    "    def forward(self, obs, deterministic=False):\n",
    "        # Use the built-in feature extractor\n",
    "        latent = self.extract_features(obs)\n",
    "        latent_pi, latent_vf = self.mlp_extractor(latent)\n",
    "        raw_mean = self.action_net(latent_pi)\n",
    "        # Squash the mean so that it lies in [-1, 1]\n",
    "        mean_actions = torch.tanh(raw_mean)\n",
    "        log_std = torch.clamp(self.log_std, -20, 2)\n",
    "        std = torch.exp(log_std)\n",
    "        # Create a Normal distribution with the squashed mean and computed std\n",
    "        distribution = torch.distributions.Normal(mean_actions, std)\n",
    "        if deterministic:\n",
    "            actions = mean_actions\n",
    "        else:\n",
    "            actions = distribution.rsample()  # reparameterized sample\n",
    "            # Apply tanh to ensure the final actions are in [-1, 1]\n",
    "            actions = torch.tanh(actions)\n",
    "        # Compute log probability (note: no tanh correction term here)\n",
    "        log_prob = distribution.log_prob(actions).sum(dim=1, keepdim=True)\n",
    "        value = self.value_net(latent_vf)\n",
    "        return actions, value, log_prob\n",
    "\n",
    "###############################################################################\n",
    "# CONSTANTS & UTILS\n",
    "###############################################################################\n",
    "FULL_VIEW_SIZE = (1200, 800)\n",
    "SCALING_FACTOR_X = FULL_VIEW_SIZE[0] / 600.0\n",
    "SCALING_FACTOR_Y = FULL_VIEW_SIZE[1] / 600.0\n",
    "SCALING_FACTOR   = (SCALING_FACTOR_X + SCALING_FACTOR_Y) / 2\n",
    "\n",
    "DOT_RADIUS       = int(15 * SCALING_FACTOR)\n",
    "TARGET_RADIUS    = int(10 * SCALING_FACTOR)\n",
    "OBSTACLE_RADIUS  = int(10 * SCALING_FACTOR)\n",
    "COLLISION_BUFFER = int(5  * SCALING_FACTOR)\n",
    "MAX_SPEED        = 3 * SCALING_FACTOR\n",
    "NOISE_MAGNITUDE  = 0.5\n",
    "RENDER_FPS       = 30\n",
    "\n",
    "START_POS = np.array([FULL_VIEW_SIZE[0]//2, FULL_VIEW_SIZE[1]//2], dtype=np.float32)\n",
    "\n",
    "WHITE = (255, 255, 255)\n",
    "GRAY  = (128, 128, 128)\n",
    "YELLOW= (255, 255, 0)\n",
    "BLACK = (0, 0, 0)\n",
    "\n",
    "def distance(a, b):\n",
    "    return math.hypot(a[0] - b[0], a[1] - b[1])\n",
    "\n",
    "def check_line_collision(start, end, center, radius):\n",
    "    dx = end[0] - start[0]\n",
    "    dy = end[1] - start[1]\n",
    "    fx = center[0] - start[0]\n",
    "    fy = center[1] - start[1]\n",
    "    l2 = dx*dx + dy*dy\n",
    "    if l2 < 1e-9:\n",
    "        return distance(start, center) <= radius\n",
    "    t = max(0, min(1, (fx*dx + fy*dy) / l2))\n",
    "    px = start[0] + t*dx\n",
    "    py = start[1] + t*dy\n",
    "    return distance((px, py), center) <= radius\n",
    "\n",
    "def line_collision(pos, new_pos, obstacles):\n",
    "    for obs in obstacles:\n",
    "        if check_line_collision(pos, new_pos, obs, OBSTACLE_RADIUS + COLLISION_BUFFER):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def inside_obstacle(pos, obstacles):\n",
    "    for obs in obstacles:\n",
    "        if distance(pos, obs) <= (OBSTACLE_RADIUS + DOT_RADIUS):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def potential_field_dir(pos, goal, obstacles):\n",
    "    \"\"\"\n",
    "    Returns a normalized direction from pos to goal,\n",
    "    plus repulsion from obstacles.\n",
    "    \"\"\"\n",
    "    gx = goal[0] - pos[0]\n",
    "    gy = goal[1] - pos[1]\n",
    "    dg = math.hypot(gx, gy)\n",
    "    if dg < 1e-6:\n",
    "        return np.zeros(2, dtype=np.float32)\n",
    "    att = np.array([gx / dg, gy / dg], dtype=np.float32)\n",
    "\n",
    "    repulse_x = 0.0\n",
    "    repulse_y = 0.0\n",
    "    repulsion_radius = 23.0 * SCALING_FACTOR\n",
    "    repulsion_gain   = 30000.0\n",
    "\n",
    "    for obs in obstacles:\n",
    "        dx = pos[0] - obs[0]\n",
    "        dy = pos[1] - obs[1]\n",
    "        dobs = math.hypot(dx, dy)\n",
    "        if dobs < 1e-9:\n",
    "            continue\n",
    "        if dobs < repulsion_radius:\n",
    "            pushx    = dx / dobs\n",
    "            pushy    = dy / dobs\n",
    "            strength = repulsion_gain / (dobs**2)\n",
    "            repulse_x += pushx * strength\n",
    "            repulse_y += pushy * strength\n",
    "\n",
    "    px = att[0] + repulse_x\n",
    "    py = att[1] + repulse_y\n",
    "    mg = math.hypot(px, py)\n",
    "    if mg < 1e-9:\n",
    "        return np.zeros(2, dtype=np.float32)\n",
    "    return np.array([px / mg, py / mg], dtype=np.float32)\n",
    "\n",
    "###############################################################################\n",
    "# METRICS CALLBACK\n",
    "###############################################################################\n",
    "class MetricsCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Logs training metrics and saves various plots after training.\n",
    "    Plots include:\n",
    "      - Episode Reward\n",
    "      - Average Gamma per Episode\n",
    "      - Gamma Std per Episode\n",
    "      - Total Model Loss\n",
    "      - Critic (Value) Loss\n",
    "      - Actor (Policy) Loss\n",
    "      - Entropy Loss\n",
    "      - Episode Length\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.episode_rewards   = []\n",
    "        self.episode_lengths   = []\n",
    "        self.episode_mean_gammas = []\n",
    "        self.episode_std_gammas  = []\n",
    "        self.total_reward = 0.0\n",
    "        self.ep_length    = 0\n",
    "        self.current_episode_gammas = []  # store gamma each step\n",
    "        self.n_collisions = 0\n",
    "        self.n_episodes   = 0\n",
    "        # Loss tracking:\n",
    "        self.losses         = []\n",
    "        self.value_losses   = []\n",
    "        self.policy_losses  = []\n",
    "        self.entropy_losses = []\n",
    "        self.training_steps = []\n",
    "        self.n_updates      = 0\n",
    "\n",
    "    def _on_training_start(self):\n",
    "        self.episode_rewards.clear()\n",
    "        self.episode_lengths.clear()\n",
    "        self.episode_mean_gammas.clear()\n",
    "        self.episode_std_gammas.clear()\n",
    "        self.total_reward = 0.0\n",
    "        self.ep_length    = 0\n",
    "        self.n_collisions = 0\n",
    "        self.n_episodes   = 0\n",
    "        self.current_episode_gammas.clear()\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        actions = self.locals['actions']\n",
    "        rewards = self.locals['rewards']\n",
    "        dones   = self.locals['dones']\n",
    "        infos   = self.locals['infos']\n",
    "        \n",
    "        # Compute gamma from the action (mapping [-1,1] -> [0,1])\n",
    "        gamma_val = 0.5 * (actions[0] + 1.0)\n",
    "        self.current_episode_gammas.append(gamma_val)\n",
    "        r = float(rewards[0])\n",
    "        self.total_reward += r\n",
    "        self.ep_length    += 1\n",
    "\n",
    "        if dones[0]:\n",
    "            self.episode_rewards.append(self.total_reward)\n",
    "            self.episode_lengths.append(self.ep_length)\n",
    "            mean_gamma = np.mean(self.current_episode_gammas)\n",
    "            std_gamma  = np.std(self.current_episode_gammas)\n",
    "            self.episode_mean_gammas.append(mean_gamma)\n",
    "            self.episode_std_gammas.append(std_gamma)\n",
    "            self.total_reward = 0.0\n",
    "            self.ep_length    = 0\n",
    "            self.current_episode_gammas.clear()\n",
    "            self.n_episodes  += 1\n",
    "            if 'terminal_reason' in infos[0] and infos[0]['terminal_reason'] == 'collision':\n",
    "                self.n_collisions += 1\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        self.n_updates += 1\n",
    "        logs = self.model.logger.name_to_value or {}\n",
    "        if \"train/loss\" in logs:\n",
    "            self.losses.append(logs[\"train/loss\"])\n",
    "            self.training_steps.append(self.n_updates)\n",
    "        if \"train/value_loss\" in logs:\n",
    "            self.value_losses.append(logs[\"train/value_loss\"])\n",
    "        if \"train/policy_gradient_loss\" in logs:\n",
    "            self.policy_losses.append(logs[\"train/policy_gradient_loss\"])\n",
    "        if \"train/entropy_loss\" in logs:\n",
    "            self.entropy_losses.append(logs[\"train/entropy_loss\"])\n",
    "\n",
    "    def _moving_average(self, data, window=10):\n",
    "        if len(data) < window:\n",
    "            return np.array(data)\n",
    "        return np.convolve(data, np.ones(window)/window, mode='valid')\n",
    "\n",
    "    def save_metrics(self, save_dir=\"training_metrics\"):\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        # 1) Episode Rewards\n",
    "        if self.episode_rewards:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(self.episode_rewards, label=\"Episode Reward\", alpha=0.6)\n",
    "            ma_rewards = self._moving_average(self.episode_rewards, 10)\n",
    "            if len(ma_rewards):\n",
    "                plt.plot(range(10 - 1, 10 - 1 + len(ma_rewards)), \n",
    "                         ma_rewards, label=\"MA(10)\", color='red', linewidth=2)\n",
    "            plt.xlabel(\"Episode\")\n",
    "            plt.ylabel(\"Reward\")\n",
    "            plt.title(\"Episode Rewards\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.savefig(os.path.join(save_dir, \"episode_rewards.png\"))\n",
    "            plt.close()\n",
    "        # 2) Average Gamma per Episode\n",
    "        if self.episode_mean_gammas:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(self.episode_mean_gammas, label=\"Average Gamma\", alpha=0.6)\n",
    "            ma_gamma = self._moving_average(self.episode_mean_gammas, 10)\n",
    "            if len(ma_gamma):\n",
    "                plt.plot(range(10 - 1, 10 - 1 + len(ma_gamma)), \n",
    "                         ma_gamma, label=\"MA(10)\", color='red', linewidth=2)\n",
    "            plt.xlabel(\"Episode\")\n",
    "            plt.ylabel(\"Gamma (avg)\")\n",
    "            plt.title(\"Average Gamma per Episode\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.savefig(os.path.join(save_dir, \"average_gamma.png\"))\n",
    "            plt.close()\n",
    "        # 3) Gamma Std per Episode\n",
    "        if self.episode_std_gammas:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(self.episode_std_gammas, label=\"Gamma Std\", alpha=0.6)\n",
    "            ma_gstd = self._moving_average(self.episode_std_gammas, 10)\n",
    "            if len(ma_gstd):\n",
    "                plt.plot(range(10 - 1, 10 - 1 + len(ma_gstd)), \n",
    "                         ma_gstd, label=\"MA(10)\", color='red', linewidth=2)\n",
    "            plt.xlabel(\"Episode\")\n",
    "            plt.ylabel(\"Gamma Std\")\n",
    "            plt.title(\"Gamma Std per Episode\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.savefig(os.path.join(save_dir, \"gamma_std.png\"))\n",
    "            plt.close()\n",
    "        # 4) Total Model Loss\n",
    "        if self.losses:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(self.training_steps, self.losses, label=\"Total Model Loss\", alpha=0.7)\n",
    "            if len(self.losses) >= 10:\n",
    "                ma_loss = self._moving_average(self.losses, 10)\n",
    "                plt.plot(range(self.training_steps[0] + (10 - 1),\n",
    "                               self.training_steps[0] + (10 - 1) + len(ma_loss)),\n",
    "                         ma_loss, label=\"MA(10)\", color='red', linewidth=2)\n",
    "            plt.xlabel(\"Training Updates\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.title(\"Total Model Loss Over Rollouts\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.savefig(os.path.join(save_dir, \"total_loss.png\"))\n",
    "            plt.close()\n",
    "        # 5) Critic (Value) Loss\n",
    "        if self.value_losses:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(self.value_losses, label=\"Value Loss\", alpha=0.7)\n",
    "            if len(self.value_losses) >= 10:\n",
    "                ma_val_loss = self._moving_average(self.value_losses, 10)\n",
    "                plt.plot(range(10 - 1, 10 - 1 + len(ma_val_loss)), \n",
    "                         ma_val_loss, label=\"MA(10)\", color='red', linewidth=2)\n",
    "            plt.xlabel(\"Rollout End #\")\n",
    "            plt.ylabel(\"Value Loss\")\n",
    "            plt.title(\"Value (Critic) Loss\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.savefig(os.path.join(save_dir, \"value_loss.png\"))\n",
    "            plt.close()\n",
    "        # 6) Actor (Policy) Loss\n",
    "        if self.policy_losses:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(self.policy_losses, label=\"Policy Loss\", alpha=0.7)\n",
    "            if len(self.policy_losses) >= 10:\n",
    "                ma_pol_loss = self._moving_average(self.policy_losses, 10)\n",
    "                plt.plot(range(10 - 1, 10 - 1 + len(ma_pol_loss)), \n",
    "                         ma_pol_loss, label=\"MA(10)\", color='red', linewidth=2)\n",
    "            plt.xlabel(\"Rollout End #\")\n",
    "            plt.ylabel(\"Policy Loss\")\n",
    "            plt.title(\"Policy (Actor) Loss\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.savefig(os.path.join(save_dir, \"policy_loss.png\"))\n",
    "            plt.close()\n",
    "        # 7) Entropy Loss\n",
    "        if self.entropy_losses:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(self.entropy_losses, label=\"Entropy Loss\", alpha=0.7)\n",
    "            if len(self.entropy_losses) >= 10:\n",
    "                ma_ent_loss = self._moving_average(self.entropy_losses, 10)\n",
    "                plt.plot(range(10 - 1, 10 - 1 + len(ma_ent_loss)), \n",
    "                         ma_ent_loss, label=\"MA(10)\", color='red', linewidth=2)\n",
    "            plt.xlabel(\"Rollout End #\")\n",
    "            plt.ylabel(\"Entropy Loss\")\n",
    "            plt.title(\"Entropy Loss (Exploration)\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.savefig(os.path.join(save_dir, \"entropy_loss.png\"))\n",
    "            plt.close()\n",
    "        # 8) Episode Length\n",
    "        if self.episode_lengths:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(self.episode_lengths, label=\"Episode Length\", alpha=0.6)\n",
    "            ma_length = self._moving_average(self.episode_lengths, 10)\n",
    "            if len(ma_length):\n",
    "                plt.plot(range(10 - 1, 10 - 1 + len(ma_length)), \n",
    "                         ma_length, label=\"MA(10)\", color='red', linewidth=2)\n",
    "            plt.xlabel(\"Episode\")\n",
    "            plt.ylabel(\"Length (# steps)\")\n",
    "            plt.title(\"Episode Length\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.savefig(os.path.join(save_dir, \"episode_length.png\"))\n",
    "            plt.close()\n",
    "        # Also save a summary text\n",
    "        with open(os.path.join(save_dir, \"summary.txt\"), \"w\") as f:\n",
    "            f.write(f\"Total Episodes: {len(self.episode_rewards)}\\n\")\n",
    "            if self.episode_rewards:\n",
    "                avg_reward = np.mean(self.episode_rewards)\n",
    "                f.write(f\"Mean Episode Reward: {avg_reward:.3f}\\n\")\n",
    "            f.write(f\"Collisions Count: {self.n_collisions}\\n\")\n",
    "            if self.episode_mean_gammas:\n",
    "                mean_gamma_all = np.mean(self.episode_mean_gammas)\n",
    "                f.write(f\"Mean of Average-Gamma: {mean_gamma_all:.3f}\\n\")\n",
    "\n",
    "###############################################################################\n",
    "# A SIMPLE RENDER CALLBACK (OPTIONAL) FOR LIVE VIEW\n",
    "###############################################################################\n",
    "class RenderCallback(BaseCallback):\n",
    "    def __init__(self, render_freq=1, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.render_freq = render_freq\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.render_freq == 0:\n",
    "            self.model.env.envs[0].render()\n",
    "        return True\n",
    "\n",
    "###############################################################################\n",
    "# DEMO ARBITRATION ENV\n",
    "###############################################################################\n",
    "class DemoArbitrationEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": RENDER_FPS}\n",
    "    def __init__(self, visualize=False):\n",
    "        super().__init__()\n",
    "        self.visualize = visualize\n",
    "        # Observation: [dot_x, dot_y, h_dir_x, h_dir_y, goal_x, goal_y, w_dir_x, w_dir_y, dist_ratio, obs_dist_ratio]\n",
    "        low  = np.array([0, 0, -1, -1, 0, 0, -1, -1, 0, 0], dtype=np.float32)\n",
    "        high = np.array([FULL_VIEW_SIZE[0], FULL_VIEW_SIZE[1], 1, 1,\n",
    "                         FULL_VIEW_SIZE[0], FULL_VIEW_SIZE[1], 1, 1, 1, 1], dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low=low, high=high, shape=(10,), dtype=np.float32)\n",
    "        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(), dtype=np.float32)\n",
    "        self.dot_pos = None\n",
    "        self.goal_pos = None\n",
    "        self.obstacles = []\n",
    "        self.goals = []\n",
    "        self.step_count = 0\n",
    "        self.max_steps = 300\n",
    "        self.episode_reward = 0.0\n",
    "        self.max_dist = math.hypot(FULL_VIEW_SIZE[0], FULL_VIEW_SIZE[1])\n",
    "        self.alpha = 3.0\n",
    "        self.beta  = 3.0\n",
    "        self.goal_threshold = 100.0\n",
    "        self.obs_threshold  = 100.0\n",
    "        self.SCENARIO_SEEDS = [0, 1, 2, 58, 487]\n",
    "        self.scenario_index = 0\n",
    "        self.episode_counter = 0\n",
    "        self.random_seed_probability = 0.3\n",
    "        if self.visualize and pygame is not None:\n",
    "            pygame.init()\n",
    "            self.window = pygame.display.set_mode(FULL_VIEW_SIZE)\n",
    "            pygame.display.set_caption(\"Demo Arbitration Environment\")\n",
    "            self.clock = pygame.time.Clock()\n",
    "        else:\n",
    "            self.window = None\n",
    "            self.clock = None\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.episode_counter += 1\n",
    "        use_random = (random.random() < self.random_seed_probability)\n",
    "        if use_random:\n",
    "            scenario_seed = random.randint(0, 9999999)\n",
    "        else:\n",
    "            scenario_seed = self.SCENARIO_SEEDS[self.scenario_index]\n",
    "            self.scenario_index = (self.scenario_index + 1) % len(self.SCENARIO_SEEDS)\n",
    "        self.randomize_env(scenario_seed)\n",
    "        self.step_count = 0\n",
    "        self.episode_reward = 0.0\n",
    "        self.dot_pos = START_POS.copy()\n",
    "        if self.goals:\n",
    "            idx = random.randint(0, len(self.goals) - 1)\n",
    "            self.goal_pos = self.goals[idx].copy()\n",
    "        else:\n",
    "            self.goal_pos = np.array([random.uniform(0.2*FULL_VIEW_SIZE[0], 0.8*FULL_VIEW_SIZE[0]),\n",
    "                                       random.uniform(0.2*FULL_VIEW_SIZE[1], 0.8*FULL_VIEW_SIZE[1])],\n",
    "                                      dtype=np.float32)\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def randomize_env(self, scenario_seed):\n",
    "        random.seed(scenario_seed)\n",
    "        np.random.seed(scenario_seed)\n",
    "        margin = 50 * SCALING_FACTOR\n",
    "        N_GOALS = 8\n",
    "        N_OBSTACLES = 5\n",
    "        min_goal_distance = 300 * SCALING_FACTOR\n",
    "        new_goals = []\n",
    "        attempts = 0\n",
    "        while len(new_goals) < N_GOALS and attempts < 2000:\n",
    "            x = random.uniform(margin, FULL_VIEW_SIZE[0] - margin)\n",
    "            y = random.uniform(margin, FULL_VIEW_SIZE[1] - margin)\n",
    "            candidate = np.array([x, y], dtype=np.float32)\n",
    "            if distance(candidate, START_POS) >= min_goal_distance:\n",
    "                new_goals.append(candidate)\n",
    "            attempts += 1\n",
    "        self.goals = new_goals[:N_GOALS]\n",
    "        new_obstacles = []\n",
    "        if len(self.goals) > 1:\n",
    "            obstacle_goals = random.sample(self.goals, k=min(min(N_GOALS-1, N_OBSTACLES), len(self.goals)-1))\n",
    "        else:\n",
    "            obstacle_goals = self.goals\n",
    "        for goal in obstacle_goals:\n",
    "            t = random.uniform(0.6, 0.8)\n",
    "            base_point = START_POS + t*(goal - START_POS)\n",
    "            vec = goal - START_POS\n",
    "            vec_norm = np.linalg.norm(vec)\n",
    "            if vec_norm < 1e-6:\n",
    "                perp = np.array([0, 0], dtype=np.float32)\n",
    "            else:\n",
    "                perp = np.array([-vec[1], vec[0]], dtype=np.float32)\n",
    "                perp /= np.linalg.norm(perp)\n",
    "            offset_mag = random.uniform(20*SCALING_FACTOR, 40*SCALING_FACTOR)\n",
    "            offset = perp * offset_mag * random.choice([-1, 1])\n",
    "            candidate = base_point + offset\n",
    "            candidate[0] = np.clip(candidate[0], margin, FULL_VIEW_SIZE[0] - margin)\n",
    "            candidate[1] = np.clip(candidate[1], margin, FULL_VIEW_SIZE[1] - margin)\n",
    "            valid = True\n",
    "            if distance(candidate, START_POS) < (DOT_RADIUS + OBSTACLE_RADIUS + 10):\n",
    "                valid = False\n",
    "            if distance(candidate, goal) < (TARGET_RADIUS + OBSTACLE_RADIUS + 20):\n",
    "                valid = False\n",
    "            for obs in new_obstacles:\n",
    "                if distance(candidate, obs) < (2*OBSTACLE_RADIUS + 10):\n",
    "                    valid = False\n",
    "            if valid:\n",
    "                new_obstacles.append(candidate)\n",
    "        self.obstacles = new_obstacles\n",
    "\n",
    "    def step(self, action):\n",
    "        raw_a = float(action)\n",
    "        gamma_val = 0.5 * (raw_a + 1.0)  # map [-1,1] -> [0,1]\n",
    "        self.step_count += 1\n",
    "        w_dir = potential_field_dir(self.dot_pos, self.goal_pos, self.obstacles)\n",
    "        noise = np.random.normal(0, NOISE_MAGNITUDE, size=2)\n",
    "        h_dir = w_dir + noise\n",
    "        hm = np.hypot(h_dir[0], h_dir[1])\n",
    "        if hm > 1e-6:\n",
    "            h_dir /= hm\n",
    "        c_dir = gamma_val * w_dir + (1 - gamma_val) * h_dir\n",
    "        cm = np.hypot(c_dir[0], c_dir[1])\n",
    "        if cm > 1e-6:\n",
    "            c_dir /= cm\n",
    "        move_vec = c_dir * MAX_SPEED\n",
    "        new_pos = self.dot_pos + move_vec\n",
    "        if not line_collision(self.dot_pos, new_pos, self.obstacles):\n",
    "            new_pos[0] = np.clip(new_pos[0], 0, FULL_VIEW_SIZE[0])\n",
    "            new_pos[1] = np.clip(new_pos[1], 0, FULL_VIEW_SIZE[1])\n",
    "            self.dot_pos = new_pos\n",
    "        collided = inside_obstacle(self.dot_pos, self.obstacles)\n",
    "        info = {}\n",
    "        if collided:\n",
    "            original_reward = -2.0\n",
    "            done = True\n",
    "            info[\"terminal_reason\"] = \"collision\"\n",
    "        else:\n",
    "            original_reward = 0.0\n",
    "            done = False\n",
    "            info[\"terminal_reason\"] = None\n",
    "        truncated = (self.step_count >= self.max_steps)\n",
    "        if truncated and not done:\n",
    "            info[\"terminal_reason\"] = \"timeout\"\n",
    "        d_goal = distance(self.dot_pos, self.goal_pos)\n",
    "        if self.obstacles:\n",
    "            d_obs = min(distance(self.dot_pos, obs) for obs in self.obstacles)\n",
    "        else:\n",
    "            d_obs = 999999.0\n",
    "        if (d_goal < self.goal_threshold) or (d_obs < self.obs_threshold):\n",
    "            shaping_reward = self.alpha * gamma_val\n",
    "        else:\n",
    "            shaping_reward = -self.beta * gamma_val\n",
    "        reward = original_reward + shaping_reward\n",
    "        self.episode_reward += reward\n",
    "        obs = self._get_obs()\n",
    "        return obs, float(reward), done, truncated, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        to_g = self.goal_pos - self.dot_pos\n",
    "        d = math.hypot(to_g[0], to_g[1])\n",
    "        dist_ratio = d / self.max_dist if self.max_dist > 1e-6 else 0.0\n",
    "        w_dir = potential_field_dir(self.dot_pos, self.goal_pos, self.obstacles)\n",
    "        noise = np.random.normal(0, NOISE_MAGNITUDE, size=2)\n",
    "        h_dir = w_dir + noise\n",
    "        hm = np.hypot(h_dir[0], h_dir[1])\n",
    "        if hm > 1e-6:\n",
    "            h_dir /= hm\n",
    "        if self.obstacles:\n",
    "            min_obs_distance = min(distance(self.dot_pos, obs) for obs in self.obstacles)\n",
    "        else:\n",
    "            min_obs_distance = self.max_dist\n",
    "        obs_dist_ratio = min_obs_distance / self.max_dist\n",
    "        obs = np.concatenate([self.dot_pos, h_dir, self.goal_pos, w_dir, [dist_ratio], [obs_dist_ratio]]).astype(np.float32)\n",
    "        return obs\n",
    "\n",
    "    def render(self):\n",
    "        if not self.visualize or (pygame is None):\n",
    "            return\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                return\n",
    "        self.window.fill(WHITE)\n",
    "        for obs in self.obstacles:\n",
    "            pygame.draw.circle(self.window, GRAY, (int(obs[0]), int(obs[1])), OBSTACLE_RADIUS)\n",
    "        for gpos in self.goals:\n",
    "            pygame.draw.circle(self.window, YELLOW, (int(gpos[0]), int(gpos[1])), TARGET_RADIUS)\n",
    "        pygame.draw.circle(self.window, BLACK, (int(self.goal_pos[0]), int(self.goal_pos[1])), TARGET_RADIUS+2, width=2)\n",
    "        pygame.draw.circle(self.window, BLACK, (int(self.dot_pos[0]), int(self.dot_pos[1])), DOT_RADIUS, width=2)\n",
    "        pygame.display.flip()\n",
    "        self.clock.tick(RENDER_FPS)\n",
    "\n",
    "    def close(self):\n",
    "        if self.visualize and pygame is not None:\n",
    "            pygame.quit()\n",
    "        super().close()\n",
    "\n",
    "###############################################################################\n",
    "# TRAINING FUNCTION\n",
    "###############################################################################\n",
    "def train_model(total_timesteps=500_000, visualize=False):\n",
    "    env = DemoArbitrationEnv(visualize=visualize)\n",
    "    env = DummyVecEnv([lambda: env])\n",
    "    metrics_callback = MetricsCallback()\n",
    "    callbacks = [metrics_callback]\n",
    "    if visualize:\n",
    "        render_callback = RenderCallback(render_freq=1)\n",
    "        callbacks.append(render_callback)\n",
    "    # Use our custom GammaMlpPolicy instead of the default MlpPolicy\n",
    "    model = PPO(\n",
    "        GammaMlpPolicy,\n",
    "        env,\n",
    "        learning_rate=3e-4,\n",
    "        n_steps=1024,\n",
    "        batch_size=1024,\n",
    "        n_epochs=4,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        clip_range=0.2,\n",
    "        verbose=1,\n",
    "        tensorboard_log=\"./ppo_tensorboard/\",\n",
    "        policy_kwargs={\"net_arch\": [{\"pi\": [256, 256], \"vf\": [256, 256]}],\n",
    "                       \"activation_fn\": nn.ReLU}\n",
    "    )\n",
    "    model.learn(total_timesteps=total_timesteps, callback=callbacks, log_interval=1)\n",
    "    os.makedirs(\"trained_models\", exist_ok=True)\n",
    "    model_path = os.path.join(\"trained_models\", \"gamma_ppo_model\")\n",
    "    model.save(model_path)\n",
    "    print(f\"Model saved to {model_path}.zip\")\n",
    "    metrics_callback.save_metrics(save_dir=\"training_metrics\")\n",
    "    print(\"Metrics saved to 'training_metrics/'\")\n",
    "    env.close()\n",
    "\n",
    "###############################################################################\n",
    "# OPTIONAL: WATCH THE TRAINED MODEL\n",
    "###############################################################################\n",
    "def watch_trained_model(model_path=\"trained_models/gamma_ppo_model\"):\n",
    "    model = PPO.load(model_path)\n",
    "    env = DemoArbitrationEnv(visualize=True)\n",
    "    obs, _ = env.reset()\n",
    "    done, truncated = False, False\n",
    "    while not (done or truncated):\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        env.render()\n",
    "    env.close()\n",
    "\n",
    "###############################################################################\n",
    "# MAIN (EXAMPLE)\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    train_model(total_timesteps=1_000_000, visualize=False)\n",
    "    # Optionally:\n",
    "    # watch_trained_model(\"trained_models/gamma_ppo_model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
